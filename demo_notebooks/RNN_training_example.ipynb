{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a6c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climsim_utils.data_utils import *\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20ac0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = '../grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = '../preprocessing/normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc').astype(np.float32)\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc').astype(np.float32)\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc').astype(np.float32)\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc').astype(np.float32)\n",
    "\n",
    "ml_backend = 'pytorch'\n",
    "input_abbrev = 'mlexpand'\n",
    "output_abbrev = 'mlo'\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale,\n",
    "                  ml_backend = ml_backend,\n",
    "                  normalize = True,\n",
    "                  input_abbrev = input_abbrev,\n",
    "                  output_abbrev = output_abbrev,\n",
    "                  save_h5=True,\n",
    "                  save_npy=False,\n",
    "                  )\n",
    "#data.set_to_v2_vars()\n",
    "data.set_to_v4_vars()\n",
    "\n",
    "#data.area_wgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14bf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim_data/ClimSim_low-res-expanded/train/preprocessed/\"\n",
    "tr_data_fname = \"train_first4months.h5\"\n",
    "tr_data_fname = \"train_y1.h5\"\n",
    "tr_data_fname = \"train_y1-2.h5\"\n",
    "tr_data_path = data_dir + tr_data_fname\n",
    "\n",
    "use_val = False \n",
    "use_val = True\n",
    "\n",
    "if use_val:\n",
    "    val_data_fname = \"train_y5.h5\"\n",
    "    val_data_path = data_dir + val_data_fname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22eedd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vars_1D_outp = []; vars_2D_outp = []\n",
    "\n",
    "all_vars = list(data.output_scale.keys())\n",
    "for var in all_vars:\n",
    "    if 'lev' in data.output_scale[var].dims:\n",
    "        vars_2D_outp.append(var)\n",
    "    else:\n",
    "        vars_1D_outp.append(var)  \n",
    "        \n",
    "yscale_lev = data.output_scale[vars_2D_outp].to_dataarray(dim='features', name='outputs_lev').transpose().values\n",
    "yscale_sca = data.output_scale[vars_1D_outp].to_dataarray(dim='features', name='outputs_sca').transpose().values       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0014a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ptend_t', 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', 'ptend_v']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_2D_outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "02ec7cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83877478, 0.9382394 , 0.9382394 , 1.02946286, 0.89260949,\n",
       "       0.92079816, 1.02168695, 1.04693169, 0.92079816, 0.89260949,\n",
       "       1.04693169, 1.02168695, 0.9382394 , 0.83877478, 1.02946286,\n",
       "       0.9382394 , 0.89260949, 1.02168695, 0.92079816, 1.04693169,\n",
       "       1.0786298 , 1.12313795, 1.12313795, 1.1663253 , 1.12313795,\n",
       "       1.0786298 , 1.1663253 , 1.12313795, 1.02168695, 0.89260949,\n",
       "       1.04693169, 0.92079816, 0.92079816, 1.04693169, 0.89260949,\n",
       "       1.02168695, 1.12313795, 1.1663253 , 1.0786298 , 1.12313795,\n",
       "       1.1663253 , 1.12313795, 1.12313795, 1.0786298 , 1.04693169,\n",
       "       0.92079816, 1.02168695, 0.89260949, 0.9382394 , 1.02946286,\n",
       "       0.83877478, 0.9382394 , 1.02168695, 1.04693169, 0.89260949,\n",
       "       0.92079816, 1.04693169, 1.02168695, 0.92079816, 0.89260949,\n",
       "       1.02946286, 0.9382394 , 0.9382394 , 0.83877478, 0.83877478,\n",
       "       0.9382394 , 0.9382394 , 1.02946286, 0.89260949, 0.92079816,\n",
       "       1.02168695, 1.04693169, 0.92079816, 0.89260949, 1.04693169,\n",
       "       1.02168695, 0.9382394 , 0.83877478, 1.02946286, 0.9382394 ,\n",
       "       0.89260949, 1.02168695, 0.92079816, 1.04693169, 1.0786298 ,\n",
       "       1.12313795, 1.12313795, 1.1663253 , 1.12313795, 1.0786298 ,\n",
       "       1.1663253 , 1.12313795, 1.02168695, 0.89260949, 1.04693169,\n",
       "       0.92079816, 0.92079816, 1.04693169, 0.89260949, 1.02168695,\n",
       "       1.12313795, 1.1663253 , 1.0786298 , 1.12313795, 1.1663253 ,\n",
       "       1.12313795, 1.12313795, 1.0786298 , 1.04693169, 0.92079816,\n",
       "       1.02168695, 0.89260949, 0.9382394 , 1.02946286, 0.83877478,\n",
       "       0.9382394 , 1.02168695, 1.04693169, 0.89260949, 0.92079816,\n",
       "       1.04693169, 1.02168695, 0.92079816, 0.89260949, 1.02946286,\n",
       "       0.9382394 , 0.9382394 , 0.83877478, 0.83877478, 0.9382394 ,\n",
       "       0.9382394 , 1.02946286, 0.89260949, 0.92079816, 1.02168695,\n",
       "       1.04693169, 0.92079816, 0.89260949, 1.04693169, 1.02168695,\n",
       "       0.9382394 , 0.83877478, 1.02946286, 0.9382394 , 0.89260949,\n",
       "       1.02168695, 0.92079816, 1.04693169, 1.0786298 , 1.12313795,\n",
       "       1.12313795, 1.1663253 , 1.12313795, 1.0786298 , 1.1663253 ,\n",
       "       1.12313795, 1.02168695, 0.89260949, 1.04693169, 0.92079816,\n",
       "       0.92079816, 1.04693169, 0.89260949, 1.02168695, 1.12313795,\n",
       "       1.1663253 , 1.0786298 , 1.12313795, 1.1663253 , 1.12313795,\n",
       "       1.12313795, 1.0786298 , 1.04693169, 0.92079816, 1.02168695,\n",
       "       0.89260949, 0.9382394 , 1.02946286, 0.83877478, 0.9382394 ,\n",
       "       1.02168695, 1.04693169, 0.89260949, 0.92079816, 1.04693169,\n",
       "       1.02168695, 0.92079816, 0.89260949, 1.02946286, 0.9382394 ,\n",
       "       0.9382394 , 0.83877478, 0.83877478, 0.9382394 , 0.9382394 ,\n",
       "       1.02946286, 0.89260949, 0.92079816, 1.02168695, 1.04693169,\n",
       "       0.92079816, 0.89260949, 1.04693169, 1.02168695, 0.9382394 ,\n",
       "       0.83877478, 1.02946286, 0.9382394 , 0.89260949, 1.02168695,\n",
       "       0.92079816, 1.04693169, 1.0786298 , 1.12313795, 1.12313795,\n",
       "       1.1663253 , 1.12313795, 1.0786298 , 1.1663253 , 1.12313795,\n",
       "       1.02168695, 0.89260949, 1.04693169, 0.92079816, 0.92079816,\n",
       "       1.04693169, 0.89260949, 1.02168695, 1.12313795, 1.1663253 ,\n",
       "       1.0786298 , 1.12313795, 1.1663253 , 1.12313795, 1.12313795,\n",
       "       1.0786298 , 1.04693169, 0.92079816, 1.02168695, 0.89260949,\n",
       "       0.9382394 , 1.02946286, 0.83877478, 0.9382394 , 1.02168695,\n",
       "       1.04693169, 0.89260949, 0.92079816, 1.04693169, 1.02168695,\n",
       "       0.92079816, 0.89260949, 1.02946286, 0.9382394 , 0.9382394 ,\n",
       "       0.83877478, 0.83877478, 0.9382394 , 0.9382394 , 1.02946286,\n",
       "       0.89260949, 0.92079816, 1.02168695, 1.04693169, 0.92079816,\n",
       "       0.89260949, 1.04693169, 1.02168695, 0.9382394 , 0.83877478,\n",
       "       1.02946286, 0.9382394 , 0.89260949, 1.02168695, 0.92079816,\n",
       "       1.04693169, 1.0786298 , 1.12313795, 1.12313795, 1.1663253 ,\n",
       "       1.12313795, 1.0786298 , 1.1663253 , 1.12313795, 1.02168695,\n",
       "       0.89260949, 1.04693169, 0.92079816, 0.92079816, 1.04693169,\n",
       "       0.89260949, 1.02168695, 1.12313795, 1.1663253 , 1.0786298 ,\n",
       "       1.12313795, 1.1663253 , 1.12313795, 1.12313795, 1.0786298 ,\n",
       "       1.04693169, 0.92079816, 1.02168695, 0.89260949, 0.9382394 ,\n",
       "       1.02946286, 0.83877478, 0.9382394 , 1.02168695, 1.04693169,\n",
       "       0.89260949, 0.92079816, 1.04693169, 1.02168695, 0.92079816,\n",
       "       0.89260949, 1.02946286, 0.9382394 , 0.9382394 , 0.83877478,\n",
       "       0.83877478, 0.9382394 , 0.9382394 , 1.02946286, 0.89260949,\n",
       "       0.92079816, 1.02168695, 1.04693169, 0.92079816, 0.89260949,\n",
       "       1.04693169, 1.02168695, 0.9382394 , 0.83877478, 1.02946286,\n",
       "       0.9382394 , 0.89260949, 1.02168695, 0.92079816, 1.04693169,\n",
       "       1.0786298 , 1.12313795, 1.12313795, 1.1663253 , 1.12313795,\n",
       "       1.0786298 , 1.1663253 , 1.12313795, 1.02168695, 0.89260949,\n",
       "       1.04693169, 0.92079816, 0.92079816, 1.04693169, 0.89260949,\n",
       "       1.02168695, 1.12313795, 1.1663253 , 1.0786298 , 1.12313795,\n",
       "       1.1663253 , 1.12313795, 1.12313795, 1.0786298 , 1.04693169,\n",
       "       0.92079816, 1.02168695, 0.89260949, 0.9382394 , 1.02946286,\n",
       "       0.83877478, 0.9382394 , 1.02168695, 1.04693169, 0.89260949,\n",
       "       0.92079816, 1.04693169, 1.02168695, 0.92079816, 0.89260949,\n",
       "       1.02946286, 0.9382394 , 0.9382394 , 0.83877478])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.area_wgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaed23cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 1kB\n",
       "Dimensions:         (lev: 60)\n",
       "Dimensions without coordinates: lev\n",
       "Data variables: (12/14)\n",
       "    ptend_t         (lev) float32 240B 1.005e+03 1.005e+03 ... 1.005e+03\n",
       "    ptend_q0001     (lev) float32 240B 2.835e+06 2.835e+06 ... 2.835e+06\n",
       "    ptend_q0002     (lev) float32 240B 5.669e+06 5.669e+06 ... 5.669e+06\n",
       "    ptend_q0003     (lev) float32 240B 2.835e+06 2.835e+06 ... 2.835e+06\n",
       "    ptend_u         (lev) float32 240B 250.0 250.0 250.0 ... 250.0 250.0 250.0\n",
       "    ptend_v         (lev) float32 240B 500.0 500.0 500.0 ... 500.0 500.0 500.0\n",
       "    ...              ...\n",
       "    cam_out_PRECSC  float32 4B 1.244e+07\n",
       "    cam_out_PRECC   float32 4B 1.313e+06\n",
       "    cam_out_SOLS    float32 4B 0.005\n",
       "    cam_out_SOLL    float32 4B 0.0046\n",
       "    cam_out_SOLSD   float32 4B 0.0061\n",
       "    cam_out_SOLLD   float32 4B 0.0095</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-e7f46bb6-9ab3-4196-9227-7c2e64914fae' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-e7f46bb6-9ab3-4196-9227-7c2e64914fae' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>lev</span>: 60</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-35b4fa82-761a-4590-a96e-53d2c198deba' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-35b4fa82-761a-4590-a96e-53d2c198deba' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-bd22a82f-f31b-437a-b35b-b166e7b404d5' class='xr-section-summary-in' type='checkbox'  checked><label for='section-bd22a82f-f31b-437a-b35b-b166e7b404d5' class='xr-section-summary' >Data variables: <span>(14)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>ptend_t</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>1.005e+03 1.005e+03 ... 1.005e+03</div><input id='attrs-d837ecc0-8a1b-420f-bede-a3c18c0f092e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-d837ecc0-8a1b-420f-bede-a3c18c0f092e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2708ec1e-5fde-436a-9f2a-3a555cb3a841' class='xr-var-data-in' type='checkbox'><label for='data-2708ec1e-5fde-436a-9f2a-3a555cb3a841' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64, 1004.64,\n",
       "       1004.64, 1004.64, 1004.64, 1004.64], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ptend_q0001</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>2.835e+06 2.835e+06 ... 2.835e+06</div><input id='attrs-be8cdf24-899b-4fe4-8ad7-9a2068a25f89' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-be8cdf24-899b-4fe4-8ad7-9a2068a25f89' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1549eb54-9561-4f93-8e4a-2969add720b4' class='xr-var-data-in' type='checkbox'><label for='data-1549eb54-9561-4f93-8e4a-2969add720b4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.],\n",
       "      dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ptend_q0002</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>5.669e+06 5.669e+06 ... 5.669e+06</div><input id='attrs-153fdde2-c5ea-448e-9113-861bda8bdb75' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-153fdde2-c5ea-448e-9113-861bda8bdb75' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-02870f79-64be-4e84-9120-0c05028f0996' class='xr-var-data-in' type='checkbox'><label for='data-02870f79-64be-4e84-9120-0c05028f0996' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.,\n",
       "       5669400., 5669400., 5669400., 5669400., 5669400., 5669400.],\n",
       "      dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ptend_q0003</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>2.835e+06 2.835e+06 ... 2.835e+06</div><input id='attrs-1dd797cb-f8e7-4287-8ad8-2653af62fd78' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1dd797cb-f8e7-4287-8ad8-2653af62fd78' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b0c22d76-a6d6-4a0f-bfed-ea6eb33cfb16' class='xr-var-data-in' type='checkbox'><label for='data-b0c22d76-a6d6-4a0f-bfed-ea6eb33cfb16' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.,\n",
       "       2834700., 2834700., 2834700., 2834700., 2834700., 2834700.],\n",
       "      dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ptend_u</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>250.0 250.0 250.0 ... 250.0 250.0</div><input id='attrs-a34db621-d336-4e81-ba65-01d9ff00cb2f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a34db621-d336-4e81-ba65-01d9ff00cb2f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-56615107-2b41-465c-9e4a-c949710cbf67' class='xr-var-data-in' type='checkbox'><label for='data-56615107-2b41-465c-9e4a-c949710cbf67' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([250., 250., 250., 250., 250., 250., 250., 250., 250., 250., 250.,\n",
       "       250., 250., 250., 250., 250., 250., 250., 250., 250., 250., 250.,\n",
       "       250., 250., 250., 250., 250., 250., 250., 250., 250., 250., 250.,\n",
       "       250., 250., 250., 250., 250., 250., 250., 250., 250., 250., 250.,\n",
       "       250., 250., 250., 250., 250., 250., 250., 250., 250., 250., 250.,\n",
       "       250., 250., 250., 250., 250.], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ptend_v</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>500.0 500.0 500.0 ... 500.0 500.0</div><input id='attrs-35a6b698-8198-4a4e-b99e-ed8ddddf477c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-35a6b698-8198-4a4e-b99e-ed8ddddf477c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9e47e495-1f6b-45f4-8a87-ea95d8883a95' class='xr-var-data-in' type='checkbox'><label for='data-9e47e495-1f6b-45f4-8a87-ea95d8883a95' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500.,\n",
       "       500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500.,\n",
       "       500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500.,\n",
       "       500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500.,\n",
       "       500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500.,\n",
       "       500., 500., 500., 500., 500.], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_NETSW</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0024</div><input id='attrs-37f7adaf-2d86-4ba5-bb85-0fec5290768c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-37f7adaf-2d86-4ba5-bb85-0fec5290768c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4a46a0b6-896e-4b27-a928-7ff6ea991dbb' class='xr-var-data-in' type='checkbox'><label for='data-4a46a0b6-896e-4b27-a928-7ff6ea991dbb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.0024, dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_FLWDS</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.005</div><input id='attrs-8cf9bb3f-30ef-46f1-be3c-bb94d93f29fc' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8cf9bb3f-30ef-46f1-be3c-bb94d93f29fc' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4b96d063-8cc2-4c6a-a5f3-6ac6da432edd' class='xr-var-data-in' type='checkbox'><label for='data-4b96d063-8cc2-4c6a-a5f3-6ac6da432edd' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.005, dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_PRECSC</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>1.244e+07</div><input id='attrs-72c7a561-b4b3-4158-acba-2c9d87ddf345' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-72c7a561-b4b3-4158-acba-2c9d87ddf345' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-50efabaf-6c11-43f3-9561-5262a4b670cc' class='xr-var-data-in' type='checkbox'><label for='data-50efabaf-6c11-43f3-9561-5262a4b670cc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(12441600., dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_PRECC</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>1.313e+06</div><input id='attrs-4b1597f1-c0c4-4f59-bb3e-7b292b4cdd88' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-4b1597f1-c0c4-4f59-bb3e-7b292b4cdd88' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3ba6c7cc-d05a-47f1-8ff2-d589429f9ea2' class='xr-var-data-in' type='checkbox'><label for='data-3ba6c7cc-d05a-47f1-8ff2-d589429f9ea2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(1313280., dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_SOLS</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.005</div><input id='attrs-b15d18dc-3100-4ecb-84a4-f99353681989' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-b15d18dc-3100-4ecb-84a4-f99353681989' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-465548eb-553e-4258-aee8-5d59fa18ba6a' class='xr-var-data-in' type='checkbox'><label for='data-465548eb-553e-4258-aee8-5d59fa18ba6a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.005, dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_SOLL</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0046</div><input id='attrs-8bae7268-0419-4c86-b211-014d1ffd934e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8bae7268-0419-4c86-b211-014d1ffd934e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fd6cdf70-4b4c-49e7-939b-05aa6c970dda' class='xr-var-data-in' type='checkbox'><label for='data-fd6cdf70-4b4c-49e7-939b-05aa6c970dda' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.0046, dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_SOLSD</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0061</div><input id='attrs-fd0ea470-019a-4cd8-9a74-5d0ce337a638' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-fd0ea470-019a-4cd8-9a74-5d0ce337a638' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5cecaa87-cd7c-4804-936c-d3d995597ff3' class='xr-var-data-in' type='checkbox'><label for='data-5cecaa87-cd7c-4804-936c-d3d995597ff3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.0061, dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_out_SOLLD</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0095</div><input id='attrs-82373df2-b3ca-4e22-84c8-48c3f9ff47ed' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-82373df2-b3ca-4e22-84c8-48c3f9ff47ed' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a66e4834-ced2-45e8-9061-d1d84f78ab01' class='xr-var-data-in' type='checkbox'><label for='data-a66e4834-ced2-45e8-9061-d1d84f78ab01' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.0095, dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-5b5eb740-a550-4970-a6bc-b7821afa8b29' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-5b5eb740-a550-4970-a6bc-b7821afa8b29' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-43f99a6b-ced7-4dcc-a750-0eb35f91e5ea' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-43f99a6b-ced7-4dcc-a750-0eb35f91e5ea' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 1kB\n",
       "Dimensions:         (lev: 60)\n",
       "Dimensions without coordinates: lev\n",
       "Data variables: (12/14)\n",
       "    ptend_t         (lev) float32 240B 1.005e+03 1.005e+03 ... 1.005e+03\n",
       "    ptend_q0001     (lev) float32 240B 2.835e+06 2.835e+06 ... 2.835e+06\n",
       "    ptend_q0002     (lev) float32 240B 5.669e+06 5.669e+06 ... 5.669e+06\n",
       "    ptend_q0003     (lev) float32 240B 2.835e+06 2.835e+06 ... 2.835e+06\n",
       "    ptend_u         (lev) float32 240B 250.0 250.0 250.0 ... 250.0 250.0 250.0\n",
       "    ptend_v         (lev) float32 240B 500.0 500.0 500.0 ... 500.0 500.0 500.0\n",
       "    ...              ...\n",
       "    cam_out_PRECSC  float32 4B 1.244e+07\n",
       "    cam_out_PRECC   float32 4B 1.313e+06\n",
       "    cam_out_SOLS    float32 4B 0.005\n",
       "    cam_out_SOLL    float32 4B 0.0046\n",
       "    cam_out_SOLSD   float32 4B 0.0061\n",
       "    cam_out_SOLLD   float32 4B 0.0095"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.output_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3ed52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6) (8,)\n"
     ]
    }
   ],
   "source": [
    "print(yscale_lev.shape, yscale_sca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038d6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['input_lev', 'input_sca', 'output_lev', 'output_sca']>\n",
      "<KeysViewHDF5 []>\n",
      "<KeysViewHDF5 ['varnames']>\n",
      "['state_t' 'state_q0001' 'state_q0002' 'state_q0003' 'state_u' 'state_v'\n",
      " 'pbuf_ozone' 'pbuf_CH4' 'pbuf_N2O']\n"
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "\n",
    "hf = h5py.File(data_path, 'r')\n",
    "print(hf.keys())\n",
    "# <KeysViewHDF5 ['input_lev', 'input_sca', 'output_lev', 'output_sca']>\n",
    "print(hf.attrs.keys())\n",
    "print(hf['input_lev'].attrs.keys())\n",
    "print(hf['input_lev'].attrs.get('varnames'))\n",
    "# future training data should have a \"varnames\" attribute for each dataset type \n",
    "\n",
    "#2D Input variables: ['state_t', 'state_q0001', 'state_q0002', 'state_q0003', 'state_u', 'state_v', \n",
    "# 'pbuf_ozone', 'pbuf_CH4', 'pbuf_N2O']\n",
    "# We need pressure!\n",
    "\n",
    "#1D (scalar) Input variables: ['state_ps', 'pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX', 'pbuf_TAUX', \n",
    "# 'pbuf_TAUY', 'pbuf_COSZRS', 'cam_in_ALDIF', 'cam_in_ALDIR', 'cam_in_ASDIF', 'cam_in_ASDIR', \n",
    "# 'cam_in_LWUP', 'cam_in_ICEFRAC', 'cam_in_LANDFRAC', 'cam_in_OCNFRAC', 'cam_in_SNOWHICE', \n",
    "# 'cam_in_SNOWHLAND', 'lat', 'lon']\n",
    "\n",
    "#2D Output variables: ['ptend_t', 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', 'ptend_v']\n",
    "\n",
    "#1D (scalar) Output variables: ['cam_out_NETSW', 'cam_out_FLWDS', 'cam_out_PRECSC', \n",
    "#'cam_out_PRECC', 'cam_out_SOLS', 'cam_out_SOLL', 'cam_out_SOLSD', 'cam_out_SOLLD']\n",
    "hf.close()\n",
    "\n",
    "#state_q0001 lev, ncol kg/kg Specific humidity\n",
    "#state_q0002 lev, ncol kg/kg Cloud liquid mixing ratio\n",
    "#state_q0003 lev, ncol kg/kg Cloud ice mixing ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d07b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Attributes of HDF5 object at 139879727771344>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf = h5py.File(data_path, 'r')\n",
    "\n",
    "hf['input_lev'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec536b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parameter as Parameter\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, RNN_type='LSTM', \n",
    "                 nx = 9, nx_sfc=17, \n",
    "                 ny = 8, ny_sfc=8, \n",
    "                 nneur=(64,64), \n",
    "                 outputs_one_longer=False, # if True, inputs are a sequence\n",
    "                 # of N and outputs a sequence of N+1 (e.g. predicting fluxes)\n",
    "                 concat=False, out_scale=None, out_sfc_scale = None):\n",
    "        # Simple bidirectional RNN (Either LSTM or GRU) for predicting column \n",
    "        # outputs shaped either (B, L, Ny) or (B, L+1, Ny) from column inputs\n",
    "        # (B, L, Nx) and optionally surface inputs (B, Nx_sfc) \n",
    "        # If surface inputs exist, they are used to initialize first (upward) RNN \n",
    "        # Assumes top-of-atmosphere is first in memory i.e. at index 0 \n",
    "        # if it's not the flip operations need to be moved!\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny \n",
    "        self.nx_sfc = nx_sfc \n",
    "        self.ny_sfc = ny_sfc\n",
    "        self.nneur = nneur \n",
    "        self.outputs_one_longer=outputs_one_longer\n",
    "        if len(nneur) < 1 or len(nneur) > 3:\n",
    "            sys.exit(\"Number of RNN layers and length of nneur should be 2 or 3\")\n",
    "\n",
    "        self.RNN_type=RNN_type\n",
    "        if self.RNN_type=='LSTM':\n",
    "            RNN_model = nn.LSTM\n",
    "        elif self.RNN_type=='GRU':\n",
    "            RNN_model = nn.GRU\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "                    \n",
    "        self.concat=concat\n",
    "        \n",
    "        if out_scale is not None:\n",
    "            cuda = torch.cuda.is_available() \n",
    "            device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "            self.yscale_lev = torch.from_numpy(out_scale).to(device)\n",
    "            self.yscale_sca = torch.from_numpy(out_sfc_scale).to(device)\n",
    "\n",
    "        if self.nx_sfc > 0:\n",
    "            self.mlp_surface1  = nn.Linear(nx_sfc, self.nneur[0])\n",
    "            if self.RNN_type==\"LSTM\":\n",
    "                self.mlp_surface2  = nn.Linear(nx_sfc, self.nneur[0])\n",
    "\n",
    "        self.rnn1      = RNN_model(nx,            self.nneur[0], batch_first=True) # (input_size, hidden_size, num_layers=1\n",
    "        self.rnn2      = RNN_model(self.nneur[0], self.nneur[1], batch_first=True)\n",
    "        if len(self.nneur)==3:\n",
    "            self.rnn3      = RNN_model(self.nneur[1], self.nneur[2], batch_first=True)\n",
    "\n",
    "        # The final hidden variable is either the output from the last RNN, or\n",
    "        # the  concatenated outputs from all RNNs\n",
    "        if concat:\n",
    "            nh_rnn = sum(nneur)\n",
    "        else:\n",
    "            nh_rnn = nneur[-1]\n",
    "\n",
    "        self.mlp_output = nn.Linear(nh_rnn, self.ny)\n",
    "        if self.ny_sfc>0:\n",
    "            self.mlp_surface_output = nn.Linear(nneur[-1], self.ny_sfc)\n",
    "            \n",
    "    def postprocessing(self, out, out_sfc):\n",
    "        out_denorm = out / self.yscale_lev\n",
    "        out_sfc_denorm  = out_sfc / self.yscale_sca\n",
    "        return out_denorm, out_sfc_denorm\n",
    "    \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "            \n",
    "        # batch_size = inputs_main.shape[0]\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "        sfc1 = self.mlp_surface1(inputs_sfc)\n",
    "        sfc1 = nn.Tanh()(sfc1)\n",
    "\n",
    "        if self.RNN_type==\"LSTM\":\n",
    "            sfc2 = self.mlp_surface2(inputs_sfc)\n",
    "            sfc2 = nn.Tanh()(sfc2)\n",
    "            hidden = (sfc1.view(1,-1,self.nneur[0]), sfc2.view(1,-1,self.nneur[0])) # (h0, c0)\n",
    "        else:\n",
    "            hidden = (sfc1.view(1,-1,self.nneur[0]))\n",
    "\n",
    "        # print(f'Using state1 {hidden}')\n",
    "        # TOA is first in memory, so we need to flip the axis\n",
    "        inputs_main = torch.flip(inputs_main, [1])\n",
    "      \n",
    "        out, hidden = self.rnn1(inputs_main, hidden)\n",
    "        \n",
    "        if self.outputs_one_longer:\n",
    "            out = torch.cat((sfc1, out),axis=1)\n",
    "\n",
    "        out = torch.flip(out, [1]) # the surface was processed first, but for\n",
    "        # the second RNN (and the final output) we want TOA first\n",
    "        \n",
    "        out2, hidden2 = self.rnn2(out) \n",
    "        \n",
    "        (last_h, last_c) = hidden2\n",
    "\n",
    "        if len(self.nneur)==3:\n",
    "            rnn3_input = torch.flip(out2, [1])\n",
    "            \n",
    "            out3, hidden3 = self.rnn3(rnn3_input) \n",
    "            \n",
    "            out3 = torch.flip(out3, [1])\n",
    "            \n",
    "            if self.concat:\n",
    "                rnnout = torch.cat((out3, out2, out),axis=2)\n",
    "            else:\n",
    "                rnnout = out3\n",
    "        else:\n",
    "            if self.concat:\n",
    "                rnnout = torch.cat((out2, out),axis=2)\n",
    "            else:\n",
    "                rnnout = out2\n",
    "        \n",
    "        out = self.mlp_output(rnnout)\n",
    "\n",
    "        if self.ny_sfc>0:\n",
    "            #print(\"shape last_c\", last_c.shape)\n",
    "            # use cell state or hidden state?\n",
    "            out_sfc = self.mlp_surface_output(last_h.squeeze())\n",
    "            return out, out_sfc\n",
    "        else:\n",
    "            return out \n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3cf6fc-d804-41a1-9c67-0d11b411651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class RNN_autoreg(nn.Module):\n",
    "    def __init__(self, nlay=60, nx = 4, nx_sfc=3, ny = 4, ny_sfc=3, nneur=(64,64), \n",
    "                cell_type=\"LSTM\",\n",
    "                memory=\"None\", # \"None\", \"Hidden\", or \"Output\",\n",
    "                concat=False,\n",
    "                use_initial_mlp=False, ensemble_size=1,\n",
    "                random_init_cx=False,\n",
    "                use_intermediate_mlp=True,\n",
    "                add_pres=False,\n",
    "                third_rnn=False,\n",
    "                add_stochastic_layer=False,\n",
    "                coeff_stochastic = 0.0,\n",
    "                dtype=torch.float32,\n",
    "                out_scale=None, out_sfc_scale=None):\n",
    "        super(RNN_autoreg, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny \n",
    "        self.nlay = nlay \n",
    "        self.nx_sfc = nx_sfc \n",
    "        self.ny_sfc = ny_sfc\n",
    "        self.nneur = nneur \n",
    "        self.use_initial_mlp=use_initial_mlp\n",
    "        self.add_pres = add_pres\n",
    "        if self.add_pres:\n",
    "            self.preslay = LayerPressure()\n",
    "            nx = nx +1\n",
    "        self.nh_rnn1 = self.nneur[0]\n",
    "        self.nx_rnn2 = self.nneur[0]\n",
    "        self.nh_rnn2 = self.nneur[1]\n",
    "        self.concat = concat\n",
    "        self.model_type=cell_type\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.random_init_cx= random_init_cx\n",
    "        self.third_rnn=third_rnn\n",
    "        self.add_stochastic_layer = add_stochastic_layer\n",
    "        self.coeff_stochastic = coeff_stochastic\n",
    "        self.dtype=dtype\n",
    "        if self.ensemble_size>1 and not add_stochastic_layer:\n",
    "        # In this case the stochasticity comes purely from random initialization\n",
    "        # of hidden states (probably not enough)\n",
    "            self.random_init_cx=True\n",
    "        self.use_intermediate_mlp=use_intermediate_mlp\n",
    "        self.share_weights = False\n",
    "        if self.share_weights:\n",
    "            self.use_intermediate_mlp=False\n",
    "        if self.use_initial_mlp:\n",
    "            self.nx_rnn1 = self.nneur[0]\n",
    "        else:\n",
    "            self.share_weights=False\n",
    "            self.nx_rnn1 = nx\n",
    "        self.memory = memory\n",
    "        if out_scale is not None:\n",
    "            cuda = torch.cuda.is_available() \n",
    "            device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "            self.yscale_lev = torch.from_numpy(out_scale).to(device)\n",
    "            self.yscale_sca = torch.from_numpy(out_sfc_scale).to(device)\n",
    "        if memory == 'None':\n",
    "            raise NotImplementedError()\n",
    "        elif memory == 'Output':\n",
    "            print(\"Building RNN that feeds its output t0,z0 to its inputs at t1,z0\")\n",
    "            self.rnn1_mem = None \n",
    "            self.nh_mem = self.ny\n",
    "            self.nx_rnn1 = self.nx_rnn1 + self.nh_mem\n",
    "            self.nh_rnn1 = self.nneur[0]\n",
    "        elif memory == 'Hidden':\n",
    "            self.nh_mem = self.nneur[1]\n",
    "            print(\"Building RNN that feeds its hidden memory at t0,z0 to its inputs at t1,z0\")\n",
    "            print(\"Initial mlp: {}, intermediate mlp: {}\".format(self.use_initial_mlp, self.use_intermediate_mlp))\n",
    " \n",
    "            self.rnn1_mem = None \n",
    "            self.nx_rnn1 = self.nx_rnn1 + self.nh_mem\n",
    "            self.nh_rnn1 = self.nneur[0]\n",
    "        elif memory == 'CustomLSTM':\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            sys.exit(\"memory argument must equal one of : 'None', 'Hidden', or 'CustomLSTM'\")\n",
    "            \n",
    "        print(\"nx rnn1\", self.nx_rnn1, \"nh rnn1\", self.nh_rnn1)\n",
    "        print(\"nx rnn2\", self.nx_rnn2, \"nh rnn2\", self.nh_rnn2)  \n",
    "        print(\"Cell type:\", cell_type)\n",
    "        if self.use_initial_mlp:\n",
    "            self.mlp_initial = nn.Linear(nx, self.nneur[0])\n",
    "\n",
    "        self.mlp_surface1  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "        if not self.random_init_cx:\n",
    "            self.mlp_surface2  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "\n",
    "        if self.model_type==\"LSTM\":\n",
    "            # self.rnn1      = nn.LSTMCell(self.nx_rnn1, self.nh_rnn1)  # (input_size, hidden_size)\n",
    "            # self.rnn2      = nn.LSTMCell(self.nx_rnn2, self.nh_rnn2)\n",
    "            self.rnn1      = nn.LSTM(self.nx_rnn1, self.nh_rnn1,  batch_first=True)  # (input_size, hidden_size)\n",
    "            if not self.share_weights:\n",
    "                self.rnn2      = nn.LSTM(self.nx_rnn2, self.nh_rnn2,  batch_first=True)\n",
    "                if self.third_rnn:\n",
    "                    self.rnn3      = nn.LSTM(self.nh_rnn2, self.nneur[2],  batch_first=True)\n",
    "                \n",
    "        elif self.model_type==\"GRU\":\n",
    "              self.rnn1      = nn.GRU(self.nx_rnn1, self.nh_rnn1,  batch_first=True)   # (input_size, hidden_size)\n",
    "              self.rnn2      = nn.GRU(self.nx_rnn2, self.nh_rnn2,  batch_first=True) \n",
    "        elif self.model_type==\"RNN\":\n",
    "              self.rnn1      = nn.RNN(self.nx_rnn1, self.nh_rnn1,  batch_first=True)   # (input_size, hidden_size)\n",
    "              self.rnn2      = nn.RNN(self.nx_rnn2, self.nh_rnn2,  batch_first=True) \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        if self.add_stochastic_layer:\n",
    "            from models_torch_kernels import StochasticGRUCell, MyStochasticGRUCell\n",
    "            nx_srnn = self.nh_rnn2\n",
    "            # self.rnn_stochastic = StochasticGRUCell(nx_srnn, self.nh_rnn2)  # (input_size, hidden_size)\n",
    "            self.rnn_stochastic = MyStochasticGRUCell(nx_srnn, self.nh_rnn2)  # (input_size, hidden_size)\n",
    "        if concat:\n",
    "            nh_rnn = self.nh_rnn2+self.nh_rnn1\n",
    "        else:\n",
    "            nh_rnn = self.nh_rnn2\n",
    "\n",
    "        if self.use_intermediate_mlp: \n",
    "            self.mlp_latent = nn.Linear(nh_rnn, self.nh_rnn1)\n",
    "            self.mlp_output = nn.Linear(self.nh_rnn1, self.ny)\n",
    "        else:\n",
    "            self.mlp_output = nn.Linear(nh_rnn, self.ny)\n",
    "            \n",
    "        if self.ny_sfc>0:\n",
    "            self.mlp_surface_output = nn.Linear(nneur[-1], self.ny_sfc)\n",
    "            \n",
    "    def postprocessing(self, out, out_sfc):\n",
    "        out_denorm = out / self.yscale_lev\n",
    "        out_sfc_denorm  = out_sfc / self.yscale_sca\n",
    "        return out_denorm, out_sfc_denorm\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.rnn1_mem = None\n",
    "\n",
    "    def detach_states(self):\n",
    "        self.rnn1_mem = self.rnn1_mem.detach()\n",
    "   \n",
    "    def get_states(self):\n",
    "        return self.rnn1_mem.detach()\n",
    "\n",
    "    def set_states(self, states):\n",
    "        self.rnn1_mem = states \n",
    "        \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "        if self.ensemble_size>0:\n",
    "            inputs_main = inputs_main.unsqueeze(0)\n",
    "            inputs_sfc = inputs_sfc.unsqueeze(0)\n",
    "            inputs_main = torch.repeat_interleave(inputs_main,repeats=self.ensemble_size,dim=0)\n",
    "            inputs_sfc = torch.repeat_interleave(inputs_sfc,repeats=self.ensemble_size,dim=0)\n",
    "            inputs_main = inputs_main.flatten(0,1)\n",
    "            inputs_sfc = inputs_sfc.flatten(0,1)\n",
    "                    \n",
    "        batch_size = inputs_main.shape[0]\n",
    "        if self.add_pres:\n",
    "            sp = inputs_sfc[:,-1]\n",
    "            pres  = self.preslay(sp)\n",
    "            inputs_main = torch.cat((inputs_main,torch.unsqueeze(pres,2)),dim=2)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "     \n",
    "        if self.rnn1_mem is None: \n",
    "            # self.rnn1_mem = torch.randn(batch_size, self.nlay, self.nh_mem,device=device)\n",
    "            self.rnn1_mem = torch.randn((batch_size, self.nlay, self.nh_mem),dtype=self.dtype,device=device)\n",
    "\n",
    "        hx = self.mlp_surface1(inputs_sfc)\n",
    "        hx = nn.Tanh()(hx)\n",
    "\n",
    "        # TOA is first in memory, so to start at the surface we need to go backwards\n",
    "        inputs_main = torch.flip(inputs_main, [1])\n",
    "\n",
    "        # The input (a vertical sequence) is concatenated with the\n",
    "        # output of the RNN from the previous time step \n",
    "        if self.model_type in [\"LSTM\"]:\n",
    "            if self.random_init_cx:\n",
    "                # cx = torch.randn(batch_size, self.nh_rnn1,device=device)\n",
    "                cx = torch.randn((batch_size, self.nh_rnn1),dtype=self.dtype,device=device)\n",
    "            else:\n",
    "                cx = self.mlp_surface2(inputs_sfc)\n",
    "                cx = nn.Tanh()(cx)\n",
    "            hidden = (torch.unsqueeze(hx,0), torch.unsqueeze(cx,0))\n",
    "        else:\n",
    "            hidden = (torch.unsqueeze(hx,0))\n",
    "\n",
    "        if self.use_initial_mlp:\n",
    "            rnn1_input = self.mlp_initial(inputs_main)\n",
    "            rnn1_input = nn.Tanh()(rnn1_input)\n",
    "        else:\n",
    "            rnn1_input = inputs_main \n",
    "            \n",
    "        rnn1_input = torch.cat((rnn1_input,self.rnn1_mem), axis=2)\n",
    "\n",
    "        rnn1out, states = self.rnn1(rnn1_input, hidden)\n",
    "\n",
    "        rnn1out = torch.flip(rnn1out, [1])\n",
    "\n",
    "        hx2 = torch.randn((batch_size, self.nh_rnn2),dtype=self.dtype,device=device)  # (batch, hidden_size)\n",
    "        if self.model_type in [\"LSTM\"]:\n",
    "            cx2 = torch.randn((batch_size, self.nh_rnn2),dtype=self.dtype,device=device)\n",
    "            hidden2 = (torch.unsqueeze(hx2,0), torch.unsqueeze(cx2,0))\n",
    "        else:\n",
    "            hidden2 = (torch.unsqueeze(hx2,0))\n",
    "\n",
    "        input_rnn2 = rnn1out\n",
    "            \n",
    "        if self.share_weights:\n",
    "            rnn2out, states = self.rnn1(input_rnn2, hidden2)\n",
    "        else:\n",
    "            rnn2out, states = self.rnn2(input_rnn2, hidden2)\n",
    "\n",
    "        (last_h, last_c) = states\n",
    "\n",
    "        if self.third_rnn:\n",
    "            rnn3in = torch.flip(rnn2out, [1])\n",
    "            rnn2out, states = self.rnn3(rnn3in)\n",
    "\n",
    "        if self.concat:\n",
    "            rnn2out = torch.cat((rnn2out,rnn1out),axis=2)\n",
    "        \n",
    "        if self.use_intermediate_mlp: \n",
    "            rnn2out = self.mlp_latent(rnn2out)\n",
    "          \n",
    "        if self.memory==\"Hidden\": \n",
    "            if not self.third_rnn: self.rnn1_mem = torch.flip(rnn2out, [1])\n",
    "            \n",
    "        # Add a stochastic perturbation\n",
    "        # Convective memory is still based on the deterministic model,\n",
    "        # and does not include the stochastic perturbation\n",
    "        # concat and use_intermediate_mlp should be set to false\n",
    "        if self.add_stochastic_layer:\n",
    "            # srnn_input = torch.transpose(rnn2out,0,1)\n",
    "            srnn_input = torch.transpose(self.rnn1_mem,0,1)\n",
    "            # transpose is needed because this layer assumes seq. dim first\n",
    "            z = self.rnn_stochastic(srnn_input)\n",
    "            z = torch.flip(z, [0])\n",
    "\n",
    "            z = torch.transpose(z,0,1)\n",
    "            # z = torch.flip(z, [1])\n",
    "            # rnn2out = z\n",
    "            # z is a perburbation added to the hidden state\n",
    "            # rnn2out = rnn2out + 0.01*z \n",
    "            rnn2out = rnn2out + self.coeff_stochastic*z \n",
    "\n",
    "        out = self.mlp_output(rnn2out)\n",
    "        if self.memory==\"Output\":\n",
    "            if not self.third_rnn: self.rnn1_mem = torch.flip(out, [1])\n",
    "\n",
    "        if self.ny_sfc>0:\n",
    "            #print(\"shape last_c\", last_c.shape)\n",
    "            # use cell state or hidden state?\n",
    "            out_sfc = self.mlp_surface_output(last_h.squeeze())\n",
    "            return out, out_sfc\n",
    "        else:\n",
    "            return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e39e2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, original_model, out_scale, out_sfc_scale\n",
    "                ):\n",
    "        super(WrappedModel, self).__init__()\n",
    "        self.original_model = original_model\n",
    "        \n",
    "    def postprocessing(self, out, out_sfc):\n",
    "        out_denorm = out / self.out_scale\n",
    "        out_sfc_denorm  = out_sfc / self.out_sfc_scale\n",
    "        return out_denorm, out_sfc_denorm\n",
    "    \n",
    "    def combine_outputs(self, out, out_sfc):\n",
    "        out = out.transpose(1,2).flatten(1)\n",
    "        out = torch.cat((out,out_sfc),dim=1)\n",
    "    \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "        #x = self.preprocessing(x)\n",
    " \n",
    "        out, out_sfc = self.original_model(inputs_main, inputs_sfc)\n",
    "        \n",
    "        out, out_sfc = self.postprocessing(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6277670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    mp_autocast = True \n",
    "    print(torch.cuda.is_bf16_supported())\n",
    "    # if torch.cuda.is_bf16_supported(): \n",
    "    #     dtype=torch.bfloat16 \n",
    "    #     use_scaler = False\n",
    "    # else:\n",
    "    #     dtype=torch.float16\n",
    "    #     use_scaler = True \n",
    "    dtype=torch.float16\n",
    "    use_scaler = True \n",
    "else:\n",
    "    mp_autocast = False\n",
    "    use_scaler = False\n",
    "    \n",
    "    \n",
    "if use_scaler:\n",
    "    # scaler = torch.amp.GradScaler(autocast = True)\n",
    "    scaler = torch.amp.GradScaler(device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63eb56f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RNN that feeds its hidden memory at t0,z0 to its inputs at t1,z0\n",
      "Initial mlp: False, intermediate mlp: False\n",
      "nx rnn1 170 nh rnn1 160\n",
      "nx rnn2 160 nh rnn2 160\n",
      "Cell type: LSTM\n",
      "cuda\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "RNN_autoreg                              --\n",
      "├─Linear: 1-1                            2,880\n",
      "├─Linear: 1-2                            2,880\n",
      "├─LSTM: 1-3                              212,480\n",
      "├─LSTM: 1-4                              206,080\n",
      "├─Linear: 1-5                            966\n",
      "├─Linear: 1-6                            1,288\n",
      "=================================================================\n",
      "Total params: 426,574\n",
      "Trainable params: 426,574\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "nlev = 60 \n",
    "\n",
    "nx = 9\n",
    "nx_sfc = 17\n",
    "ny = 6\n",
    "ny_sfc = 8\n",
    "\n",
    "add_refpres = True\n",
    "if add_refpres:\n",
    "    nx = nx + 1\n",
    "\n",
    "autoregressive = True\n",
    "\n",
    "memory = \"Hidden\"\n",
    "concat = False \n",
    "use_initial_mlp = False \n",
    "use_intermediate_mlp = False\n",
    "add_pres = False \n",
    "ensemble_size = 1\n",
    "add_stochastic_layer = False\n",
    "\n",
    "nneur = (64,64)\n",
    "nneur = (128,128)\n",
    "nneur = (160, 160)\n",
    "\n",
    "if autoregressive:\n",
    "    model = RNN_autoreg(cell_type='LSTM', \n",
    "                nlay = nlev, \n",
    "                nx = nx, nx_sfc=nx_sfc, \n",
    "                ny = ny, ny_sfc=ny_sfc, \n",
    "                nneur=nneur,\n",
    "                memory=memory,\n",
    "                concat=concat,\n",
    "                use_initial_mlp=use_initial_mlp,\n",
    "                use_intermediate_mlp=use_intermediate_mlp,\n",
    "                add_pres=add_pres,\n",
    "                ensemble_size=ensemble_size,\n",
    "                add_stochastic_layer=add_stochastic_layer,      \n",
    "                out_scale = yscale_lev,\n",
    "                out_sfc_scale = yscale_sca)\n",
    "else:\n",
    "\n",
    "    model = MyRNN(RNN_type='LSTM', \n",
    "                 nx = nx, nx_sfc=nx_sfc, \n",
    "                 ny = ny, ny_sfc=ny_sfc, \n",
    "                 nneur=nneur,\n",
    "                 out_scale = yscale_lev,\n",
    "                 out_sfc_scale = yscale_sca)\n",
    "\n",
    "cuda = torch.cuda.is_available() \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "infostr = summary(model)\n",
    "num_params = infostr.total_params\n",
    "print(infostr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1261f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10240, 60, 6]) torch.Size([10240, 8])\n"
     ]
    }
   ],
   "source": [
    "test_with_real_data = False\n",
    "\n",
    "if test_with_real_data:\n",
    "    \n",
    "    hf = h5py.File(data_path, 'r')\n",
    "    x_lay = hf['input_lev'][:]\n",
    "    x_sfc = hf['input_sca'][:]\n",
    "    y_lay = hf['output_lev'][:]\n",
    "    y_sfc = hf['output_sca'][:]\n",
    "    hf.close()\n",
    "\n",
    "    print(x_lay.shape, x_lay.min(), x_lay.max())\n",
    "    print(x_sfc.shape, x_sfc.min(), x_sfc.max())\n",
    "    print(y_lay.shape, y_lay.min(), y_lay.max())\n",
    "    print(y_sfc.shape, y_sfc.min(), y_sfc.max())\n",
    "\n",
    "    # Test the model with real data \n",
    "    nb = 1024 \n",
    "\n",
    "    x_lay0 = torch.from_numpy(x_lay[0:10*nb]).to(device)\n",
    "    x_sfc0 = torch.from_numpy(x_sfc[0:10*nb]).to(device)\n",
    "    y_lay0 = torch.from_numpy(y_lay[0:10*nb]).to(device)\n",
    "    y_sfc0 = torch.from_numpy(y_sfc[0:10*nb]).to(device)\n",
    "\n",
    "    print(x_lay0.shape, x_sfc0.shape)\n",
    "    print(y_lay0.shape, y_sfc0.shape)\n",
    "\n",
    "    #ns, nlay, nx = x_lay0.shape\n",
    "    #_, nx_sfc    = x_sfc0.shape\n",
    "    #_, _, ny     = y_lay0.shape\n",
    "    #_, ny_sfc    = y_sfc0.shape\n",
    "\n",
    "\n",
    "    out, out_sfc = model(x_lay0, x_sfc0)\n",
    "    print(out.shape, out_sfc.shape)\n",
    "else:\n",
    "    # Test the model with dummy data \n",
    "    x_lay = torch.zeros((10*nb, nlev, nx))\n",
    "    x_sfc = torch.zeros((10*nb, nx_sfc))\n",
    "\n",
    "    x_lay = x_lay.to(device)\n",
    "    x_sfc = x_sfc.to(device)\n",
    "\n",
    "    out, out_sfc = model(x_lay, x_sfc)\n",
    "    print(out.shape, out_sfc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35bba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, out_sfc = model.postprocessing(out, out_sfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c610834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape ps torch.Size([10240, 1]) min tensor(75208.2969) max tensor(102864.2344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/28272/ipykernel_2837606/3482244970.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  state_ps = state_ps*(data.input_max['state_ps'].values - data.input_min['state_ps'].values) + data.input_mean['state_ps'].values\n"
     ]
    }
   ],
   "source": [
    "state_ps = x_sfc0[:,0:1]\n",
    "\n",
    "if data.normalize:\n",
    "    state_ps = state_ps*(data.input_max['state_ps'].values - data.input_min['state_ps'].values) + data.input_mean['state_ps'].values\n",
    "\n",
    "print(\"shape ps\", state_ps.shape, \"min\", state_ps.min(), \"max\", state_ps.max())\n",
    "\n",
    "#pressure_grid_p1 = np.array(data.grid_info['P0']*data.grid_info['hyai'])[:,np.newaxis,np.newaxis]\n",
    "pressure_grid_p1 = torch.from_numpy(np.array(data.grid_info['P0']*data.grid_info['hyai'])[np.newaxis,:])\n",
    "#print(pressure_grid_p1.shape)\n",
    "pressure_grid_p2 = torch.from_numpy(data.grid_info['hybi'].values[np.newaxis, :]) * state_ps\n",
    "#print(pressure_grid_p2.shape)\n",
    "pressure_grid = pressure_grid_p1 + pressure_grid_p2\n",
    "#print(pressure_grid.shape, pressure_grid.min(), pressure_grid.max())\n",
    "dp     = pressure_grid[:,1:61] - pressure_grid[:,0:60]\n",
    "\n",
    "\n",
    "p1 = np.array(data.grid_info['P0']*data.grid_info['hyam'])[np.newaxis,:] \n",
    "p2 = data.grid_info['hybm'].values[np.newaxis, :] * data.grid_info['P0'].values\n",
    "\n",
    "pref = p1 + p2 \n",
    "print(pref/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147abd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator_xy(torch.utils.data.Dataset):\n",
    "    def __init__(self, filepath, nloc=384, nlev=60, add_refpres=True, cuda=False):\n",
    "        self.filepath = filepath\n",
    "        # The file list will be divided into chunks (a list of lists)eg [[12,4,32],[1,9,3]..]\n",
    "        # where the length of each item is the chunk size; i.e. how many files \n",
    "        # are loaded at once (in this example 3 files)\n",
    "        # self.chunk_size = chunk_size # how many batches are loaded at once in getitem\n",
    "        self.nloc = nloc\n",
    "        self.nlev = nlev\n",
    "        # self.nloc = int(os.path.basename(self.filepath).split('_')[-1])\n",
    "        # self.stateful = stateful\n",
    "        self.refpres = np.array([7.83478113e-02,1.41108318e-01,2.52923297e-01,4.49250635e-01,\n",
    "                    7.86346161e-01,1.34735576e+00,2.24477729e+00,3.61643148e+00,\n",
    "                    5.61583643e+00,8.40325322e+00,1.21444894e+01,1.70168280e+01,\n",
    "                    2.32107981e+01,3.09143463e+01,4.02775807e+01,5.13746323e+01,\n",
    "                    6.41892284e+01,7.86396576e+01,9.46300920e+01,1.12091274e+02,\n",
    "                    1.30977804e+02,1.51221318e+02,1.72673905e+02,1.95087710e+02,\n",
    "                    2.18155935e+02,2.41600379e+02,2.65258515e+02,2.89122322e+02,\n",
    "                    3.13312087e+02,3.38006999e+02,3.63373492e+02,3.89523338e+02,\n",
    "                    4.16507922e+02,4.44331412e+02,4.72957206e+02,5.02291917e+02,\n",
    "                    5.32152273e+02,5.62239392e+02,5.92149276e+02,6.21432841e+02,\n",
    "                    6.49689897e+02,6.76656485e+02,7.02242188e+02,7.26498589e+02,\n",
    "                    7.49537645e+02,7.71445217e+02,7.92234260e+02,8.11856675e+02,\n",
    "                    8.30259643e+02,8.47450653e+02,8.63535902e+02,8.78715875e+02,\n",
    "                    8.93246018e+02,9.07385213e+02,9.21354397e+02,9.35316717e+02,\n",
    "                    9.49378056e+02,9.63599599e+02,9.78013432e+02,9.92635544e+02],dtype=np.float32)\n",
    "        self.refpres_norm = (self.refpres-self.refpres.min())/(self.refpres.max()-self.refpres.min())*2 - 1\n",
    "\n",
    "        if 'train' in self.filepath:\n",
    "            self.is_validation = False\n",
    "            print(\"Training dataset, path is: {}\".format(self.filepath))\n",
    "        else:\n",
    "            self.is_validation = True\n",
    "            print(\"Validation dataset, path is: {}\".format(self.filepath))\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.add_refpres = add_refpres\n",
    "        # batch_idx_expanded =  [0,1,2,3...ntime*1024]\n",
    "        hdf = h5py.File(self.filepath, 'r')\n",
    "        self.ntimesteps = hdf['input_lev'].shape[0]//self.nloc\n",
    "        hdf.close()\n",
    "        print(\"Number of locations {}; time steps {}\".format(self.nloc, self.ntimesteps))\n",
    "        # indices_all = list(np.arange(self.ntimesteps*self.nloc))\n",
    "        # chunksize_tot = self.nloc*self.chunk_size\n",
    "        # indices_chunked = self.chunkize(indices_all,chunksize_tot,False) \n",
    "        # self.hdf = h5py.File(self.filepath, 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ntimesteps*self.nloc\n",
    "    \n",
    "    def __getitem__(self, batch_indices):\n",
    "        hdf = h5py.File(self.filepath, 'r')\n",
    "        # hdf = self.hdf\n",
    "        \n",
    "        x_lay_b = hdf['input_lev'][batch_indices,:]\n",
    "        x_sfc_b = hdf['input_sca'][batch_indices,:]\n",
    "        y_lay_b = hdf['output_lev'][batch_indices,:]\n",
    "        y_sfc_b = hdf['output_sca'][batch_indices,:]\n",
    "        \n",
    "        if self.add_refpres:\n",
    "            dim0,dim1,dim2 = x_lay_b.shape\n",
    "            # if self.norm==\"minmax\":\n",
    "            refpres_norm = self.refpres_norm.reshape((1,-1,1))\n",
    "            refpres_norm = np.repeat(refpres_norm, dim0,axis=0)\n",
    "            #self.x[:,:,nx-1] = refpres_norm\n",
    "            x_lay_b = np.concatenate((x_lay_b, refpres_norm),axis=2)\n",
    "            # self.x  = torch.cat((self.x,refpres_norm),dim=3)\n",
    "            del refpres_norm \n",
    "\n",
    "        hdf.close()\n",
    "\n",
    "        x_lay_b = torch.from_numpy(x_lay_b)\n",
    "        x_sfc_b = torch.from_numpy(x_sfc_b)\n",
    "        y_lay_b = torch.from_numpy(y_lay_b)\n",
    "        y_sfc_b = torch.from_numpy(y_sfc_b)\n",
    "\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # x_lay_b = x_lay_b.to(device)\n",
    "        # x_sfc_b = x_sfc_b.to(device)\n",
    "        # y_lay_b = y_lay_b.to(device)\n",
    "        # sp = sp.to(device)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return x_lay_b, x_sfc_b, y_lay_b, y_sfc_b\n",
    "\n",
    "def chunkize(filelist, chunk_size, shuffle_before_chunking=False, shuffle_after_chunking=True):\n",
    "    import random\n",
    "    # Takes a list, shuffles it (optional), and divides into chunks of length n\n",
    "    # (no concept of batches within this function, chunk size is given in number of samples)\n",
    "    def divide(filelist,chunk_size):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(filelist), chunk_size): \n",
    "            yield filelist[i:i + chunk_size]  \n",
    "    if shuffle_before_chunking:\n",
    "        random.shuffle(filelist)\n",
    "        # we need the indices to be sorted within a chunk because these indices\n",
    "        # are used to index into the first dimension of a H5 file\n",
    "        for i in range(filelist):\n",
    "            filelist[i] = sorted(filelist[i])\n",
    "            \n",
    "    mylist = list(divide(filelist,chunk_size))\n",
    "    if shuffle_after_chunking:\n",
    "        random.shuffle(mylist)  \n",
    "    return mylist\n",
    "\n",
    "\n",
    "class BatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, num_samples_per_chunk, num_samples, shuffle=False):\n",
    "        self.num_samples_per_chunk = num_samples_per_chunk\n",
    "        self.num_samples = num_samples\n",
    "        indices_all = list(range(self.num_samples))\n",
    "        print(\"Shuffling the indices: {}\".format(shuffle))\n",
    "        self.indices_chunked = chunkize(indices_all,self.num_samples_per_chunk,\n",
    "                                        shuffle_before_chunking=False,\n",
    "                                        shuffle_after_chunking=shuffle)\n",
    "        #print(\"indices chunked [0]\", self.indices_chunked[0])\n",
    "        # one item is one chunk, consisting of chunk_factor*batch_size samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices_chunked)\n",
    "        # for batch in self.indices_chunked:\n",
    "        #     yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1725a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data = True \n",
    "\n",
    "train_locs = nloc = 384\n",
    "batch_size = train_locs \n",
    "\n",
    "# To improve IO, which is a bottleneck, increase the batch size by a factor of chunk_factor and \n",
    "# load this many batches at once. These chunks then need to be manually split into batches \n",
    "# within the data iteration loop    \n",
    "\n",
    "# chunk size in number of batches\n",
    "#chunk_size = 72 # one day (3 time steps in an hour, 72 in a day)\n",
    "#chunk_size = 360     \n",
    "chunk_size = 720 # 10 days\n",
    "# chunk size in number of elements\n",
    "num_samples_per_chunk = chunk_size*batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "795048a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset, path is: /network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim_data/ClimSim_low-res-expanded/train/preprocessed/train_y1-2.h5\n",
      "Number of locations 384; time steps 50324\n",
      "Shuffling the indices: True\n",
      "Training dataset, path is: /network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim_data/ClimSim_low-res-expanded/train/preprocessed/train_y5.h5\n",
      "Number of locations 384; time steps 26279\n",
      "Shuffling the indices: True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 2\n",
    "prefetch_factor = 1\n",
    "pin = False\n",
    "persistent=False\n",
    "\n",
    "\n",
    "train_data = generator_xy(tr_data_path, nloc=train_locs, add_refpres=add_refpres)\n",
    "\n",
    "train_batch_sampler = BatchSampler(num_samples_per_chunk, num_samples=train_data.__len__(), shuffle=shuffle_data)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, num_workers=num_workers, sampler=train_batch_sampler, \n",
    "                          batch_size=None,batch_sampler=None,prefetch_factor=prefetch_factor, \n",
    "                          pin_memory=pin, persistent_workers=persistent)\n",
    "\n",
    "if use_val:\n",
    "    \n",
    "    val_data = generator_xy(val_data_path, nloc=train_locs, add_refpres=add_refpres)\n",
    "\n",
    "    val_batch_sampler = BatchSampler(num_samples_per_chunk, num_samples=val_data.__len__(), shuffle=shuffle_data)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_data, num_workers=num_workers,sampler=val_batch_sampler,\n",
    "                            batch_size=None,batch_sampler=None,prefetch_factor=prefetch_factor, \n",
    "                            pin_memory=pin, persistent_workers=persistent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6e76580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9512"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78b54b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybi = torch.from_numpy(data.grid_info['hybi'].values).to(device)\n",
    "hyai = torch.from_numpy(data.grid_info['hyai'].values).to(device)\n",
    "sp_max = torch.from_numpy(data.input_max['state_ps'].values).to(device)\n",
    "sp_min = torch.from_numpy(data.input_min['state_ps'].values).to(device)\n",
    "sp_mean = torch.from_numpy(data.input_mean['state_ps'].values).to(device)\n",
    "\n",
    "def corrcoeff_pairs_batchfirst(A, B):\n",
    "    nb, nlev, nx = A.shape\n",
    "    # A and B are (N,M1,M2) vectors. \n",
    "    # Reshape to (N,M1*M2), then compute corrcoef for each (N,m),(N,m) pair\n",
    "    # reshape back at the end\n",
    "    A = A.reshape(nb,-1)\n",
    "    B = B.reshape(nb,-1)\n",
    "\n",
    "    A_mA = A - A.mean(0)[None,:]\n",
    "    B_mB = B - B.mean(0)[None,:] # (N, M)  \n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(0)\n",
    "    ssB = (B_mB**2).sum(0)\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    # dividend = np.dot(A_mA, B_mB.T) # (M,M)\n",
    "    dividend = np.sum(A_mA*B_mB, axis=0)  # (M)\n",
    "    \n",
    "    # divisor = np.sqrt(np.dot(ssA[:, None],ssB[None])) # (M, M)\n",
    "    divisor =  np.sqrt(ssA*ssB)\n",
    "    corrcoef = dividend / divisor\n",
    "    return corrcoef.reshape(nlev, nx)\n",
    "\n",
    "\n",
    "def my_mse(y_true_lay, y_true_sfc, y_pred_lay, y_pred_sfc):\n",
    "    mse1 = torch.mean(torch.square(y_pred_lay - y_true_lay))\n",
    "    mse2 = torch.mean(torch.square(y_pred_sfc - y_true_sfc))\n",
    "    return (mse1+mse2)/2\n",
    "\n",
    "def my_mse_flatten(y_true_lay, y_true_sfc, y_pred_lay, y_pred_sfc):\n",
    "\n",
    "    if len(y_true_lay.shape)==4: # autoregressive, time dimension included \n",
    "        y_pred_flat =  torch.cat(( y_pred_lay.flatten(start_dim=0,end_dim=1).flatten(start_dim=1) , y_pred_sfc.flatten(start_dim=0,end_dim=1) ),dim=1)\n",
    "        y_true_flat =  torch.cat(( y_true_lay.flatten(start_dim=0,end_dim=1).flatten(start_dim=1) , y_true_sfc.flatten(start_dim=0,end_dim=1) ),dim=1)\n",
    "    else:\n",
    "        y_pred_flat =  torch.cat((y_pred_lay.flatten(start_dim=1),y_pred_sfc),dim=1)\n",
    "        y_true_flat =  torch.cat((y_true_lay.flatten(start_dim=1),y_true_sfc),dim=1)\n",
    "\n",
    "    mse = torch.mean(torch.square(y_pred_flat - y_true_flat))\n",
    "    return mse\n",
    "\n",
    "def energy_metric(yto, ypo, sp, hyai,hybi):\n",
    "    \n",
    "    cp = torch.tensor(1004.0)\n",
    "    Lv = torch.tensor(2.5104e6)\n",
    "    one_over_grav = torch.tensor(0.1020408163) # 1/9.8\n",
    "    if len(yto.shape)==3:\n",
    "        thick= one_over_grav*(sp * (hybi[1:61].view(1,-1)-hybi[0:60].view(1,-1)) \n",
    "                             + torch.tensor(100000)*(hyai[1:61].view(1,-1)-hyai[0:60].view(1,-1)))\n",
    "    \n",
    "        dq_pred = ypo[:,:,0]\n",
    "        dT_pred = ypo[:,:,1] \n",
    "        dq_true = yto[:,:,0]\n",
    "        dT_true = yto[:,:,1]\n",
    "        \n",
    "        energy=torch.mean(torch.square(torch.sum(dq_pred*thick*(Lv)+dT_pred*thick*cp,1)\n",
    "                                -      torch.sum(dq_true*thick*(Lv)+dT_true*thick*cp,1)))\n",
    "    else: \n",
    "        # time dimension included\n",
    "                                       #      batch,time,1     (1,1,30)\n",
    "        thick= one_over_grav *(sp * (hybi[1:61].view(1,1,-1)-hybi[0:60].view(1,1,-1)) \n",
    "             + torch.tensor(100000)*(hyai[1:61].view(1,1,-1)-hyai[0:60].view(1,1,-1)))\n",
    "        dT_pred = ypo[:,:,:,0]\n",
    "        dq_pred = ypo[:,:,:,1] \n",
    "        \n",
    "        dT_true = yto[:,:,:,0]\n",
    "        dq_true = yto[:,:,:,1] \n",
    "        \n",
    "        energy=torch.mean(torch.square(torch.sum(dq_pred*thick*Lv + dT_pred*thick*cp,2)\n",
    "                                -      torch.sum(dq_true*thick*Lv + dT_true*thick*cp,2)))\n",
    "    return energy\n",
    "\n",
    "def get_energy_metric(hyai, hybi):\n",
    "    def energy(y_true, y_pred, sp):\n",
    "        return energy_metric(y_true, y_pred, sp, hyai, hybi)\n",
    "    return energy\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    mse = torch.mean(torch.square(y_pred- y_true))\n",
    "    return mse\n",
    "\n",
    "\n",
    "def loss_con(y_true_norm, y_pred_norm, y_true, y_pred, sp, _lambda):\n",
    "\n",
    "    energy = energy_metric(y_true, y_pred, sp, hyai,hybi)\n",
    "    #mse = torch.mean(torch.square(y_pred- y_true))\n",
    "    mse = my_mse_flatten(y_true_norm, y_pred_norm)\n",
    "    loss = mse + _lambda*energy\n",
    "    return loss, energy, mse\n",
    "\n",
    "def get_loss_con(hyai, hybi, _lambda, denorm_func):\n",
    "    def hybrid_loss(y_true_norm, y_pred_norm, y_true, y_pred, sp):\n",
    "        return loss_con(y_true_norm, y_pred_norm, y_true, y_pred, sp, _lambda)\n",
    "    return hybrid_loss\n",
    "\n",
    "def my_hybrid_loss(mse, energy, _lambda):\n",
    "    loss = mse + _lambda*energy\n",
    "    return loss \n",
    "\n",
    "def get_hybrid_loss(_lambda):\n",
    "    def hybrid_loss(mse, energy):\n",
    "        return my_hybrid_loss(mse, energy, _lambda)\n",
    "    return hybrid_loss\n",
    "\n",
    "metric_h_con = get_energy_metric(hyai, hybi)\n",
    "#loss_fn = my_mse_flatten\n",
    "_lambda = torch.tensor(1.0e-7) \n",
    "_lambda = torch.tensor(1.0e-6) \n",
    "\n",
    "loss_fn = get_hybrid_loss(_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ec5df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fa03d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.regression import R2Score\n",
    "import time\n",
    "\n",
    "if not autoregressive:\n",
    "    timewindow = 1\n",
    "    timestep_scheduling=False\n",
    "else:\n",
    "    timewindow = 3\n",
    "    timestep_scheduling=True\n",
    "    timestep_schedule = np.arange(1000)\n",
    "    timestep_schedule[:] = timewindow\n",
    "\n",
    "    if timestep_scheduling:\n",
    "        timestep_schedule[0:3] = 1\n",
    "        timestep_schedule[3:4] = timewindow-1\n",
    "        timestep_schedule[4:] = timewindow\n",
    "        timestep_schedule[5:] = timewindow+1\n",
    "        timestep_schedule[6:] = timewindow+2\n",
    "\n",
    "    \n",
    "use_wandb = False\n",
    "\n",
    "class model_train_eval:\n",
    "    def __init__(self, dataloader, model, autoregressive=True, train=True):\n",
    "        super().__init__()\n",
    "        self.loader = dataloader\n",
    "        self.train = train\n",
    "        self.report_freq = 800\n",
    "        self.model = model \n",
    "        self.autoregressive = autoregressive\n",
    "        if self.autoregressive:\n",
    "            self.model.reset_states()\n",
    "        self.metric_R2 =  R2Score().to(device) \n",
    "        self.metric_R2_heating =  R2Score().to(device) \n",
    "        self.metric_R2_precc =  R2Score().to(device) \n",
    "        self.metric_R2_moistening =  R2Score().to(device) \n",
    "\n",
    "        self.metrics= {'loss': 0, 'mean_squared_error': 0,  # the latter is just MSE\n",
    "                        'mean_absolute_error': 0, 'R2' : 0, 'R2_heating' : 0,'R2_moistening' : 0,  \n",
    "                        'R2_precc' : 0, 'R2_lev' : np.zeros((nlev,ny)),\n",
    "                        'h_conservation' : 0 }\n",
    "\n",
    "    def eval_one_epoch(self, epoch, timewindow=1):\n",
    "        report_freq = self.report_freq\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0\n",
    "        epoch_mse = 0.0; epoch_mae = 0.0\n",
    "        epoch_R2precc = 0.0\n",
    "        epoch_hcon = 0.0\n",
    "        epoch_r2_lev = 0.0\n",
    "        t_comp =0 \n",
    "        if self.autoregressive:\n",
    "            preds_lay = []; preds_sfc = []\n",
    "            targets_lay = []; targets_sfc = [] \n",
    "            sps = []\n",
    "        t0_it = time.time()\n",
    "        j = 0; k = 0; k2=2    \n",
    "        if self.autoregressive:\n",
    "            loss_update_start_index = 60\n",
    "        else:\n",
    "            loss_update_start_index = 0\n",
    "        for i,data in enumerate(self.loader):\n",
    "            inputs_lay_chunks, inputs_sfc_chunks, targets_lay_chunks, targets_sfc_chunks = data\n",
    "            inputs_lay_chunks   = inputs_lay_chunks.to(device)\n",
    "            inputs_sfc_chunks   = inputs_sfc_chunks.to(device)\n",
    "            targets_sfc_chunks  = targets_sfc_chunks.to(device)\n",
    "            targets_lay_chunks  = targets_lay_chunks.to(device)\n",
    "            \n",
    "            inputs_lay_chunks    = torch.split(inputs_lay_chunks, batch_size)\n",
    "            inputs_sfc_chunks    = torch.split(inputs_sfc_chunks, batch_size)\n",
    "            targets_sfc_chunks   = torch.split(targets_sfc_chunks, batch_size)\n",
    "            targets_lay_chunks   = torch.split(targets_lay_chunks, batch_size)\n",
    "         \n",
    "            # to speed-up IO, we loaded chunks=many batches, which now need to be divided into batches\n",
    "            for ichunk in range(len(inputs_lay_chunks)):\n",
    "                inputs_lay = inputs_lay_chunks[ichunk]\n",
    "                inputs_sfc = inputs_sfc_chunks[ichunk]\n",
    "                target_lay = targets_lay_chunks[ichunk]\n",
    "                target_sfc = targets_sfc_chunks[ichunk]\n",
    "                sp = inputs_sfc[:,0:1] # surface pressure\n",
    "\n",
    "\n",
    "                tcomp0= time.time()\n",
    "                    \n",
    "                if mp_autocast:\n",
    "                    with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                        pred_lay, pred_sfc = self.model(inputs_lay, inputs_sfc)\n",
    "                else:\n",
    "                    pred_lay, pred_sfc = self.model(inputs_lay, inputs_sfc)\n",
    "                    \n",
    "                if self.autoregressive:\n",
    "                    # In the autoregressive training case are gathering many time steps before computing loss\n",
    "                    preds_lay.append(pred_lay)\n",
    "                    preds_sfc.append(pred_sfc)\n",
    "                    targets_lay.append(target_lay)\n",
    "                    targets_sfc.append(target_sfc)\n",
    "                    sps.append(sp) \n",
    "                    \n",
    "                else:\n",
    "                    preds_lay = pred_lay\n",
    "                    preds_sfc = pred_sfc \n",
    "                    targets_lay = target_lay\n",
    "                    targets_sfc = target_sfc\n",
    "                    sps = sp\n",
    "                    \n",
    "                if (not self.autoregressive) or (self.autoregressive and (j+1) % timewindow==0):\n",
    "            \n",
    "                    if self.autoregressive:\n",
    "                        preds_lay   = torch.stack(preds_lay)\n",
    "                        preds_sfc   = torch.stack(preds_sfc)\n",
    "                        targets_lay = torch.stack(targets_lay)\n",
    "                        targets_sfc = torch.stack(targets_sfc)\n",
    "                        sps         = torch.stack(sps)\n",
    "                                \n",
    "                    if mp_autocast:\n",
    "                        with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                            #loss = loss_fn(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                            \n",
    "                            mse = my_mse_flatten(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                            \n",
    "                            ypo_lay, ypo_sfc = model.postprocessing(preds_lay, preds_sfc)\n",
    "                            yto_lay, yto_sfc = model.postprocessing(targets_lay, targets_sfc)\n",
    "                            sps_denorm = sp = sps*(sp_max - sp_min) + sp_mean\n",
    "                            h_con = metric_h_con(yto_lay, ypo_lay, sps_denorm)\n",
    "                            \n",
    "                            loss = loss_fn(mse, h_con)\n",
    "                    else:\n",
    "                        #loss = loss_fn(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                        \n",
    "                        mse = my_mse_flatten(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                        ypo_lay, ypo_sfc = model.postprocessing(preds_lay, preds_sfc)\n",
    "                        yto_lay, yto_sfc = model.postprocessing(targets_lay, targets_sfc)\n",
    "                        sps_denorm = sp = sps*(sp_max - sp_min) + sp_mean\n",
    "                        h_con = metric_h_con(yto_lay, ypo_lay, sps_denorm)\n",
    "                        loss = loss_fn(mse, h_con)\n",
    "                        \n",
    "                    if self.train:\n",
    "                        if use_scaler:\n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            loss.backward()       \n",
    "                            optimizer.step()\n",
    "            \n",
    "                        optimizer.zero_grad()\n",
    "                            \n",
    "                    running_loss    += loss.item()\n",
    "                    #mae             = metrics.mean_absolute_error(targets_lay, preds_lay)\n",
    "                    if j>loss_update_start_index:\n",
    "                        with torch.no_grad():\n",
    "                            epoch_loss      += loss.item()\n",
    "                            #epoch_energy    += energy.item()\n",
    "                            epoch_mse       += mse.item()\n",
    "                            #epoch_mae       += mae.item()\n",
    "                        \n",
    "                           # yto, ypo =  denorm_func(targets_lay, preds_lay)\n",
    "                            # -------------- TO-DO:  DE-NORM OUTPUT --------------\n",
    "                            #yto, ypo = targets_lay, preds_lay\n",
    "\n",
    "                            epoch_hcon  += h_con.item()\n",
    "                            \n",
    "                            self.metric_R2.update(ypo_lay.reshape((-1,ny)), yto_lay.reshape((-1,ny)))\n",
    "                            self.metric_R2_heating.update(ypo_lay[:,:,0].reshape(-1,1), yto_lay[:,:,0].reshape(-1,1))\n",
    "                            self.metric_R2_moistening.update(ypo_lay[:,:,1].reshape(-1,1), yto_lay[:,:,1].reshape(-1,1))\n",
    "\n",
    "                            self.metric_R2_precc.update(ypo_sfc[:,3].reshape(-1,1), yto_sfc[:,3].reshape(-1,1))\n",
    "                            \n",
    "                            r2_np = np.corrcoef((ypo_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy(),yto_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy()))[0,1]\n",
    "                            epoch_R2precc += r2_np\n",
    "                            #print(\"R2 numpy\", r2_np, \"R2 torch\", self.metric_R2_precc(ypo_sfc[:,3:4], yto_sfc[:,3:4]) )\n",
    "\n",
    "                            ypo_lay = ypo_lay.reshape(-1,nlev,ny).detach().cpu().numpy()\n",
    "                            yto_lay = yto_lay.reshape(-1,nlev,ny).detach().cpu().numpy()\n",
    "\n",
    "                            epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay) \n",
    "                           # if track_ks:\n",
    "                           #     if (j+1) % max(timewindow*4,12)==0:\n",
    "                           #         epoch_ks += kolmogorov_smirnov(yto,ypo).item()\n",
    "                           #         k2 += 1\n",
    "                            k += 1\n",
    "                    if self.autoregressive:\n",
    "                        preds_lay = []; preds_sfc = []\n",
    "                        targets_lay = []; targets_sfc = [] \n",
    "                        sps = []\n",
    "                    if self.autoregressive: \n",
    "                        model.detach_states()\n",
    "                \n",
    "                t_comp += time.time() - tcomp0\n",
    "                # # print statistics \n",
    "                if j % report_freq == (report_freq-1): # print every 200 minibatches\n",
    "                    elaps = time.time() - t0_it\n",
    "                    running_loss = running_loss / (report_freq/timewindow)\n",
    "                    #running_energy = running_energy / (report_freq/timewindow)\n",
    "                    r2raw = self.metric_R2.compute()\n",
    "                    #r2raw_prec = self.metric_R2_precc.compute()\n",
    "\n",
    "                    #ypo_lay, ypo_sfc = model.postprocessing(preds_lay, preds_sfc)\n",
    "                    #yto_lay, yto_sfc = model.postprocessing(targets_lay, targets_sfc) \n",
    "                    #r2_np = np.corrcoef((ypo_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy(),yto_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy()))[0,1]\n",
    "\n",
    "                   # print(torch.mean(ypo_sfc[:,3] -  yto_sfc[:,3]))\n",
    "                   # print(torch.mean(preds_sfc[:,3] - targets_sfc[:,3]))\n",
    "                    \n",
    "\n",
    "                    print(\"[{:d}, {:d}] Loss: {:.2e}  runR2: {:.2f},  elapsed {:.1f}s (compute {:.1f})\" .format(epoch + 1, \n",
    "                                                    j+1, running_loss, r2raw, elaps, t_comp))\n",
    "                    running_loss = 0.0\n",
    "                    running_energy = 0.0\n",
    "                    t0_it = time.time()\n",
    "                    t_comp = 0\n",
    "                j += 1\n",
    "\n",
    "        self.metrics['loss'] =  epoch_loss / k\n",
    "        self.metrics['mean_squared_error'] = epoch_mse / k\n",
    "        self.metrics[\"h_conservation\"] =  epoch_hcon / k\n",
    "\n",
    "        #self.metrics['energymetric'] = epoch_energy / k\n",
    "        #self.metrics['mean_absolute_error'] = epoch_mae / k\n",
    "        #self.metrics['ks'] =  epoch_ks / k2\n",
    "        self.metrics['R2'] = self.metric_R2.compute()\n",
    "        self.metrics['R2_heating'] = self.metric_R2_heating.compute()\n",
    "        self.metrics['R2_moistening'] = self.metric_R2_moistening.compute()\n",
    "\n",
    "        #self.metrics['R2_precc'] = self.metric_R2_precc.compute()\n",
    "        self.metrics['R2_precc'] = epoch_R2precc / k\n",
    "        \n",
    "        self.metrics['R2_lev'] = epoch_r2_lev / k\n",
    "\n",
    "        self.metric_R2.reset(); self.metric_R2_heating.reset(); self.metric_R2_precc.reset()\n",
    "        if self.autoregressive:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "        datatype = \"TRAIN\" if self.train else \"VAL\"\n",
    "        print('Epoch {} {} loss: {:.2e}  MSE: {:.2e}  h-con:  {:.2e}   R2: {:.2f}  R2-dT/dt: {:.2f}   R2-dq/dt: {:.2f}   R2-precc: {:.3f}'.format(epoch+1, datatype, \n",
    "                                                            self.metrics['loss'], \n",
    "                                                            self.metrics['mean_squared_error'], \n",
    "                                                            self.metrics['h_conservation'],\n",
    "                                                            self.metrics['R2'],\n",
    "                                                            self.metrics['R2_heating'],\n",
    "                                                            self.metrics['R2_moistening'],                                                              \n",
    "                                                            self.metrics['R2_precc'] ))\n",
    "\n",
    "    if cuda: torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f48c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n",
      "[1, 800] Loss: 2.69e-03  runR2: 0.31, runR2Prec: 0.76,  elapsed 13.9s (compute 4.9)\n",
      "[1, 1600] Loss: 1.82e-03  runR2: 0.41, runR2Prec: 0.81,  elapsed 6.2s (compute 5.7)\n",
      "[1, 2400] Loss: 1.50e-03  runR2: 0.46, runR2Prec: 0.88,  elapsed 5.9s (compute 5.6)\n",
      "[1, 3200] Loss: 1.54e-03  runR2: 0.48, runR2Prec: 0.89,  elapsed 4.8s (compute 4.5)\n",
      "[1, 4000] Loss: 1.56e-03  runR2: 0.51, runR2Prec: 0.89,  elapsed 6.5s (compute 4.7)\n",
      "[1, 4800] Loss: 1.48e-03  runR2: 0.53, runR2Prec: 0.88,  elapsed 5.5s (compute 5.1)\n",
      "[1, 5600] Loss: 1.38e-03  runR2: 0.54, runR2Prec: 0.84,  elapsed 5.4s (compute 5.1)\n",
      "[1, 6400] Loss: 1.40e-03  runR2: 0.55, runR2Prec: 0.89,  elapsed 5.9s (compute 4.8)\n",
      "[1, 7200] Loss: 1.22e-03  runR2: 0.56, runR2Prec: 0.86,  elapsed 5.4s (compute 5.0)\n",
      "[1, 8000] Loss: 1.19e-03  runR2: 0.57, runR2Prec: 0.85,  elapsed 6.7s (compute 4.9)\n",
      "[1, 8800] Loss: 1.25e-03  runR2: 0.58, runR2Prec: 0.88,  elapsed 6.4s (compute 5.0)\n",
      "[1, 9600] Loss: 1.41e-03  runR2: 0.59, runR2Prec: 0.92,  elapsed 5.5s (compute 5.1)\n",
      "[1, 10400] Loss: 1.35e-03  runR2: 0.59, runR2Prec: 0.85,  elapsed 5.8s (compute 4.9)\n",
      "[1, 11200] Loss: 1.36e-03  runR2: 0.60, runR2Prec: 0.87,  elapsed 5.4s (compute 4.6)\n",
      "[1, 12000] Loss: 1.30e-03  runR2: 0.61, runR2Prec: 0.91,  elapsed 7.3s (compute 5.0)\n",
      "[1, 12800] Loss: 1.23e-03  runR2: 0.61, runR2Prec: 0.88,  elapsed 5.0s (compute 4.7)\n",
      "[1, 13600] Loss: 1.09e-03  runR2: 0.61, runR2Prec: 0.88,  elapsed 7.6s (compute 5.0)\n",
      "[1, 14400] Loss: 1.07e-03  runR2: 0.62, runR2Prec: 0.97,  elapsed 5.8s (compute 5.4)\n",
      "[1, 15200] Loss: 1.37e-03  runR2: 0.62, runR2Prec: 0.90,  elapsed 7.0s (compute 5.0)\n",
      "[1, 16000] Loss: 1.21e-03  runR2: 0.62, runR2Prec: 0.87,  elapsed 5.8s (compute 4.5)\n",
      "[1, 16800] Loss: 1.19e-03  runR2: 0.62, runR2Prec: 0.92,  elapsed 6.8s (compute 5.3)\n",
      "[1, 17600] Loss: 1.16e-03  runR2: 0.62, runR2Prec: 0.90,  elapsed 8.3s (compute 5.3)\n",
      "[1, 18400] Loss: 1.17e-03  runR2: 0.63, runR2Prec: 0.94,  elapsed 6.5s (compute 5.1)\n",
      "[1, 19200] Loss: 1.18e-03  runR2: 0.63, runR2Prec: 0.90,  elapsed 5.8s (compute 4.6)\n",
      "[1, 20000] Loss: 1.38e-03  runR2: 0.63, runR2Prec: 0.93,  elapsed 6.7s (compute 4.9)\n",
      "[1, 20800] Loss: 1.22e-03  runR2: 0.63, runR2Prec: 0.88,  elapsed 5.5s (compute 4.9)\n",
      "[1, 21600] Loss: 1.16e-03  runR2: 0.63, runR2Prec: 0.92,  elapsed 6.7s (compute 4.7)\n",
      "[1, 22400] Loss: 1.17e-03  runR2: 0.63, runR2Prec: 0.89,  elapsed 4.9s (compute 4.7)\n",
      "[1, 23200] Loss: 1.17e-03  runR2: 0.64, runR2Prec: 0.80,  elapsed 5.7s (compute 4.5)\n",
      "[1, 24000] Loss: 1.17e-03  runR2: 0.64, runR2Prec: 0.96,  elapsed 5.5s (compute 4.3)\n",
      "Epoch 1 TRAIN loss: 1.35e-03  MSE: 1.35e-03  R2: 0.64  R2-dT/dt: 0.56   R2-precc: 0.883\n",
      "Epoch 1/5 complete, took 190.93 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.16e-03  runR2: 0.65, runR2Prec: 0.96,  elapsed 14.5s (compute 4.9)\n",
      "[2, 1600] Loss: 1.18e-03  runR2: 0.66, runR2Prec: 0.90,  elapsed 5.6s (compute 5.2)\n",
      "[2, 2400] Loss: 1.02e-03  runR2: 0.66, runR2Prec: 0.95,  elapsed 5.6s (compute 5.2)\n",
      "[2, 3200] Loss: 1.13e-03  runR2: 0.67, runR2Prec: 0.89,  elapsed 4.7s (compute 4.4)\n",
      "[2, 4000] Loss: 1.21e-03  runR2: 0.67, runR2Prec: 0.93,  elapsed 7.2s (compute 4.9)\n",
      "[2, 4800] Loss: 1.25e-03  runR2: 0.67, runR2Prec: 0.93,  elapsed 5.5s (compute 5.1)\n",
      "[2, 5600] Loss: 1.18e-03  runR2: 0.67, runR2Prec: 0.90,  elapsed 5.2s (compute 4.9)\n",
      "[2, 6400] Loss: 1.22e-03  runR2: 0.67, runR2Prec: 0.91,  elapsed 6.5s (compute 4.8)\n",
      "[2, 7200] Loss: 1.06e-03  runR2: 0.67, runR2Prec: 0.92,  elapsed 5.5s (compute 5.1)\n",
      "[2, 8000] Loss: 1.05e-03  runR2: 0.67, runR2Prec: 0.89,  elapsed 5.6s (compute 4.7)\n",
      "[2, 8800] Loss: 1.12e-03  runR2: 0.67, runR2Prec: 0.93,  elapsed 5.5s (compute 5.0)\n",
      "[2, 9600] Loss: 1.27e-03  runR2: 0.67, runR2Prec: 0.96,  elapsed 5.6s (compute 5.2)\n",
      "[2, 10400] Loss: 1.22e-03  runR2: 0.67, runR2Prec: 0.90,  elapsed 5.5s (compute 5.1)\n",
      "[2, 11200] Loss: 1.25e-03  runR2: 0.67, runR2Prec: 0.88,  elapsed 5.6s (compute 5.2)\n",
      "[2, 12000] Loss: 1.19e-03  runR2: 0.68, runR2Prec: 0.92,  elapsed 6.5s (compute 6.2)\n",
      "[2, 12800] Loss: 1.12e-03  runR2: 0.68, runR2Prec: 0.89,  elapsed 5.3s (compute 4.9)\n",
      "[2, 13600] Loss: 9.99e-04  runR2: 0.68, runR2Prec: 0.90,  elapsed 5.2s (compute 4.9)\n",
      "[2, 14400] Loss: 9.87e-04  runR2: 0.68, runR2Prec: 0.97,  elapsed 5.5s (compute 5.1)\n",
      "[2, 15200] Loss: 1.27e-03  runR2: 0.68, runR2Prec: 0.93,  elapsed 6.0s (compute 5.3)\n",
      "[2, 16000] Loss: 1.13e-03  runR2: 0.68, runR2Prec: 0.89,  elapsed 5.6s (compute 5.3)\n",
      "[2, 16800] Loss: 1.12e-03  runR2: 0.68, runR2Prec: 0.96,  elapsed 5.8s (compute 5.4)\n",
      "[2, 17600] Loss: 1.09e-03  runR2: 0.68, runR2Prec: 0.95,  elapsed 7.4s (compute 4.9)\n",
      "[2, 18400] Loss: 1.10e-03  runR2: 0.68, runR2Prec: 0.95,  elapsed 7.2s (compute 4.8)\n",
      "[2, 19200] Loss: 1.11e-03  runR2: 0.68, runR2Prec: 0.92,  elapsed 5.6s (compute 5.3)\n",
      "[2, 20000] Loss: 1.31e-03  runR2: 0.68, runR2Prec: 0.95,  elapsed 6.4s (compute 6.0)\n",
      "[2, 20800] Loss: 1.15e-03  runR2: 0.68, runR2Prec: 0.90,  elapsed 5.4s (compute 5.0)\n",
      "[2, 21600] Loss: 1.10e-03  runR2: 0.68, runR2Prec: 0.93,  elapsed 6.8s (compute 5.0)\n",
      "[2, 22400] Loss: 1.11e-03  runR2: 0.68, runR2Prec: 0.91,  elapsed 6.2s (compute 5.5)\n",
      "[2, 23200] Loss: 1.10e-03  runR2: 0.68, runR2Prec: 0.85,  elapsed 5.3s (compute 5.0)\n",
      "[2, 24000] Loss: 1.12e-03  runR2: 0.68, runR2Prec: 0.97,  elapsed 5.2s (compute 4.5)\n",
      "Epoch 2 TRAIN loss: 1.14e-03  MSE: 1.14e-03  R2: 0.68  R2-dT/dt: 0.67   R2-precc: 0.928\n",
      "Epoch 2/5 complete, took 183.77 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.11e-03  runR2: 0.67, runR2Prec: 0.97,  elapsed 14.0s (compute 5.0)\n",
      "[3, 1600] Loss: 1.13e-03  runR2: 0.68, runR2Prec: 0.92,  elapsed 5.4s (compute 5.0)\n",
      "[3, 2400] Loss: 9.71e-04  runR2: 0.68, runR2Prec: 0.96,  elapsed 5.3s (compute 5.0)\n",
      "[3, 3200] Loss: 1.09e-03  runR2: 0.69, runR2Prec: 0.88,  elapsed 5.4s (compute 5.0)\n",
      "[3, 4000] Loss: 1.17e-03  runR2: 0.69, runR2Prec: 0.94,  elapsed 5.1s (compute 4.8)\n",
      "[3, 4800] Loss: 1.21e-03  runR2: 0.69, runR2Prec: 0.94,  elapsed 5.4s (compute 4.9)\n",
      "[3, 5600] Loss: 1.14e-03  runR2: 0.69, runR2Prec: 0.93,  elapsed 5.7s (compute 5.3)\n",
      "[3, 6400] Loss: 1.18e-03  runR2: 0.69, runR2Prec: 0.94,  elapsed 5.7s (compute 5.4)\n",
      "[3, 7200] Loss: 1.02e-03  runR2: 0.69, runR2Prec: 0.93,  elapsed 5.1s (compute 4.8)\n",
      "[3, 8000] Loss: 1.01e-03  runR2: 0.69, runR2Prec: 0.92,  elapsed 6.2s (compute 5.2)\n",
      "[3, 8800] Loss: 1.08e-03  runR2: 0.70, runR2Prec: 0.95,  elapsed 5.8s (compute 5.4)\n",
      "[3, 9600] Loss: 1.23e-03  runR2: 0.70, runR2Prec: 0.96,  elapsed 5.7s (compute 5.4)\n",
      "[3, 10400] Loss: 1.19e-03  runR2: 0.70, runR2Prec: 0.90,  elapsed 5.9s (compute 5.5)\n",
      "[3, 11200] Loss: 1.21e-03  runR2: 0.70, runR2Prec: 0.90,  elapsed 5.7s (compute 5.4)\n",
      "[3, 12000] Loss: 1.16e-03  runR2: 0.70, runR2Prec: 0.94,  elapsed 5.5s (compute 5.2)\n",
      "[3, 12800] Loss: 1.09e-03  runR2: 0.70, runR2Prec: 0.89,  elapsed 5.2s (compute 4.9)\n",
      "[3, 13600] Loss: 9.72e-04  runR2: 0.70, runR2Prec: 0.90,  elapsed 5.5s (compute 5.1)\n",
      "[3, 14400] Loss: 9.60e-04  runR2: 0.70, runR2Prec: 0.97,  elapsed 5.6s (compute 5.3)\n",
      "[3, 15200] Loss: 1.24e-03  runR2: 0.70, runR2Prec: 0.93,  elapsed 5.9s (compute 5.2)\n",
      "[3, 16000] Loss: 1.10e-03  runR2: 0.70, runR2Prec: 0.91,  elapsed 5.6s (compute 5.3)\n",
      "[3, 16800] Loss: 1.09e-03  runR2: 0.70, runR2Prec: 0.97,  elapsed 5.4s (compute 5.1)\n",
      "[3, 17600] Loss: 1.06e-03  runR2: 0.70, runR2Prec: 0.95,  elapsed 6.9s (compute 5.0)\n",
      "[3, 18400] Loss: 1.07e-03  runR2: 0.70, runR2Prec: 0.95,  elapsed 6.5s (compute 4.7)\n",
      "[3, 19200] Loss: 1.09e-03  runR2: 0.70, runR2Prec: 0.93,  elapsed 5.0s (compute 4.7)\n",
      "[3, 20000] Loss: 1.29e-03  runR2: 0.70, runR2Prec: 0.95,  elapsed 5.0s (compute 4.7)\n",
      "[3, 20800] Loss: 1.13e-03  runR2: 0.70, runR2Prec: 0.91,  elapsed 5.2s (compute 4.7)\n",
      "[3, 21600] Loss: 1.07e-03  runR2: 0.70, runR2Prec: 0.93,  elapsed 5.0s (compute 4.7)\n",
      "[3, 22400] Loss: 1.08e-03  runR2: 0.70, runR2Prec: 0.92,  elapsed 5.2s (compute 4.7)\n",
      "[3, 23200] Loss: 1.08e-03  runR2: 0.70, runR2Prec: 0.88,  elapsed 5.0s (compute 4.7)\n",
      "[3, 24000] Loss: 1.10e-03  runR2: 0.70, runR2Prec: 0.97,  elapsed 5.1s (compute 4.7)\n",
      "Epoch 3 TRAIN loss: 1.11e-03  MSE: 1.11e-03  R2: 0.70  R2-dT/dt: 0.69   R2-precc: 0.935\n",
      "Epoch 3/5 complete, took 174.42 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 1 \n",
      "[4, 800] Loss: 1.09e-03  runR2: 0.69, runR2Prec: 0.97,  elapsed 14.1s (compute 5.2)\n",
      "[4, 1600] Loss: 1.11e-03  runR2: 0.69, runR2Prec: 0.92,  elapsed 5.4s (compute 5.1)\n",
      "[4, 2400] Loss: 9.53e-04  runR2: 0.70, runR2Prec: 0.96,  elapsed 5.6s (compute 5.2)\n",
      "[4, 3200] Loss: 1.07e-03  runR2: 0.70, runR2Prec: 0.89,  elapsed 5.5s (compute 5.1)\n",
      "[4, 4000] Loss: 1.15e-03  runR2: 0.70, runR2Prec: 0.94,  elapsed 5.6s (compute 5.2)\n",
      "[4, 4800] Loss: 1.19e-03  runR2: 0.70, runR2Prec: 0.94,  elapsed 5.6s (compute 5.2)\n",
      "[4, 5600] Loss: 1.12e-03  runR2: 0.70, runR2Prec: 0.94,  elapsed 5.6s (compute 5.2)\n",
      "[4, 6400] Loss: 1.16e-03  runR2: 0.70, runR2Prec: 0.94,  elapsed 5.5s (compute 5.2)\n",
      "[4, 7200] Loss: 1.00e-03  runR2: 0.70, runR2Prec: 0.94,  elapsed 5.3s (compute 5.0)\n",
      "[4, 8000] Loss: 9.96e-04  runR2: 0.71, runR2Prec: 0.93,  elapsed 5.9s (compute 5.2)\n",
      "[4, 8800] Loss: 1.06e-03  runR2: 0.71, runR2Prec: 0.96,  elapsed 5.4s (compute 5.1)\n",
      "[4, 9600] Loss: 1.22e-03  runR2: 0.71, runR2Prec: 0.96,  elapsed 5.6s (compute 5.2)\n",
      "[4, 10400] Loss: 1.17e-03  runR2: 0.71, runR2Prec: 0.90,  elapsed 5.7s (compute 5.0)\n",
      "[4, 11200] Loss: 1.19e-03  runR2: 0.71, runR2Prec: 0.91,  elapsed 5.1s (compute 4.8)\n",
      "[4, 12000] Loss: 1.14e-03  runR2: 0.71, runR2Prec: 0.95,  elapsed 5.8s (compute 5.0)\n",
      "[4, 12800] Loss: 1.08e-03  runR2: 0.71, runR2Prec: 0.90,  elapsed 5.6s (compute 5.2)\n",
      "[4, 13600] Loss: 9.58e-04  runR2: 0.71, runR2Prec: 0.90,  elapsed 5.4s (compute 5.1)\n",
      "[4, 14400] Loss: 9.47e-04  runR2: 0.71, runR2Prec: 0.98,  elapsed 5.4s (compute 5.1)\n",
      "[4, 15200] Loss: 1.23e-03  runR2: 0.71, runR2Prec: 0.93,  elapsed 5.7s (compute 5.0)\n",
      "[4, 16000] Loss: 1.08e-03  runR2: 0.72, runR2Prec: 0.91,  elapsed 5.5s (compute 5.2)\n",
      "[4, 16800] Loss: 1.08e-03  runR2: 0.72, runR2Prec: 0.96,  elapsed 5.7s (compute 4.9)\n",
      "[4, 17600] Loss: 1.05e-03  runR2: 0.72, runR2Prec: 0.95,  elapsed 6.8s (compute 5.2)\n",
      "[4, 18400] Loss: 1.06e-03  runR2: 0.72, runR2Prec: 0.95,  elapsed 6.5s (compute 5.1)\n",
      "[4, 19200] Loss: 1.07e-03  runR2: 0.72, runR2Prec: 0.93,  elapsed 4.7s (compute 4.3)\n",
      "[4, 20000] Loss: 1.27e-03  runR2: 0.72, runR2Prec: 0.95,  elapsed 6.3s (compute 4.8)\n",
      "[4, 20800] Loss: 1.12e-03  runR2: 0.72, runR2Prec: 0.91,  elapsed 5.6s (compute 5.2)\n",
      "[4, 21600] Loss: 1.06e-03  runR2: 0.72, runR2Prec: 0.94,  elapsed 6.0s (compute 5.7)\n",
      "[4, 22400] Loss: 1.07e-03  runR2: 0.72, runR2Prec: 0.92,  elapsed 5.7s (compute 5.3)\n",
      "[4, 23200] Loss: 1.07e-03  runR2: 0.72, runR2Prec: 0.89,  elapsed 5.4s (compute 5.1)\n",
      "[4, 24000] Loss: 1.09e-03  runR2: 0.72, runR2Prec: 0.97,  elapsed 4.5s (compute 4.1)\n",
      "Epoch 4 TRAIN loss: 1.09e-03  MSE: 1.09e-03  R2: 0.72  R2-dT/dt: 0.69   R2-precc: 0.938\n",
      "Epoch 4/5 complete, took 177.08 seconds, autoreg window was 1\n",
      "Epoch 5 Training rollout timesteps: 1 \n",
      "[5, 800] Loss: 1.08e-03  runR2: 0.70, runR2Prec: 0.97,  elapsed 13.7s (compute 4.8)\n",
      "[5, 1600] Loss: 1.10e-03  runR2: 0.71, runR2Prec: 0.93,  elapsed 5.3s (compute 4.8)\n",
      "[5, 2400] Loss: 9.41e-04  runR2: 0.71, runR2Prec: 0.96,  elapsed 5.1s (compute 4.7)\n",
      "[5, 3200] Loss: 1.05e-03  runR2: 0.72, runR2Prec: 0.89,  elapsed 5.3s (compute 4.7)\n",
      "[5, 4000] Loss: 1.14e-03  runR2: 0.72, runR2Prec: 0.94,  elapsed 5.2s (compute 4.9)\n",
      "[5, 4800] Loss: 1.18e-03  runR2: 0.72, runR2Prec: 0.95,  elapsed 4.9s (compute 4.5)\n",
      "[5, 5600] Loss: 1.11e-03  runR2: 0.72, runR2Prec: 0.95,  elapsed 6.2s (compute 4.9)\n",
      "[5, 6400] Loss: 1.15e-03  runR2: 0.72, runR2Prec: 0.94,  elapsed 5.0s (compute 4.7)\n",
      "[5, 7200] Loss: 9.91e-04  runR2: 0.72, runR2Prec: 0.94,  elapsed 5.4s (compute 5.0)\n",
      "[5, 8000] Loss: 9.86e-04  runR2: 0.72, runR2Prec: 0.94,  elapsed 5.7s (compute 4.7)\n",
      "[5, 8800] Loss: 1.05e-03  runR2: 0.72, runR2Prec: 0.96,  elapsed 5.0s (compute 4.6)\n",
      "[5, 9600] Loss: 1.20e-03  runR2: 0.72, runR2Prec: 0.96,  elapsed 5.4s (compute 5.0)\n",
      "[5, 10400] Loss: 1.16e-03  runR2: 0.72, runR2Prec: 0.91,  elapsed 5.3s (compute 5.0)\n",
      "[5, 11200] Loss: 1.18e-03  runR2: 0.72, runR2Prec: 0.91,  elapsed 5.5s (compute 5.1)\n",
      "[5, 12000] Loss: 1.13e-03  runR2: 0.73, runR2Prec: 0.95,  elapsed 5.6s (compute 5.3)\n",
      "[5, 12800] Loss: 1.07e-03  runR2: 0.73, runR2Prec: 0.90,  elapsed 5.7s (compute 5.4)\n",
      "[5, 13600] Loss: 9.49e-04  runR2: 0.73, runR2Prec: 0.91,  elapsed 5.6s (compute 5.2)\n",
      "[5, 14400] Loss: 9.38e-04  runR2: 0.73, runR2Prec: 0.98,  elapsed 5.3s (compute 5.0)\n",
      "[5, 15200] Loss: 1.22e-03  runR2: 0.73, runR2Prec: 0.92,  elapsed 5.7s (compute 4.9)\n",
      "[5, 16000] Loss: 1.07e-03  runR2: 0.73, runR2Prec: 0.92,  elapsed 5.3s (compute 4.9)\n",
      "[5, 16800] Loss: 1.07e-03  runR2: 0.73, runR2Prec: 0.96,  elapsed 5.3s (compute 5.0)\n",
      "[5, 17600] Loss: 1.04e-03  runR2: 0.73, runR2Prec: 0.95,  elapsed 6.7s (compute 5.0)\n",
      "[5, 18400] Loss: 1.05e-03  runR2: 0.73, runR2Prec: 0.95,  elapsed 6.6s (compute 5.2)\n",
      "[5, 19200] Loss: 1.06e-03  runR2: 0.73, runR2Prec: 0.93,  elapsed 5.6s (compute 5.3)\n",
      "[5, 20000] Loss: 1.26e-03  runR2: 0.73, runR2Prec: 0.95,  elapsed 5.5s (compute 5.2)\n",
      "[5, 20800] Loss: 1.11e-03  runR2: 0.73, runR2Prec: 0.90,  elapsed 5.2s (compute 4.9)\n",
      "[5, 21600] Loss: 1.05e-03  runR2: 0.73, runR2Prec: 0.94,  elapsed 5.3s (compute 4.8)\n",
      "[5, 22400] Loss: 1.06e-03  runR2: 0.73, runR2Prec: 0.92,  elapsed 4.7s (compute 4.4)\n",
      "[5, 23200] Loss: 1.06e-03  runR2: 0.73, runR2Prec: 0.90,  elapsed 4.8s (compute 4.5)\n",
      "[5, 24000] Loss: 1.08e-03  runR2: 0.73, runR2Prec: 0.97,  elapsed 5.7s (compute 4.3)\n",
      "Epoch 5 TRAIN loss: 1.08e-03  MSE: 1.08e-03  R2: 0.73  R2-dT/dt: 0.70   R2-precc: 0.939\n",
      "Epoch 5/5 complete, took 172.23 seconds, autoreg window was 1\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "save_model = False\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "if use_val: val = model_train_eval(val_loader, model, autoregressive, train=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f224e16-44df-4407-89e3-87aea0784227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n",
      "[1, 800] Loss: 1.66e-03  runR2: 0.46,  elapsed 16.0s (compute 5.8)\n",
      "[1, 1600] Loss: 1.51e-03  runR2: 0.51,  elapsed 6.5s (compute 6.2)\n",
      "[1, 2400] Loss: 1.26e-03  runR2: 0.55,  elapsed 6.7s (compute 6.2)\n",
      "[1, 3200] Loss: 1.31e-03  runR2: 0.57,  elapsed 6.2s (compute 5.8)\n",
      "[1, 4000] Loss: 1.35e-03  runR2: 0.58,  elapsed 6.4s (compute 6.1)\n",
      "[1, 4800] Loss: 1.34e-03  runR2: 0.59,  elapsed 6.1s (compute 5.7)\n",
      "[1, 5600] Loss: 1.25e-03  runR2: 0.60,  elapsed 6.2s (compute 5.9)\n",
      "[1, 6400] Loss: 1.25e-03  runR2: 0.60,  elapsed 6.5s (compute 6.1)\n",
      "[1, 7200] Loss: 1.10e-03  runR2: 0.61,  elapsed 6.4s (compute 6.2)\n",
      "[1, 8000] Loss: 1.08e-03  runR2: 0.61,  elapsed 6.7s (compute 6.1)\n",
      "[1, 8800] Loss: 1.13e-03  runR2: 0.62,  elapsed 5.3s (compute 5.0)\n",
      "[1, 9600] Loss: 1.26e-03  runR2: 0.63,  elapsed 6.5s (compute 6.1)\n",
      "[1, 10400] Loss: 1.20e-03  runR2: 0.63,  elapsed 6.3s (compute 5.9)\n",
      "[1, 11200] Loss: 1.22e-03  runR2: 0.63,  elapsed 6.9s (compute 6.5)\n",
      "[1, 12000] Loss: 1.16e-03  runR2: 0.64,  elapsed 6.6s (compute 6.2)\n",
      "[1, 12800] Loss: 1.09e-03  runR2: 0.64,  elapsed 6.7s (compute 6.2)\n",
      "[1, 13600] Loss: 9.69e-04  runR2: 0.64,  elapsed 5.8s (compute 5.5)\n",
      "[1, 14400] Loss: 9.51e-04  runR2: 0.64,  elapsed 6.8s (compute 6.4)\n",
      "[1, 15200] Loss: 1.23e-03  runR2: 0.64,  elapsed 7.1s (compute 6.4)\n",
      "[1, 16000] Loss: 1.08e-03  runR2: 0.65,  elapsed 6.7s (compute 6.3)\n",
      "[1, 16800] Loss: 1.06e-03  runR2: 0.65,  elapsed 6.7s (compute 6.4)\n",
      "[1, 17600] Loss: 1.04e-03  runR2: 0.65,  elapsed 6.8s (compute 6.1)\n",
      "[1, 18400] Loss: 1.04e-03  runR2: 0.65,  elapsed 8.6s (compute 6.4)\n",
      "[1, 19200] Loss: 1.05e-03  runR2: 0.65,  elapsed 6.9s (compute 6.7)\n",
      "[1, 20000] Loss: 1.23e-03  runR2: 0.66,  elapsed 7.4s (compute 7.1)\n",
      "[1, 20800] Loss: 1.08e-03  runR2: 0.66,  elapsed 7.0s (compute 6.7)\n",
      "[1, 21600] Loss: 1.03e-03  runR2: 0.66,  elapsed 7.3s (compute 7.0)\n",
      "[1, 22400] Loss: 1.05e-03  runR2: 0.66,  elapsed 6.6s (compute 6.3)\n",
      "[1, 23200] Loss: 1.03e-03  runR2: 0.66,  elapsed 6.1s (compute 5.8)\n",
      "[1, 24000] Loss: 1.04e-03  runR2: 0.66,  elapsed 6.5s (compute 6.3)\n",
      "Epoch 1 TRAIN loss: 1.17e-03  MSE: 1.17e-03  R2: 0.66  R2-dT/dt: 0.32   R2-precc: 0.919\n",
      "Epoch 1/5 complete, took 209.25 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.05e-03  runR2: 0.68,  elapsed 14.2s (compute 5.3)\n",
      "[2, 1600] Loss: 1.05e-03  runR2: 0.68,  elapsed 7.0s (compute 6.5)\n",
      "[2, 2400] Loss: 9.05e-04  runR2: 0.69,  elapsed 6.5s (compute 6.1)\n",
      "[2, 3200] Loss: 1.01e-03  runR2: 0.70,  elapsed 6.4s (compute 6.1)\n",
      "[2, 4000] Loss: 1.08e-03  runR2: 0.70,  elapsed 6.3s (compute 6.0)\n",
      "[2, 4800] Loss: 1.11e-03  runR2: 0.70,  elapsed 6.4s (compute 6.1)\n",
      "[2, 5600] Loss: 1.05e-03  runR2: 0.70,  elapsed 6.5s (compute 6.1)\n",
      "[2, 6400] Loss: 1.08e-03  runR2: 0.70,  elapsed 6.6s (compute 6.3)\n",
      "[2, 7200] Loss: 9.46e-04  runR2: 0.70,  elapsed 6.5s (compute 6.2)\n",
      "[2, 8000] Loss: 9.38e-04  runR2: 0.71,  elapsed 6.8s (compute 6.1)\n",
      "[2, 8800] Loss: 9.99e-04  runR2: 0.71,  elapsed 6.3s (compute 6.0)\n",
      "[2, 9600] Loss: 1.13e-03  runR2: 0.71,  elapsed 6.4s (compute 6.1)\n",
      "[2, 10400] Loss: 1.08e-03  runR2: 0.71,  elapsed 6.6s (compute 6.3)\n",
      "[2, 11200] Loss: 1.11e-03  runR2: 0.71,  elapsed 6.7s (compute 6.3)\n",
      "[2, 12000] Loss: 1.06e-03  runR2: 0.71,  elapsed 6.6s (compute 6.3)\n",
      "[2, 12800] Loss: 1.00e-03  runR2: 0.71,  elapsed 6.4s (compute 6.1)\n",
      "[2, 13600] Loss: 8.90e-04  runR2: 0.71,  elapsed 6.3s (compute 6.0)\n",
      "[2, 14400] Loss: 8.78e-04  runR2: 0.72,  elapsed 6.5s (compute 6.1)\n",
      "[2, 15200] Loss: 1.14e-03  runR2: 0.72,  elapsed 6.8s (compute 6.1)\n",
      "[2, 16000] Loss: 1.00e-03  runR2: 0.72,  elapsed 6.1s (compute 5.8)\n",
      "[2, 16800] Loss: 9.87e-04  runR2: 0.72,  elapsed 5.7s (compute 5.4)\n",
      "[2, 17600] Loss: 9.74e-04  runR2: 0.72,  elapsed 7.3s (compute 5.4)\n",
      "[2, 18400] Loss: 9.79e-04  runR2: 0.72,  elapsed 7.6s (compute 6.4)\n",
      "[2, 19200] Loss: 9.88e-04  runR2: 0.72,  elapsed 6.5s (compute 6.2)\n",
      "[2, 20000] Loss: 1.17e-03  runR2: 0.72,  elapsed 6.6s (compute 6.2)\n",
      "[2, 20800] Loss: 1.02e-03  runR2: 0.72,  elapsed 6.5s (compute 6.1)\n",
      "[2, 21600] Loss: 9.76e-04  runR2: 0.72,  elapsed 6.4s (compute 6.1)\n",
      "[2, 22400] Loss: 9.98e-04  runR2: 0.72,  elapsed 6.3s (compute 5.9)\n",
      "[2, 23200] Loss: 9.81e-04  runR2: 0.72,  elapsed 5.8s (compute 5.6)\n",
      "[2, 24000] Loss: 9.92e-04  runR2: 0.72,  elapsed 6.1s (compute 5.9)\n",
      "Epoch 2 TRAIN loss: 1.02e-03  MSE: 1.02e-03  R2: 0.72  R2-dT/dt: 0.59   R2-precc: 0.949\n",
      "Epoch 2/5 complete, took 203.35 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.02e-03  runR2: 0.71,  elapsed 14.8s (compute 6.1)\n",
      "[3, 1600] Loss: 1.01e-03  runR2: 0.71,  elapsed 6.4s (compute 6.1)\n",
      "[3, 2400] Loss: 8.64e-04  runR2: 0.72,  elapsed 6.4s (compute 6.1)\n",
      "[3, 3200] Loss: 9.68e-04  runR2: 0.73,  elapsed 6.5s (compute 6.1)\n",
      "[3, 4000] Loss: 1.04e-03  runR2: 0.73,  elapsed 7.3s (compute 6.9)\n",
      "[3, 4800] Loss: 1.07e-03  runR2: 0.73,  elapsed 6.8s (compute 6.5)\n",
      "[3, 5600] Loss: 1.01e-03  runR2: 0.73,  elapsed 6.6s (compute 6.3)\n",
      "[3, 6400] Loss: 1.04e-03  runR2: 0.73,  elapsed 6.2s (compute 5.9)\n",
      "[3, 7200] Loss: 9.10e-04  runR2: 0.73,  elapsed 6.0s (compute 5.7)\n",
      "[3, 8000] Loss: 9.08e-04  runR2: 0.73,  elapsed 5.7s (compute 5.1)\n",
      "[3, 8800] Loss: 9.61e-04  runR2: 0.74,  elapsed 5.6s (compute 5.2)\n",
      "[3, 9600] Loss: 1.09e-03  runR2: 0.74,  elapsed 6.5s (compute 6.1)\n",
      "[3, 10400] Loss: 1.05e-03  runR2: 0.74,  elapsed 6.6s (compute 6.3)\n",
      "[3, 11200] Loss: 1.08e-03  runR2: 0.74,  elapsed 6.0s (compute 5.7)\n",
      "[3, 12000] Loss: 1.03e-03  runR2: 0.74,  elapsed 6.2s (compute 5.9)\n",
      "[3, 12800] Loss: 9.71e-04  runR2: 0.74,  elapsed 6.7s (compute 6.4)\n",
      "[3, 13600] Loss: 8.65e-04  runR2: 0.74,  elapsed 6.4s (compute 6.1)\n",
      "[3, 14400] Loss: 8.52e-04  runR2: 0.74,  elapsed 6.1s (compute 5.7)\n",
      "[3, 15200] Loss: 1.11e-03  runR2: 0.74,  elapsed 6.5s (compute 5.9)\n",
      "[3, 16000] Loss: 9.76e-04  runR2: 0.74,  elapsed 6.5s (compute 6.2)\n",
      "[3, 16800] Loss: 9.62e-04  runR2: 0.74,  elapsed 6.4s (compute 6.1)\n",
      "[3, 17600] Loss: 9.50e-04  runR2: 0.74,  elapsed 6.8s (compute 6.3)\n",
      "[3, 18400] Loss: 9.55e-04  runR2: 0.74,  elapsed 7.5s (compute 6.0)\n",
      "[3, 19200] Loss: 9.65e-04  runR2: 0.74,  elapsed 6.6s (compute 6.2)\n",
      "[3, 20000] Loss: 1.14e-03  runR2: 0.74,  elapsed 6.7s (compute 6.4)\n",
      "[3, 20800] Loss: 1.00e-03  runR2: 0.74,  elapsed 6.6s (compute 6.2)\n",
      "[3, 21600] Loss: 9.54e-04  runR2: 0.74,  elapsed 6.4s (compute 6.1)\n",
      "[3, 22400] Loss: 9.77e-04  runR2: 0.74,  elapsed 6.4s (compute 6.1)\n",
      "[3, 23200] Loss: 9.59e-04  runR2: 0.74,  elapsed 6.2s (compute 5.9)\n",
      "[3, 24000] Loss: 9.72e-04  runR2: 0.74,  elapsed 5.9s (compute 5.6)\n",
      "Epoch 3 TRAIN loss: 9.87e-04  MSE: 9.87e-04  R2: 0.74  R2-dT/dt: 0.62   R2-precc: 0.957\n",
      "Epoch 3/5 complete, took 202.15 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 1.00e-03  runR2: 0.72,  elapsed 13.1s (compute 4.2)\n",
      "[4, 1600] Loss: 9.78e-04  runR2: 0.73,  elapsed 6.9s (compute 5.0)\n",
      "[4, 2400] Loss: 8.38e-04  runR2: 0.73,  elapsed 5.6s (compute 5.3)\n",
      "[4, 3200] Loss: 9.39e-04  runR2: 0.74,  elapsed 5.5s (compute 4.9)\n",
      "[4, 4000] Loss: 1.01e-03  runR2: 0.74,  elapsed 6.5s (compute 5.1)\n",
      "[4, 4800] Loss: 1.04e-03  runR2: 0.74,  elapsed 5.5s (compute 5.0)\n",
      "[4, 5600] Loss: 9.76e-04  runR2: 0.74,  elapsed 5.5s (compute 5.1)\n",
      "[4, 6400] Loss: 1.01e-03  runR2: 0.74,  elapsed 5.6s (compute 5.0)\n",
      "[4, 7200] Loss: 8.81e-04  runR2: 0.74,  elapsed 5.6s (compute 4.4)\n",
      "[4, 8000] Loss: 8.73e-04  runR2: 0.74,  elapsed 6.5s (compute 4.3)\n",
      "[4, 8800] Loss: 9.24e-04  runR2: 0.75,  elapsed 6.0s (compute 5.1)\n",
      "[4, 9600] Loss: 1.06e-03  runR2: 0.75,  elapsed 5.2s (compute 4.8)\n",
      "[4, 10400] Loss: 1.01e-03  runR2: 0.75,  elapsed 5.7s (compute 4.4)\n",
      "[4, 11200] Loss: 1.04e-03  runR2: 0.75,  elapsed 5.8s (compute 4.8)\n",
      "[4, 12000] Loss: 9.94e-04  runR2: 0.75,  elapsed 6.0s (compute 5.2)\n",
      "[4, 12800] Loss: 9.35e-04  runR2: 0.75,  elapsed 5.2s (compute 4.8)\n",
      "[4, 13600] Loss: 8.30e-04  runR2: 0.75,  elapsed 6.6s (compute 5.1)\n",
      "[4, 14400] Loss: 8.16e-04  runR2: 0.75,  elapsed 5.4s (compute 5.0)\n",
      "[4, 15200] Loss: 1.07e-03  runR2: 0.75,  elapsed 7.2s (compute 5.0)\n",
      "[4, 16000] Loss: 9.39e-04  runR2: 0.75,  elapsed 5.6s (compute 5.2)\n",
      "[4, 16800] Loss: 9.27e-04  runR2: 0.75,  elapsed 6.1s (compute 5.1)\n",
      "[4, 17600] Loss: 9.14e-04  runR2: 0.75,  elapsed 7.7s (compute 4.9)\n",
      "[4, 18400] Loss: 9.17e-04  runR2: 0.75,  elapsed 6.1s (compute 4.3)\n",
      "[4, 19200] Loss: 9.28e-04  runR2: 0.75,  elapsed 5.1s (compute 4.2)\n",
      "[4, 20000] Loss: 1.10e-03  runR2: 0.75,  elapsed 5.7s (compute 4.7)\n",
      "[4, 20800] Loss: 9.66e-04  runR2: 0.75,  elapsed 5.5s (compute 5.0)\n",
      "[4, 21600] Loss: 9.20e-04  runR2: 0.75,  elapsed 5.3s (compute 5.0)\n",
      "[4, 22400] Loss: 9.38e-04  runR2: 0.75,  elapsed 5.5s (compute 5.1)\n",
      "[4, 23200] Loss: 9.21e-04  runR2: 0.75,  elapsed 5.5s (compute 5.2)\n",
      "[4, 24000] Loss: 9.36e-04  runR2: 0.75,  elapsed 4.7s (compute 4.4)\n",
      "Epoch 4 TRAIN loss: 9.53e-04  MSE: 9.53e-04  R2: 0.75  R2-dT/dt: 0.64   R2-precc: 0.963\n",
      "Epoch 4/5 complete, took 182.55 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 9.65e-04  runR2: 0.73,  elapsed 13.2s (compute 4.5)\n",
      "[5, 1600] Loss: 9.46e-04  runR2: 0.73,  elapsed 5.6s (compute 4.5)\n",
      "[5, 2400] Loss: 8.12e-04  runR2: 0.74,  elapsed 6.2s (compute 4.5)\n",
      "[5, 3200] Loss: 9.08e-04  runR2: 0.74,  elapsed 5.3s (compute 4.5)\n",
      "[5, 4000] Loss: 9.79e-04  runR2: 0.74,  elapsed 5.3s (compute 4.8)\n",
      "[5, 4800] Loss: 1.01e-03  runR2: 0.74,  elapsed 5.8s (compute 4.7)\n",
      "[5, 5600] Loss: 9.48e-04  runR2: 0.74,  elapsed 4.8s (compute 4.5)\n",
      "[5, 6400] Loss: 9.81e-04  runR2: 0.74,  elapsed 5.9s (compute 4.6)\n",
      "[5, 7200] Loss: 8.58e-04  runR2: 0.74,  elapsed 5.7s (compute 4.7)\n",
      "[5, 8000] Loss: 8.48e-04  runR2: 0.75,  elapsed 5.5s (compute 4.6)\n",
      "[5, 8800] Loss: 9.00e-04  runR2: 0.75,  elapsed 5.7s (compute 4.6)\n",
      "[5, 9600] Loss: 1.03e-03  runR2: 0.75,  elapsed 5.2s (compute 5.0)\n",
      "[5, 10400] Loss: 9.86e-04  runR2: 0.75,  elapsed 5.3s (compute 4.9)\n",
      "[5, 11200] Loss: 1.02e-03  runR2: 0.75,  elapsed 5.1s (compute 4.7)\n",
      "[5, 12000] Loss: 9.72e-04  runR2: 0.75,  elapsed 4.8s (compute 4.4)\n",
      "[5, 12800] Loss: 9.10e-04  runR2: 0.75,  elapsed 4.6s (compute 4.3)\n",
      "[5, 13600] Loss: 8.14e-04  runR2: 0.75,  elapsed 6.3s (compute 4.2)\n",
      "[5, 14400] Loss: 7.97e-04  runR2: 0.75,  elapsed 4.3s (compute 4.1)\n",
      "[5, 15200] Loss: 1.05e-03  runR2: 0.75,  elapsed 7.3s (compute 4.5)\n",
      "[5, 16000] Loss: 9.19e-04  runR2: 0.76,  elapsed 5.3s (compute 4.8)\n",
      "[5, 16800] Loss: 9.10e-04  runR2: 0.75,  elapsed 6.4s (compute 4.8)\n",
      "[5, 17600] Loss: 8.95e-04  runR2: 0.75,  elapsed 7.8s (compute 4.9)\n",
      "[5, 18400] Loss: 9.00e-04  runR2: 0.76,  elapsed 5.7s (compute 4.7)\n",
      "[5, 19200] Loss: 9.12e-04  runR2: 0.76,  elapsed 5.2s (compute 4.7)\n",
      "[5, 20000] Loss: 1.08e-03  runR2: 0.75,  elapsed 5.2s (compute 4.5)\n",
      "[5, 20800] Loss: 9.51e-04  runR2: 0.75,  elapsed 5.4s (compute 4.7)\n",
      "[5, 21600] Loss: 9.06e-04  runR2: 0.75,  elapsed 5.5s (compute 4.5)\n",
      "[5, 22400] Loss: 9.19e-04  runR2: 0.75,  elapsed 4.8s (compute 4.4)\n",
      "[5, 23200] Loss: 9.06e-04  runR2: 0.75,  elapsed 4.8s (compute 4.6)\n",
      "[5, 24000] Loss: 9.21e-04  runR2: 0.75,  elapsed 6.4s (compute 4.6)\n",
      "Epoch 5 TRAIN loss: 9.30e-04  MSE: 9.30e-04  R2: 0.75  R2-dT/dt: 0.64   R2-precc: 0.967\n",
      "Epoch 5/5 complete, took 175.05 seconds, autoreg window was 3\n"
     ]
    }
   ],
   "source": [
    "# autoreg\n",
    "num_epochs = 5\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6e45eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n",
      "[1, 800] Loss: 2.55e-03  runR2: 0.08,  elapsed 15.1s (compute 6.3)\n",
      "[1, 1600] Loss: 1.64e-03  runR2: 0.36,  elapsed 6.8s (compute 6.5)\n",
      "[1, 2400] Loss: 1.24e-03  runR2: 0.46,  elapsed 6.5s (compute 6.2)\n",
      "[1, 3200] Loss: 1.27e-03  runR2: 0.51,  elapsed 6.4s (compute 6.1)\n",
      "[1, 4000] Loss: 1.30e-03  runR2: 0.55,  elapsed 6.7s (compute 6.4)\n",
      "[1, 4800] Loss: 1.28e-03  runR2: 0.56,  elapsed 6.7s (compute 6.4)\n",
      "[1, 5600] Loss: 1.20e-03  runR2: 0.58,  elapsed 6.5s (compute 6.2)\n",
      "[1, 6400] Loss: 1.20e-03  runR2: 0.59,  elapsed 6.6s (compute 6.2)\n",
      "[1, 7200] Loss: 1.05e-03  runR2: 0.60,  elapsed 6.8s (compute 6.5)\n",
      "[1, 8000] Loss: 1.03e-03  runR2: 0.61,  elapsed 7.1s (compute 6.5)\n",
      "[1, 8800] Loss: 1.08e-03  runR2: 0.62,  elapsed 7.6s (compute 7.2)\n",
      "[1, 9600] Loss: 1.21e-03  runR2: 0.63,  elapsed 6.6s (compute 6.4)\n",
      "[1, 10400] Loss: 1.15e-03  runR2: 0.63,  elapsed 6.5s (compute 6.2)\n",
      "[1, 11200] Loss: 1.17e-03  runR2: 0.64,  elapsed 6.6s (compute 6.2)\n",
      "[1, 12000] Loss: 1.11e-03  runR2: 0.65,  elapsed 6.6s (compute 6.2)\n",
      "[1, 12800] Loss: 1.05e-03  runR2: 0.66,  elapsed 5.9s (compute 5.6)\n",
      "[1, 13600] Loss: 9.23e-04  runR2: 0.66,  elapsed 6.2s (compute 5.9)\n",
      "[1, 14400] Loss: 9.04e-04  runR2: 0.67,  elapsed 5.4s (compute 5.1)\n",
      "[1, 15200] Loss: 1.18e-03  runR2: 0.67,  elapsed 5.8s (compute 5.1)\n",
      "[1, 16000] Loss: 1.03e-03  runR2: 0.68,  elapsed 6.2s (compute 5.9)\n",
      "[1, 16800] Loss: 1.01e-03  runR2: 0.68,  elapsed 6.6s (compute 6.3)\n",
      "[1, 17600] Loss: 9.93e-04  runR2: 0.68,  elapsed 6.8s (compute 6.2)\n",
      "[1, 18400] Loss: 9.90e-04  runR2: 0.69,  elapsed 7.8s (compute 6.5)\n",
      "[1, 19200] Loss: 1.00e-03  runR2: 0.69,  elapsed 6.6s (compute 6.3)\n",
      "[1, 20000] Loss: 1.18e-03  runR2: 0.69,  elapsed 6.6s (compute 6.2)\n",
      "[1, 20800] Loss: 1.03e-03  runR2: 0.70,  elapsed 6.7s (compute 6.4)\n",
      "[1, 21600] Loss: 9.80e-04  runR2: 0.70,  elapsed 6.6s (compute 6.2)\n",
      "[1, 22400] Loss: 9.99e-04  runR2: 0.70,  elapsed 6.4s (compute 6.1)\n",
      "[1, 23200] Loss: 9.84e-04  runR2: 0.70,  elapsed 6.5s (compute 6.2)\n",
      "[1, 24000] Loss: 9.91e-04  runR2: 0.71,  elapsed 6.2s (compute 6.0)\n",
      "Epoch 1 TRAIN loss: 1.14e-03  MSE: 1.14e-03  R2: 0.71  R2-dT/dt: -0.18   R2-precc: 0.919\n",
      "Epoch 1/10 complete, took 206.21 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.02e-03  runR2: 0.77,  elapsed 14.7s (compute 6.0)\n",
      "[2, 1600] Loss: 1.00e-03  runR2: 0.77,  elapsed 6.5s (compute 6.2)\n",
      "[2, 2400] Loss: 8.56e-04  runR2: 0.78,  elapsed 6.6s (compute 6.3)\n",
      "[2, 3200] Loss: 9.60e-04  runR2: 0.78,  elapsed 6.8s (compute 6.4)\n",
      "[2, 4000] Loss: 1.03e-03  runR2: 0.78,  elapsed 6.4s (compute 6.1)\n",
      "[2, 4800] Loss: 1.06e-03  runR2: 0.78,  elapsed 6.0s (compute 5.7)\n",
      "[2, 5600] Loss: 9.96e-04  runR2: 0.78,  elapsed 5.9s (compute 5.5)\n",
      "[2, 6400] Loss: 1.03e-03  runR2: 0.78,  elapsed 5.5s (compute 5.1)\n",
      "[2, 7200] Loss: 8.98e-04  runR2: 0.79,  elapsed 6.2s (compute 5.4)\n",
      "[2, 8000] Loss: 8.91e-04  runR2: 0.79,  elapsed 7.9s (compute 7.0)\n",
      "[2, 8800] Loss: 9.46e-04  runR2: 0.79,  elapsed 6.9s (compute 6.6)\n",
      "[2, 9600] Loss: 1.08e-03  runR2: 0.79,  elapsed 6.6s (compute 6.2)\n",
      "[2, 10400] Loss: 1.03e-03  runR2: 0.79,  elapsed 6.5s (compute 6.1)\n",
      "[2, 11200] Loss: 1.06e-03  runR2: 0.79,  elapsed 6.9s (compute 6.5)\n",
      "[2, 12000] Loss: 1.01e-03  runR2: 0.80,  elapsed 6.6s (compute 6.3)\n",
      "[2, 12800] Loss: 9.49e-04  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[2, 13600] Loss: 8.46e-04  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[2, 14400] Loss: 8.31e-04  runR2: 0.80,  elapsed 6.6s (compute 6.3)\n",
      "[2, 15200] Loss: 1.09e-03  runR2: 0.80,  elapsed 7.0s (compute 6.3)\n",
      "[2, 16000] Loss: 9.52e-04  runR2: 0.80,  elapsed 7.1s (compute 6.8)\n",
      "[2, 16800] Loss: 9.40e-04  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[2, 17600] Loss: 9.26e-04  runR2: 0.80,  elapsed 7.5s (compute 6.2)\n",
      "[2, 18400] Loss: 9.29e-04  runR2: 0.80,  elapsed 7.6s (compute 6.3)\n",
      "[2, 19200] Loss: 9.41e-04  runR2: 0.80,  elapsed 6.8s (compute 6.5)\n",
      "[2, 20000] Loss: 1.11e-03  runR2: 0.80,  elapsed 6.3s (compute 6.0)\n",
      "[2, 20800] Loss: 9.78e-04  runR2: 0.80,  elapsed 5.9s (compute 5.6)\n",
      "[2, 21600] Loss: 9.28e-04  runR2: 0.80,  elapsed 6.0s (compute 5.6)\n",
      "[2, 22400] Loss: 9.46e-04  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[2, 23200] Loss: 9.32e-04  runR2: 0.80,  elapsed 5.6s (compute 5.3)\n",
      "[2, 24000] Loss: 9.44e-04  runR2: 0.80,  elapsed 5.8s (compute 5.3)\n",
      "Epoch 2 TRAIN loss: 9.69e-04  MSE: 9.69e-04  R2: 0.80  R2-dT/dt: 0.66   R2-precc: 0.959\n",
      "Epoch 2/10 complete, took 204.83 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 9.95e-04  runR2: 0.79,  elapsed 15.3s (compute 6.3)\n",
      "[3, 1600] Loss: 9.58e-04  runR2: 0.79,  elapsed 6.6s (compute 6.3)\n",
      "[3, 2400] Loss: 8.18e-04  runR2: 0.80,  elapsed 6.5s (compute 6.1)\n",
      "[3, 3200] Loss: 9.20e-04  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[3, 4000] Loss: 9.90e-04  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[3, 4800] Loss: 1.02e-03  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[3, 5600] Loss: 9.60e-04  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[3, 6400] Loss: 9.93e-04  runR2: 0.80,  elapsed 6.5s (compute 6.1)\n",
      "[3, 7200] Loss: 8.65e-04  runR2: 0.81,  elapsed 6.5s (compute 6.2)\n",
      "[3, 8000] Loss: 8.56e-04  runR2: 0.81,  elapsed 7.0s (compute 6.3)\n",
      "[3, 8800] Loss: 9.12e-04  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[3, 9600] Loss: 1.04e-03  runR2: 0.81,  elapsed 6.7s (compute 6.4)\n",
      "[3, 10400] Loss: 9.98e-04  runR2: 0.81,  elapsed 6.5s (compute 6.2)\n",
      "[3, 11200] Loss: 1.03e-03  runR2: 0.81,  elapsed 6.6s (compute 6.3)\n",
      "[3, 12000] Loss: 9.78e-04  runR2: 0.81,  elapsed 6.5s (compute 6.2)\n",
      "[3, 12800] Loss: 9.18e-04  runR2: 0.81,  elapsed 6.1s (compute 5.8)\n",
      "[3, 13600] Loss: 8.21e-04  runR2: 0.81,  elapsed 6.7s (compute 6.4)\n",
      "[3, 14400] Loss: 8.06e-04  runR2: 0.81,  elapsed 6.4s (compute 6.1)\n",
      "[3, 15200] Loss: 1.06e-03  runR2: 0.81,  elapsed 5.8s (compute 5.1)\n",
      "[3, 16000] Loss: 9.25e-04  runR2: 0.81,  elapsed 5.7s (compute 5.1)\n",
      "[3, 16800] Loss: 9.16e-04  runR2: 0.81,  elapsed 6.4s (compute 6.1)\n",
      "[3, 17600] Loss: 9.02e-04  runR2: 0.81,  elapsed 7.1s (compute 6.5)\n",
      "[3, 18400] Loss: 9.03e-04  runR2: 0.81,  elapsed 7.8s (compute 6.3)\n",
      "[3, 19200] Loss: 9.17e-04  runR2: 0.81,  elapsed 6.6s (compute 6.3)\n",
      "[3, 20000] Loss: 1.09e-03  runR2: 0.81,  elapsed 6.7s (compute 6.4)\n",
      "[3, 20800] Loss: 9.54e-04  runR2: 0.81,  elapsed 6.9s (compute 6.6)\n",
      "[3, 21600] Loss: 9.07e-04  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[3, 22400] Loss: 9.22e-04  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[3, 23200] Loss: 9.10e-04  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[3, 24000] Loss: 9.24e-04  runR2: 0.81,  elapsed 6.0s (compute 5.7)\n",
      "Epoch 3 TRAIN loss: 9.38e-04  MSE: 9.38e-04  R2: 0.81  R2-dT/dt: 0.70   R2-precc: 0.965\n",
      "Epoch 3/10 complete, took 206.53 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 9.93e-04  runR2: 0.80,  elapsed 13.7s (compute 5.0)\n",
      "[4, 1600] Loss: 9.29e-04  runR2: 0.80,  elapsed 5.5s (compute 5.1)\n",
      "[4, 2400] Loss: 7.91e-04  runR2: 0.81,  elapsed 5.6s (compute 5.2)\n",
      "[4, 3200] Loss: 8.89e-04  runR2: 0.81,  elapsed 5.5s (compute 5.0)\n",
      "[4, 4000] Loss: 9.54e-04  runR2: 0.81,  elapsed 5.5s (compute 5.2)\n",
      "[4, 4800] Loss: 9.85e-04  runR2: 0.81,  elapsed 5.6s (compute 5.1)\n",
      "[4, 5600] Loss: 9.25e-04  runR2: 0.81,  elapsed 5.5s (compute 5.1)\n",
      "[4, 6400] Loss: 9.58e-04  runR2: 0.81,  elapsed 5.7s (compute 5.4)\n",
      "[4, 7200] Loss: 8.31e-04  runR2: 0.81,  elapsed 5.3s (compute 5.0)\n",
      "[4, 8000] Loss: 8.21e-04  runR2: 0.82,  elapsed 6.5s (compute 4.7)\n",
      "[4, 8800] Loss: 8.73e-04  runR2: 0.82,  elapsed 5.6s (compute 4.8)\n",
      "[4, 9600] Loss: 1.00e-03  runR2: 0.82,  elapsed 6.1s (compute 5.1)\n",
      "[4, 10400] Loss: 9.60e-04  runR2: 0.82,  elapsed 5.7s (compute 5.3)\n",
      "[4, 11200] Loss: 9.87e-04  runR2: 0.82,  elapsed 6.5s (compute 5.0)\n",
      "[4, 12000] Loss: 9.38e-04  runR2: 0.82,  elapsed 5.4s (compute 5.0)\n",
      "[4, 12800] Loss: 8.81e-04  runR2: 0.82,  elapsed 5.7s (compute 5.2)\n",
      "[4, 13600] Loss: 7.84e-04  runR2: 0.82,  elapsed 5.3s (compute 5.0)\n",
      "[4, 14400] Loss: 7.69e-04  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[4, 15200] Loss: 1.02e-03  runR2: 0.82,  elapsed 6.5s (compute 4.9)\n",
      "[4, 16000] Loss: 8.86e-04  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 16800] Loss: 8.79e-04  runR2: 0.82,  elapsed 5.4s (compute 5.0)\n",
      "[4, 17600] Loss: 8.65e-04  runR2: 0.82,  elapsed 7.7s (compute 5.3)\n",
      "[4, 18400] Loss: 8.63e-04  runR2: 0.82,  elapsed 6.3s (compute 5.2)\n",
      "[4, 19200] Loss: 8.79e-04  runR2: 0.82,  elapsed 5.4s (compute 5.1)\n",
      "[4, 20000] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.6s (compute 5.2)\n",
      "[4, 20800] Loss: 9.17e-04  runR2: 0.82,  elapsed 6.0s (compute 5.1)\n",
      "[4, 21600] Loss: 8.71e-04  runR2: 0.82,  elapsed 5.3s (compute 4.9)\n",
      "[4, 22400] Loss: 8.84e-04  runR2: 0.82,  elapsed 6.2s (compute 5.2)\n",
      "[4, 23200] Loss: 8.70e-04  runR2: 0.82,  elapsed 5.0s (compute 4.8)\n",
      "[4, 24000] Loss: 8.86e-04  runR2: 0.82,  elapsed 6.3s (compute 5.2)\n",
      "Epoch 4 TRAIN loss: 9.02e-04  MSE: 9.02e-04  R2: 0.82  R2-dT/dt: 0.69   R2-precc: 0.971\n",
      "Epoch 4/10 complete, took 181.24 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 9.69e-04  runR2: 0.81,  elapsed 13.4s (compute 4.7)\n",
      "[5, 1600] Loss: 8.95e-04  runR2: 0.81,  elapsed 6.6s (compute 4.8)\n",
      "[5, 2400] Loss: 7.63e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[5, 3200] Loss: 8.58e-04  runR2: 0.82,  elapsed 5.8s (compute 4.2)\n",
      "[5, 4000] Loss: 9.25e-04  runR2: 0.82,  elapsed 5.5s (compute 4.4)\n",
      "[5, 4800] Loss: 9.56e-04  runR2: 0.82,  elapsed 6.0s (compute 4.7)\n",
      "[5, 5600] Loss: 8.95e-04  runR2: 0.82,  elapsed 5.5s (compute 4.7)\n",
      "[5, 6400] Loss: 9.33e-04  runR2: 0.82,  elapsed 5.7s (compute 4.1)\n",
      "[5, 7200] Loss: 8.09e-04  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[5, 8000] Loss: 7.97e-04  runR2: 0.82,  elapsed 7.8s (compute 5.1)\n",
      "[5, 8800] Loss: 8.48e-04  runR2: 0.82,  elapsed 5.9s (compute 4.7)\n",
      "[5, 9600] Loss: 9.78e-04  runR2: 0.83,  elapsed 5.1s (compute 4.7)\n",
      "[5, 10400] Loss: 9.33e-04  runR2: 0.82,  elapsed 6.3s (compute 4.7)\n",
      "[5, 11200] Loss: 9.65e-04  runR2: 0.82,  elapsed 4.6s (compute 4.3)\n",
      "[5, 12000] Loss: 9.16e-04  runR2: 0.83,  elapsed 5.9s (compute 4.2)\n",
      "[5, 12800] Loss: 8.57e-04  runR2: 0.83,  elapsed 4.6s (compute 4.3)\n",
      "[5, 13600] Loss: 7.66e-04  runR2: 0.83,  elapsed 5.6s (compute 4.6)\n",
      "[5, 14400] Loss: 7.49e-04  runR2: 0.83,  elapsed 4.9s (compute 4.6)\n",
      "[5, 15200] Loss: 9.92e-04  runR2: 0.83,  elapsed 6.5s (compute 4.7)\n",
      "[5, 16000] Loss: 8.65e-04  runR2: 0.83,  elapsed 5.3s (compute 5.0)\n",
      "[5, 16800] Loss: 8.61e-04  runR2: 0.83,  elapsed 5.3s (compute 4.9)\n",
      "[5, 17600] Loss: 8.46e-04  runR2: 0.83,  elapsed 7.4s (compute 5.1)\n",
      "[5, 18400] Loss: 8.46e-04  runR2: 0.83,  elapsed 6.2s (compute 4.6)\n",
      "[5, 19200] Loss: 8.61e-04  runR2: 0.83,  elapsed 5.3s (compute 4.9)\n",
      "[5, 20000] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.2s (compute 4.3)\n",
      "[5, 20800] Loss: 9.01e-04  runR2: 0.83,  elapsed 5.3s (compute 4.9)\n",
      "[5, 21600] Loss: 8.56e-04  runR2: 0.83,  elapsed 5.7s (compute 4.6)\n",
      "[5, 22400] Loss: 8.68e-04  runR2: 0.83,  elapsed 4.7s (compute 4.3)\n",
      "[5, 23200] Loss: 8.54e-04  runR2: 0.83,  elapsed 5.4s (compute 4.4)\n",
      "[5, 24000] Loss: 8.71e-04  runR2: 0.83,  elapsed 4.9s (compute 4.5)\n",
      "Epoch 5 TRAIN loss: 8.79e-04  MSE: 8.79e-04  R2: 0.83  R2-dT/dt: 0.67   R2-precc: 0.975\n",
      "Epoch 5/10 complete, took 177.27 seconds, autoreg window was 3\n",
      "Epoch 6 Training rollout timesteps: 4 \n",
      "[6, 800] Loss: 9.66e-04  runR2: 0.81,  elapsed 13.4s (compute 4.5)\n",
      "[6, 1600] Loss: 8.78e-04  runR2: 0.81,  elapsed 6.2s (compute 4.7)\n",
      "[6, 2400] Loss: 7.49e-04  runR2: 0.82,  elapsed 5.3s (compute 4.9)\n",
      "[6, 3200] Loss: 8.45e-04  runR2: 0.82,  elapsed 6.2s (compute 5.0)\n",
      "[6, 4000] Loss: 9.08e-04  runR2: 0.82,  elapsed 5.2s (compute 4.8)\n",
      "[6, 4800] Loss: 9.39e-04  runR2: 0.82,  elapsed 6.2s (compute 4.5)\n",
      "[6, 5600] Loss: 8.83e-04  runR2: 0.82,  elapsed 4.9s (compute 4.6)\n",
      "[6, 6400] Loss: 9.18e-04  runR2: 0.82,  elapsed 6.2s (compute 4.5)\n",
      "[6, 7200] Loss: 7.95e-04  runR2: 0.82,  elapsed 5.6s (compute 4.7)\n",
      "[6, 8000] Loss: 7.85e-04  runR2: 0.83,  elapsed 6.6s (compute 4.6)\n",
      "[6, 8800] Loss: 8.34e-04  runR2: 0.83,  elapsed 4.8s (compute 4.5)\n",
      "[6, 9600] Loss: 9.62e-04  runR2: 0.83,  elapsed 5.8s (compute 4.5)\n",
      "[6, 10400] Loss: 9.23e-04  runR2: 0.83,  elapsed 4.7s (compute 4.4)\n",
      "[6, 11200] Loss: 9.50e-04  runR2: 0.83,  elapsed 5.3s (compute 4.4)\n",
      "[6, 12000] Loss: 9.02e-04  runR2: 0.83,  elapsed 5.1s (compute 4.4)\n",
      "[6, 12800] Loss: 8.47e-04  runR2: 0.83,  elapsed 4.7s (compute 4.4)\n",
      "[6, 13600] Loss: 7.53e-04  runR2: 0.83,  elapsed 5.8s (compute 4.5)\n",
      "[6, 14400] Loss: 7.38e-04  runR2: 0.83,  elapsed 5.1s (compute 4.8)\n",
      "[6, 15200] Loss: 9.80e-04  runR2: 0.83,  elapsed 6.6s (compute 4.9)\n",
      "[6, 16000] Loss: 8.53e-04  runR2: 0.83,  elapsed 5.0s (compute 4.7)\n",
      "[6, 16800] Loss: 8.50e-04  runR2: 0.83,  elapsed 6.1s (compute 4.1)\n",
      "[6, 17600] Loss: 8.37e-04  runR2: 0.83,  elapsed 7.1s (compute 4.0)\n",
      "[6, 18400] Loss: 8.32e-04  runR2: 0.83,  elapsed 6.3s (compute 4.2)\n",
      "[6, 19200] Loss: 8.50e-04  runR2: 0.83,  elapsed 4.9s (compute 4.5)\n",
      "[6, 20000] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.3s (compute 4.5)\n",
      "[6, 20800] Loss: 8.87e-04  runR2: 0.83,  elapsed 5.0s (compute 4.7)\n",
      "[6, 21600] Loss: 8.45e-04  runR2: 0.83,  elapsed 6.4s (compute 4.6)\n",
      "[6, 22400] Loss: 8.58e-04  runR2: 0.83,  elapsed 5.5s (compute 4.6)\n",
      "[6, 23200] Loss: 8.43e-04  runR2: 0.83,  elapsed 5.1s (compute 4.8)\n",
      "[6, 24000] Loss: 8.59e-04  runR2: 0.83,  elapsed 5.7s (compute 4.7)\n",
      "Epoch 6 TRAIN loss: 8.66e-04  MSE: 8.66e-04  R2: 0.83  R2-dT/dt: 0.68   R2-precc: 0.977\n",
      "Epoch 6/10 complete, took 177.67 seconds, autoreg window was 4\n",
      "Epoch 7 Training rollout timesteps: 5 \n",
      "[7, 800] Loss: 9.70e-04  runR2: 0.81,  elapsed 13.1s (compute 4.4)\n",
      "[7, 1600] Loss: 8.68e-04  runR2: 0.82,  elapsed 5.8s (compute 4.3)\n",
      "[7, 2400] Loss: 7.41e-04  runR2: 0.82,  elapsed 4.8s (compute 4.3)\n",
      "[7, 3200] Loss: 8.35e-04  runR2: 0.83,  elapsed 6.4s (compute 4.1)\n",
      "[7, 4000] Loss: 8.98e-04  runR2: 0.83,  elapsed 4.7s (compute 4.5)\n",
      "[7, 4800] Loss: 9.29e-04  runR2: 0.83,  elapsed 6.1s (compute 4.5)\n",
      "[7, 5600] Loss: 8.75e-04  runR2: 0.83,  elapsed 5.2s (compute 4.7)\n",
      "[7, 6400] Loss: 9.09e-04  runR2: 0.83,  elapsed 6.3s (compute 4.7)\n",
      "[7, 7200] Loss: 7.87e-04  runR2: 0.83,  elapsed 4.9s (compute 4.2)\n",
      "[7, 8000] Loss: 7.77e-04  runR2: 0.83,  elapsed 7.0s (compute 4.6)\n",
      "[7, 8800] Loss: 8.26e-04  runR2: 0.83,  elapsed 5.8s (compute 4.1)\n",
      "[7, 9600] Loss: 9.55e-04  runR2: 0.83,  elapsed 4.9s (compute 4.5)\n",
      "[7, 10400] Loss: 9.14e-04  runR2: 0.83,  elapsed 5.2s (compute 4.5)\n",
      "[7, 11200] Loss: 9.41e-04  runR2: 0.83,  elapsed 5.4s (compute 4.5)\n",
      "[7, 12000] Loss: 8.93e-04  runR2: 0.83,  elapsed 4.9s (compute 4.3)\n",
      "[7, 12800] Loss: 8.39e-04  runR2: 0.83,  elapsed 4.5s (compute 4.2)\n",
      "[7, 13600] Loss: 7.46e-04  runR2: 0.83,  elapsed 6.2s (compute 4.0)\n",
      "[7, 14400] Loss: 7.31e-04  runR2: 0.83,  elapsed 5.0s (compute 4.4)\n",
      "[7, 15200] Loss: 9.71e-04  runR2: 0.83,  elapsed 6.0s (compute 4.7)\n",
      "[7, 16000] Loss: 8.45e-04  runR2: 0.83,  elapsed 5.7s (compute 4.2)\n",
      "[7, 16800] Loss: 8.42e-04  runR2: 0.83,  elapsed 4.7s (compute 4.4)\n",
      "[7, 17600] Loss: 8.29e-04  runR2: 0.83,  elapsed 7.4s (compute 4.5)\n",
      "[7, 18400] Loss: 8.26e-04  runR2: 0.83,  elapsed 6.8s (compute 4.6)\n",
      "[7, 19200] Loss: 8.42e-04  runR2: 0.83,  elapsed 4.9s (compute 4.6)\n",
      "[7, 20000] Loss: 1.00e-03  runR2: 0.83,  elapsed 5.6s (compute 4.6)\n",
      "[7, 20800] Loss: 8.80e-04  runR2: 0.83,  elapsed 5.7s (compute 4.2)\n",
      "[7, 21600] Loss: 8.37e-04  runR2: 0.83,  elapsed 4.8s (compute 4.4)\n",
      "[7, 22400] Loss: 8.52e-04  runR2: 0.83,  elapsed 5.8s (compute 4.3)\n",
      "[7, 23200] Loss: 8.35e-04  runR2: 0.83,  elapsed 4.6s (compute 4.3)\n",
      "[7, 24000] Loss: 8.53e-04  runR2: 0.83,  elapsed 5.7s (compute 4.4)\n",
      "Epoch 7 TRAIN loss: 8.58e-04  MSE: 8.58e-04  R2: 0.83  R2-dT/dt: 0.67   R2-precc: 0.978\n",
      "Epoch 7/10 complete, took 174.57 seconds, autoreg window was 5\n",
      "Epoch 8 Training rollout timesteps: 5 \n",
      "[8, 800] Loss: 9.71e-04  runR2: 0.82,  elapsed 13.4s (compute 4.4)\n",
      "[8, 1600] Loss: 8.62e-04  runR2: 0.82,  elapsed 5.6s (compute 4.4)\n",
      "[8, 2400] Loss: 7.36e-04  runR2: 0.82,  elapsed 5.8s (compute 4.4)\n",
      "[8, 3200] Loss: 8.29e-04  runR2: 0.83,  elapsed 4.8s (compute 4.4)\n",
      "[8, 4000] Loss: 8.92e-04  runR2: 0.83,  elapsed 6.3s (compute 4.3)\n",
      "[8, 4800] Loss: 9.24e-04  runR2: 0.83,  elapsed 5.5s (compute 4.4)\n",
      "[8, 5600] Loss: 8.69e-04  runR2: 0.83,  elapsed 5.8s (compute 4.3)\n",
      "[8, 6400] Loss: 9.04e-04  runR2: 0.83,  elapsed 5.4s (compute 4.2)\n",
      "[8, 7200] Loss: 7.81e-04  runR2: 0.83,  elapsed 4.8s (compute 3.5)\n",
      "[8, 8000] Loss: 7.71e-04  runR2: 0.83,  elapsed 7.5s (compute 4.0)\n",
      "[8, 8800] Loss: 8.21e-04  runR2: 0.83,  elapsed 5.5s (compute 4.6)\n",
      "[8, 9600] Loss: 9.49e-04  runR2: 0.83,  elapsed 5.8s (compute 4.4)\n",
      "[8, 10400] Loss: 9.09e-04  runR2: 0.83,  elapsed 5.5s (compute 4.7)\n",
      "[8, 11200] Loss: 9.36e-04  runR2: 0.83,  elapsed 5.8s (compute 4.3)\n",
      "[8, 12000] Loss: 8.88e-04  runR2: 0.83,  elapsed 5.0s (compute 4.4)\n",
      "[8, 12800] Loss: 8.34e-04  runR2: 0.83,  elapsed 4.8s (compute 4.4)\n",
      "[8, 13600] Loss: 7.41e-04  runR2: 0.83,  elapsed 5.8s (compute 4.3)\n",
      "[8, 14400] Loss: 7.27e-04  runR2: 0.83,  elapsed 4.8s (compute 4.6)\n",
      "[8, 15200] Loss: 9.65e-04  runR2: 0.83,  elapsed 6.3s (compute 4.5)\n",
      "[8, 16000] Loss: 8.40e-04  runR2: 0.83,  elapsed 5.5s (compute 3.9)\n",
      "[8, 16800] Loss: 8.37e-04  runR2: 0.83,  elapsed 5.6s (compute 4.7)\n",
      "[8, 17600] Loss: 8.24e-04  runR2: 0.83,  elapsed 6.7s (compute 4.3)\n",
      "[8, 18400] Loss: 8.21e-04  runR2: 0.83,  elapsed 6.5s (compute 4.5)\n",
      "[8, 19200] Loss: 8.38e-04  runR2: 0.83,  elapsed 5.1s (compute 4.3)\n",
      "[8, 20000] Loss: 9.97e-04  runR2: 0.83,  elapsed 6.0s (compute 4.4)\n",
      "[8, 20800] Loss: 8.76e-04  runR2: 0.83,  elapsed 4.8s (compute 4.4)\n",
      "[8, 21600] Loss: 8.33e-04  runR2: 0.83,  elapsed 5.7s (compute 4.3)\n",
      "[8, 22400] Loss: 8.47e-04  runR2: 0.83,  elapsed 5.7s (compute 4.5)\n",
      "[8, 23200] Loss: 8.31e-04  runR2: 0.83,  elapsed 4.7s (compute 4.5)\n",
      "[8, 24000] Loss: 8.49e-04  runR2: 0.83,  elapsed 5.5s (compute 4.1)\n",
      "Epoch 8 TRAIN loss: 8.52e-04  MSE: 8.52e-04  R2: 0.83  R2-dT/dt: 0.69   R2-precc: 0.979\n",
      "Epoch 8/10 complete, took 176.27 seconds, autoreg window was 5\n",
      "Epoch 9 Training rollout timesteps: 5 \n",
      "[9, 800] Loss: 9.75e-04  runR2: 0.82,  elapsed 13.0s (compute 4.1)\n",
      "[9, 1600] Loss: 8.57e-04  runR2: 0.82,  elapsed 6.6s (compute 4.0)\n",
      "[9, 2400] Loss: 7.32e-04  runR2: 0.83,  elapsed 3.9s (compute 3.6)\n",
      "[9, 3200] Loss: 8.25e-04  runR2: 0.83,  elapsed 6.7s (compute 3.5)\n",
      "[9, 4000] Loss: 8.87e-04  runR2: 0.83,  elapsed 4.9s (compute 4.5)\n",
      "[9, 4800] Loss: 9.19e-04  runR2: 0.83,  elapsed 6.7s (compute 4.5)\n",
      "[9, 5600] Loss: 8.65e-04  runR2: 0.83,  elapsed 4.8s (compute 4.5)\n",
      "[9, 6400] Loss: 9.00e-04  runR2: 0.83,  elapsed 5.6s (compute 4.4)\n",
      "[9, 7200] Loss: 7.78e-04  runR2: 0.83,  elapsed 5.1s (compute 4.2)\n",
      "[9, 8000] Loss: 7.68e-04  runR2: 0.83,  elapsed 6.5s (compute 4.4)\n",
      "[9, 8800] Loss: 8.17e-04  runR2: 0.83,  elapsed 6.1s (compute 4.7)\n",
      "[9, 9600] Loss: 9.44e-04  runR2: 0.83,  elapsed 5.0s (compute 4.4)\n",
      "[9, 10400] Loss: 9.05e-04  runR2: 0.83,  elapsed 5.0s (compute 4.4)\n",
      "[9, 11200] Loss: 9.32e-04  runR2: 0.83,  elapsed 5.7s (compute 4.4)\n",
      "[9, 12000] Loss: 8.84e-04  runR2: 0.84,  elapsed 4.9s (compute 4.4)\n",
      "[9, 12800] Loss: 8.30e-04  runR2: 0.83,  elapsed 5.5s (compute 4.4)\n",
      "[9, 13600] Loss: 7.38e-04  runR2: 0.83,  elapsed 5.1s (compute 4.7)\n",
      "[9, 14400] Loss: 7.23e-04  runR2: 0.84,  elapsed 4.7s (compute 4.4)\n",
      "[9, 15200] Loss: 9.61e-04  runR2: 0.84,  elapsed 6.3s (compute 4.7)\n",
      "[9, 16000] Loss: 8.35e-04  runR2: 0.84,  elapsed 4.9s (compute 4.6)\n",
      "[9, 16800] Loss: 8.34e-04  runR2: 0.83,  elapsed 5.7s (compute 4.4)\n",
      "[9, 17600] Loss: 8.21e-04  runR2: 0.83,  elapsed 7.1s (compute 4.6)\n",
      "[9, 18400] Loss: 8.17e-04  runR2: 0.84,  elapsed 6.6s (compute 4.5)\n",
      "[9, 19200] Loss: 8.34e-04  runR2: 0.84,  elapsed 4.8s (compute 4.5)\n",
      "[9, 20000] Loss: 9.93e-04  runR2: 0.84,  elapsed 6.3s (compute 4.5)\n",
      "[9, 20800] Loss: 8.72e-04  runR2: 0.83,  elapsed 4.7s (compute 4.5)\n",
      "[9, 21600] Loss: 8.30e-04  runR2: 0.83,  elapsed 5.9s (compute 3.9)\n",
      "[9, 22400] Loss: 8.39e-04  runR2: 0.83,  elapsed 5.1s (compute 3.7)\n",
      "[9, 23200] Loss: 8.28e-04  runR2: 0.83,  elapsed 3.8s (compute 3.5)\n",
      "[9, 24000] Loss: 8.45e-04  runR2: 0.83,  elapsed 6.7s (compute 4.0)\n",
      "Epoch 9 TRAIN loss: 8.48e-04  MSE: 8.48e-04  R2: 0.83  R2-dT/dt: 0.70   R2-precc: 0.980\n",
      "Epoch 9/10 complete, took 174.12 seconds, autoreg window was 5\n",
      "Epoch 10 Training rollout timesteps: 5 \n",
      "[10, 800] Loss: 9.80e-04  runR2: 0.82,  elapsed 13.3s (compute 4.5)\n",
      "[10, 1600] Loss: 8.54e-04  runR2: 0.82,  elapsed 6.5s (compute 4.3)\n",
      "[10, 2400] Loss: 7.29e-04  runR2: 0.83,  elapsed 4.6s (compute 4.4)\n",
      "[10, 3200] Loss: 8.21e-04  runR2: 0.83,  elapsed 6.7s (compute 4.2)\n",
      "[10, 4000] Loss: 8.84e-04  runR2: 0.83,  elapsed 5.1s (compute 4.4)\n",
      "[10, 4800] Loss: 9.16e-04  runR2: 0.83,  elapsed 6.1s (compute 4.3)\n",
      "[10, 5600] Loss: 8.62e-04  runR2: 0.83,  elapsed 5.7s (compute 4.4)\n",
      "[10, 6400] Loss: 8.97e-04  runR2: 0.83,  elapsed 5.2s (compute 4.3)\n",
      "[10, 7200] Loss: 7.74e-04  runR2: 0.83,  elapsed 6.2s (compute 4.4)\n",
      "[10, 8000] Loss: 7.64e-04  runR2: 0.83,  elapsed 6.4s (compute 4.4)\n",
      "[10, 8800] Loss: 8.13e-04  runR2: 0.84,  elapsed 6.2s (compute 4.3)\n",
      "[10, 9600] Loss: 9.41e-04  runR2: 0.84,  elapsed 4.7s (compute 4.4)\n",
      "[10, 10400] Loss: 9.01e-04  runR2: 0.84,  elapsed 6.5s (compute 4.3)\n",
      "[10, 11200] Loss: 9.28e-04  runR2: 0.84,  elapsed 5.4s (compute 4.7)\n",
      "[10, 12000] Loss: 8.81e-04  runR2: 0.84,  elapsed 4.8s (compute 4.5)\n",
      "[10, 12800] Loss: 8.26e-04  runR2: 0.84,  elapsed 4.9s (compute 4.4)\n",
      "[10, 13600] Loss: 7.35e-04  runR2: 0.84,  elapsed 6.2s (compute 4.3)\n",
      "[10, 14400] Loss: 7.20e-04  runR2: 0.84,  elapsed 4.7s (compute 4.4)\n",
      "[10, 15200] Loss: 9.57e-04  runR2: 0.84,  elapsed 7.0s (compute 4.3)\n",
      "[10, 16000] Loss: 8.32e-04  runR2: 0.84,  elapsed 4.8s (compute 4.5)\n",
      "[10, 16800] Loss: 8.31e-04  runR2: 0.84,  elapsed 5.6s (compute 4.0)\n",
      "[10, 17600] Loss: 8.18e-04  runR2: 0.84,  elapsed 7.3s (compute 3.6)\n",
      "[10, 18400] Loss: 8.14e-04  runR2: 0.84,  elapsed 5.8s (compute 4.2)\n",
      "[10, 19200] Loss: 8.31e-04  runR2: 0.84,  elapsed 4.9s (compute 4.2)\n",
      "[10, 20000] Loss: 9.89e-04  runR2: 0.84,  elapsed 5.8s (compute 4.3)\n",
      "[10, 20800] Loss: 8.69e-04  runR2: 0.84,  elapsed 5.2s (compute 4.5)\n",
      "[10, 21600] Loss: 8.27e-04  runR2: 0.84,  elapsed 5.5s (compute 4.5)\n",
      "[10, 22400] Loss: 8.35e-04  runR2: 0.84,  elapsed 4.9s (compute 4.2)\n",
      "[10, 23200] Loss: 8.25e-04  runR2: 0.84,  elapsed 4.5s (compute 4.2)\n",
      "[10, 24000] Loss: 8.42e-04  runR2: 0.83,  elapsed 5.5s (compute 4.3)\n",
      "Epoch 10 TRAIN loss: 8.45e-04  MSE: 8.45e-04  R2: 0.83  R2-dT/dt: 0.71   R2-precc: 0.980\n",
      "Epoch 10/10 complete, took 176.73 seconds, autoreg window was 5\n"
     ]
    }
   ],
   "source": [
    "# 128 128\n",
    "# autoreg\n",
    "num_epochs = 10\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f4057c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n",
      "[1, 800] Loss: 4.82e-03  runR2: -0.24,  elapsed 15.9s (compute 6.3)\n",
      "[1, 1600] Loss: 2.59e-03  runR2: 0.25,  elapsed 6.6s (compute 6.3)\n",
      "[1, 2400] Loss: 2.13e-03  runR2: 0.36,  elapsed 6.8s (compute 6.5)\n",
      "[1, 3200] Loss: 1.94e-03  runR2: 0.43,  elapsed 6.7s (compute 6.4)\n",
      "[1, 4000] Loss: 1.80e-03  runR2: 0.48,  elapsed 6.3s (compute 5.9)\n",
      "[1, 4800] Loss: 1.73e-03  runR2: 0.51,  elapsed 7.1s (compute 6.8)\n",
      "[1, 5600] Loss: 1.69e-03  runR2: 0.52,  elapsed 7.1s (compute 6.8)\n",
      "[1, 6400] Loss: 1.56e-03  runR2: 0.54,  elapsed 6.8s (compute 6.5)\n",
      "[1, 7200] Loss: 1.62e-03  runR2: 0.55,  elapsed 6.5s (compute 6.2)\n",
      "[1, 8000] Loss: 1.64e-03  runR2: 0.56,  elapsed 7.6s (compute 7.0)\n",
      "[1, 8800] Loss: 1.64e-03  runR2: 0.57,  elapsed 6.5s (compute 6.2)\n",
      "[1, 9600] Loss: 1.62e-03  runR2: 0.58,  elapsed 7.1s (compute 6.8)\n",
      "[1, 10400] Loss: 1.44e-03  runR2: 0.58,  elapsed 6.5s (compute 6.2)\n",
      "[1, 11200] Loss: 1.57e-03  runR2: 0.59,  elapsed 6.8s (compute 6.5)\n",
      "[1, 12000] Loss: 1.31e-03  runR2: 0.60,  elapsed 6.8s (compute 6.5)\n",
      "[1, 12800] Loss: 1.43e-03  runR2: 0.61,  elapsed 7.1s (compute 6.8)\n",
      "[1, 13600] Loss: 1.43e-03  runR2: 0.61,  elapsed 6.7s (compute 6.4)\n",
      "[1, 14400] Loss: 1.29e-03  runR2: 0.61,  elapsed 6.8s (compute 6.5)\n",
      "[1, 15200] Loss: 1.50e-03  runR2: 0.62,  elapsed 6.8s (compute 6.2)\n",
      "[1, 16000] Loss: 1.44e-03  runR2: 0.62,  elapsed 6.6s (compute 6.3)\n",
      "[1, 16800] Loss: 1.29e-03  runR2: 0.62,  elapsed 6.7s (compute 6.5)\n",
      "[1, 17600] Loss: 1.21e-03  runR2: 0.63,  elapsed 7.1s (compute 6.5)\n",
      "[1, 18400] Loss: 1.34e-03  runR2: 0.63,  elapsed 6.8s (compute 6.5)\n",
      "[1, 19200] Loss: 1.56e-03  runR2: 0.63,  elapsed 6.8s (compute 6.5)\n",
      "[1, 20000] Loss: 1.35e-03  runR2: 0.63,  elapsed 7.0s (compute 6.7)\n",
      "[1, 20800] Loss: 1.40e-03  runR2: 0.64,  elapsed 6.9s (compute 6.6)\n",
      "[1, 21600] Loss: 1.39e-03  runR2: 0.64,  elapsed 6.5s (compute 6.3)\n",
      "[1, 22400] Loss: 1.31e-03  runR2: 0.64,  elapsed 6.6s (compute 6.3)\n",
      "[1, 23200] Loss: 1.26e-03  runR2: 0.65,  elapsed 6.6s (compute 6.4)\n",
      "[1, 24000] Loss: 1.21e-03  runR2: 0.65,  elapsed 5.8s (compute 5.5)\n",
      "Epoch 1 TRAIN loss: 1.59e-03  MSE: 1.31e-03  h-con:  2.82e+03   R2: 0.65  R2-dT/dt: -0.86   R2-precc: 0.873\n",
      "Epoch 1/10 complete, took 212.42 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.38e-03  runR2: 0.69,  elapsed 15.9s (compute 5.9)\n",
      "[2, 1600] Loss: 1.45e-03  runR2: 0.71,  elapsed 6.8s (compute 6.5)\n",
      "[2, 2400] Loss: 1.26e-03  runR2: 0.71,  elapsed 6.8s (compute 6.4)\n",
      "[2, 3200] Loss: 1.22e-03  runR2: 0.71,  elapsed 6.5s (compute 6.2)\n",
      "[2, 4000] Loss: 1.23e-03  runR2: 0.72,  elapsed 6.5s (compute 6.1)\n",
      "[2, 4800] Loss: 1.25e-03  runR2: 0.73,  elapsed 6.7s (compute 6.4)\n",
      "[2, 5600] Loss: 1.23e-03  runR2: 0.72,  elapsed 6.8s (compute 6.4)\n",
      "[2, 6400] Loss: 1.20e-03  runR2: 0.72,  elapsed 6.8s (compute 6.4)\n",
      "[2, 7200] Loss: 1.29e-03  runR2: 0.72,  elapsed 6.8s (compute 6.4)\n",
      "[2, 8000] Loss: 1.30e-03  runR2: 0.73,  elapsed 7.1s (compute 6.5)\n",
      "[2, 8800] Loss: 1.35e-03  runR2: 0.73,  elapsed 6.7s (compute 6.4)\n",
      "[2, 9600] Loss: 1.35e-03  runR2: 0.73,  elapsed 6.7s (compute 6.4)\n",
      "[2, 10400] Loss: 1.20e-03  runR2: 0.73,  elapsed 6.8s (compute 6.5)\n",
      "[2, 11200] Loss: 1.32e-03  runR2: 0.73,  elapsed 6.8s (compute 6.4)\n",
      "[2, 12000] Loss: 1.12e-03  runR2: 0.74,  elapsed 6.6s (compute 6.3)\n",
      "[2, 12800] Loss: 1.22e-03  runR2: 0.74,  elapsed 6.9s (compute 6.6)\n",
      "[2, 13600] Loss: 1.24e-03  runR2: 0.74,  elapsed 6.9s (compute 6.5)\n",
      "[2, 14400] Loss: 1.14e-03  runR2: 0.74,  elapsed 6.8s (compute 6.5)\n",
      "[2, 15200] Loss: 1.31e-03  runR2: 0.74,  elapsed 7.1s (compute 6.4)\n",
      "[2, 16000] Loss: 1.28e-03  runR2: 0.74,  elapsed 6.7s (compute 6.4)\n",
      "[2, 16800] Loss: 1.16e-03  runR2: 0.74,  elapsed 6.9s (compute 6.6)\n",
      "[2, 17600] Loss: 1.08e-03  runR2: 0.74,  elapsed 7.0s (compute 6.4)\n",
      "[2, 18400] Loss: 1.21e-03  runR2: 0.74,  elapsed 6.8s (compute 6.4)\n",
      "[2, 19200] Loss: 1.43e-03  runR2: 0.74,  elapsed 6.7s (compute 6.4)\n",
      "[2, 20000] Loss: 1.23e-03  runR2: 0.74,  elapsed 6.7s (compute 6.4)\n",
      "[2, 20800] Loss: 1.28e-03  runR2: 0.74,  elapsed 6.5s (compute 6.2)\n",
      "[2, 21600] Loss: 1.28e-03  runR2: 0.74,  elapsed 6.5s (compute 6.2)\n",
      "[2, 22400] Loss: 1.21e-03  runR2: 0.75,  elapsed 6.5s (compute 6.1)\n",
      "[2, 23200] Loss: 1.17e-03  runR2: 0.75,  elapsed 6.2s (compute 5.9)\n",
      "[2, 24000] Loss: 1.13e-03  runR2: 0.75,  elapsed 5.7s (compute 5.4)\n",
      "Epoch 2 TRAIN loss: 1.25e-03  MSE: 1.03e-03  h-con:  2.12e+03   R2: 0.75  R2-dT/dt: 0.59   R2-precc: 0.946\n",
      "Epoch 2/10 complete, took 210.63 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.35e-03  runR2: 0.75,  elapsed 15.4s (compute 5.5)\n",
      "[3, 1600] Loss: 1.37e-03  runR2: 0.76,  elapsed 6.5s (compute 6.2)\n",
      "[3, 2400] Loss: 1.18e-03  runR2: 0.76,  elapsed 5.7s (compute 5.4)\n",
      "[3, 3200] Loss: 1.14e-03  runR2: 0.76,  elapsed 6.8s (compute 6.5)\n",
      "[3, 4000] Loss: 1.16e-03  runR2: 0.77,  elapsed 7.1s (compute 6.8)\n",
      "[3, 4800] Loss: 1.18e-03  runR2: 0.77,  elapsed 6.7s (compute 6.4)\n",
      "[3, 5600] Loss: 1.17e-03  runR2: 0.77,  elapsed 6.6s (compute 6.3)\n",
      "[3, 6400] Loss: 1.13e-03  runR2: 0.77,  elapsed 6.6s (compute 6.3)\n",
      "[3, 7200] Loss: 1.22e-03  runR2: 0.77,  elapsed 6.2s (compute 5.9)\n",
      "[3, 8000] Loss: 1.23e-03  runR2: 0.77,  elapsed 7.0s (compute 6.4)\n",
      "[3, 8800] Loss: 1.28e-03  runR2: 0.77,  elapsed 6.8s (compute 6.4)\n",
      "[3, 9600] Loss: 1.29e-03  runR2: 0.77,  elapsed 6.9s (compute 6.6)\n",
      "[3, 10400] Loss: 1.14e-03  runR2: 0.77,  elapsed 7.8s (compute 7.5)\n",
      "[3, 11200] Loss: 1.26e-03  runR2: 0.77,  elapsed 7.2s (compute 6.8)\n",
      "[3, 12000] Loss: 1.07e-03  runR2: 0.78,  elapsed 6.7s (compute 6.4)\n",
      "[3, 12800] Loss: 1.17e-03  runR2: 0.78,  elapsed 6.7s (compute 6.4)\n",
      "[3, 13600] Loss: 1.19e-03  runR2: 0.78,  elapsed 6.6s (compute 6.3)\n",
      "[3, 14400] Loss: 1.09e-03  runR2: 0.78,  elapsed 6.1s (compute 5.8)\n",
      "[3, 15200] Loss: 1.25e-03  runR2: 0.78,  elapsed 6.4s (compute 5.8)\n",
      "[3, 16000] Loss: 1.23e-03  runR2: 0.78,  elapsed 6.1s (compute 5.8)\n",
      "[3, 16800] Loss: 1.12e-03  runR2: 0.78,  elapsed 6.6s (compute 6.4)\n",
      "[3, 17600] Loss: 1.04e-03  runR2: 0.78,  elapsed 7.3s (compute 6.5)\n",
      "[3, 18400] Loss: 1.17e-03  runR2: 0.78,  elapsed 6.8s (compute 6.4)\n",
      "[3, 19200] Loss: 1.37e-03  runR2: 0.78,  elapsed 6.7s (compute 6.4)\n",
      "[3, 20000] Loss: 1.19e-03  runR2: 0.78,  elapsed 6.1s (compute 5.9)\n",
      "[3, 20800] Loss: 1.23e-03  runR2: 0.78,  elapsed 6.3s (compute 6.0)\n",
      "[3, 21600] Loss: 1.24e-03  runR2: 0.78,  elapsed 6.6s (compute 6.3)\n",
      "[3, 22400] Loss: 1.17e-03  runR2: 0.78,  elapsed 6.7s (compute 6.3)\n",
      "[3, 23200] Loss: 1.13e-03  runR2: 0.78,  elapsed 6.7s (compute 6.4)\n",
      "[3, 24000] Loss: 1.09e-03  runR2: 0.78,  elapsed 6.3s (compute 6.0)\n",
      "Epoch 3 TRAIN loss: 1.19e-03  MSE: 9.93e-04  h-con:  1.97e+03   R2: 0.78  R2-dT/dt: 0.61   R2-precc: 0.955\n",
      "Epoch 3/10 complete, took 208.55 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 1.35e-03  runR2: 0.76,  elapsed 14.8s (compute 5.0)\n",
      "[4, 1600] Loss: 1.31e-03  runR2: 0.77,  elapsed 5.4s (compute 5.1)\n",
      "[4, 2400] Loss: 1.14e-03  runR2: 0.77,  elapsed 5.4s (compute 5.1)\n",
      "[4, 3200] Loss: 1.10e-03  runR2: 0.78,  elapsed 5.7s (compute 5.1)\n",
      "[4, 4000] Loss: 1.12e-03  runR2: 0.78,  elapsed 5.1s (compute 4.8)\n",
      "[4, 4800] Loss: 1.12e-03  runR2: 0.78,  elapsed 5.4s (compute 5.1)\n",
      "[4, 5600] Loss: 1.12e-03  runR2: 0.78,  elapsed 5.4s (compute 5.0)\n",
      "[4, 6400] Loss: 1.08e-03  runR2: 0.78,  elapsed 5.0s (compute 4.7)\n",
      "[4, 7200] Loss: 1.17e-03  runR2: 0.78,  elapsed 5.3s (compute 5.0)\n",
      "[4, 8000] Loss: 1.17e-03  runR2: 0.78,  elapsed 6.6s (compute 5.1)\n",
      "[4, 8800] Loss: 1.23e-03  runR2: 0.78,  elapsed 6.3s (compute 5.3)\n",
      "[4, 9600] Loss: 1.23e-03  runR2: 0.78,  elapsed 5.3s (compute 5.0)\n",
      "[4, 10400] Loss: 1.09e-03  runR2: 0.78,  elapsed 6.4s (compute 5.3)\n",
      "[4, 11200] Loss: 1.20e-03  runR2: 0.78,  elapsed 5.8s (compute 5.5)\n",
      "[4, 12000] Loss: 1.01e-03  runR2: 0.79,  elapsed 6.0s (compute 5.7)\n",
      "[4, 12800] Loss: 1.11e-03  runR2: 0.79,  elapsed 5.7s (compute 5.4)\n",
      "[4, 13600] Loss: 1.13e-03  runR2: 0.79,  elapsed 5.5s (compute 5.2)\n",
      "[4, 14400] Loss: 1.04e-03  runR2: 0.79,  elapsed 5.4s (compute 5.1)\n",
      "[4, 15200] Loss: 1.19e-03  runR2: 0.79,  elapsed 5.4s (compute 4.8)\n",
      "[4, 16000] Loss: 1.18e-03  runR2: 0.79,  elapsed 5.9s (compute 5.6)\n",
      "[4, 16800] Loss: 1.07e-03  runR2: 0.79,  elapsed 5.1s (compute 4.9)\n",
      "[4, 17600] Loss: 9.85e-04  runR2: 0.79,  elapsed 7.9s (compute 4.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 18400] Loss: 1.12e-03  runR2: 0.79,  elapsed 5.0s (compute 4.7)\n",
      "[4, 19200] Loss: 1.31e-03  runR2: 0.79,  elapsed 6.4s (compute 5.2)\n",
      "[4, 20000] Loss: 1.13e-03  runR2: 0.79,  elapsed 5.3s (compute 5.0)\n",
      "[4, 20800] Loss: 1.17e-03  runR2: 0.79,  elapsed 5.3s (compute 4.9)\n",
      "[4, 21600] Loss: 1.19e-03  runR2: 0.79,  elapsed 5.2s (compute 4.9)\n",
      "[4, 22400] Loss: 1.11e-03  runR2: 0.79,  elapsed 5.4s (compute 4.8)\n",
      "[4, 23200] Loss: 1.07e-03  runR2: 0.79,  elapsed 4.5s (compute 4.3)\n",
      "[4, 24000] Loss: 1.04e-03  runR2: 0.79,  elapsed 5.8s (compute 5.0)\n",
      "Epoch 4 TRAIN loss: 1.14e-03  MSE: 9.58e-04  h-con:  1.79e+03   R2: 0.79  R2-dT/dt: 0.62   R2-precc: 0.960\n",
      "Epoch 4/10 complete, took 178.17 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 1.35e-03  runR2: 0.77,  elapsed 14.4s (compute 4.5)\n",
      "[5, 1600] Loss: 1.26e-03  runR2: 0.78,  elapsed 5.6s (compute 4.5)\n",
      "[5, 2400] Loss: 1.09e-03  runR2: 0.78,  elapsed 4.9s (compute 4.5)\n",
      "[5, 3200] Loss: 1.04e-03  runR2: 0.78,  elapsed 5.2s (compute 4.5)\n",
      "[5, 4000] Loss: 1.06e-03  runR2: 0.79,  elapsed 5.0s (compute 4.5)\n",
      "[5, 4800] Loss: 1.08e-03  runR2: 0.79,  elapsed 5.4s (compute 4.4)\n",
      "[5, 5600] Loss: 1.07e-03  runR2: 0.79,  elapsed 5.3s (compute 4.6)\n",
      "[5, 6400] Loss: 1.04e-03  runR2: 0.79,  elapsed 5.0s (compute 4.7)\n",
      "[5, 7200] Loss: 1.12e-03  runR2: 0.79,  elapsed 5.3s (compute 4.7)\n",
      "[5, 8000] Loss: 1.12e-03  runR2: 0.79,  elapsed 5.5s (compute 4.3)\n",
      "[5, 8800] Loss: 1.18e-03  runR2: 0.79,  elapsed 5.1s (compute 4.6)\n",
      "[5, 9600] Loss: 1.18e-03  runR2: 0.79,  elapsed 5.1s (compute 4.5)\n",
      "[5, 10400] Loss: 1.04e-03  runR2: 0.79,  elapsed 5.2s (compute 4.7)\n",
      "[5, 11200] Loss: 1.16e-03  runR2: 0.79,  elapsed 5.0s (compute 4.6)\n",
      "[5, 12000] Loss: 9.75e-04  runR2: 0.79,  elapsed 5.0s (compute 4.6)\n",
      "[5, 12800] Loss: 1.06e-03  runR2: 0.79,  elapsed 5.6s (compute 4.5)\n",
      "[5, 13600] Loss: 1.10e-03  runR2: 0.79,  elapsed 5.5s (compute 4.6)\n",
      "[5, 14400] Loss: 1.01e-03  runR2: 0.79,  elapsed 4.9s (compute 4.6)\n",
      "[5, 15200] Loss: 1.15e-03  runR2: 0.79,  elapsed 5.4s (compute 4.5)\n",
      "[5, 16000] Loss: 1.14e-03  runR2: 0.79,  elapsed 4.8s (compute 4.6)\n",
      "[5, 16800] Loss: 1.03e-03  runR2: 0.79,  elapsed 5.1s (compute 4.8)\n",
      "[5, 17600] Loss: 9.44e-04  runR2: 0.79,  elapsed 7.8s (compute 5.1)\n",
      "[5, 18400] Loss: 1.08e-03  runR2: 0.79,  elapsed 5.0s (compute 4.6)\n",
      "[5, 19200] Loss: 1.27e-03  runR2: 0.79,  elapsed 4.9s (compute 4.6)\n",
      "[5, 20000] Loss: 1.09e-03  runR2: 0.79,  elapsed 5.1s (compute 4.6)\n",
      "[5, 20800] Loss: 1.13e-03  runR2: 0.79,  elapsed 4.9s (compute 4.6)\n",
      "[5, 21600] Loss: 1.15e-03  runR2: 0.79,  elapsed 4.9s (compute 4.4)\n",
      "[5, 22400] Loss: 1.07e-03  runR2: 0.79,  elapsed 5.7s (compute 4.8)\n",
      "[5, 23200] Loss: 1.04e-03  runR2: 0.79,  elapsed 4.7s (compute 4.4)\n",
      "[5, 24000] Loss: 1.01e-03  runR2: 0.79,  elapsed 4.9s (compute 4.2)\n",
      "Epoch 5 TRAIN loss: 1.09e-03  MSE: 9.31e-04  h-con:  1.62e+03   R2: 0.79  R2-dT/dt: 0.63   R2-precc: 0.965\n",
      "Epoch 5/10 complete, took 166.98 seconds, autoreg window was 3\n",
      "Epoch 6 Training rollout timesteps: 4 \n",
      "[6, 800] Loss: 1.36e-03  runR2: 0.78,  elapsed 13.9s (compute 4.3)\n",
      "[6, 1600] Loss: 1.21e-03  runR2: 0.79,  elapsed 5.7s (compute 4.3)\n",
      "[6, 2400] Loss: 1.06e-03  runR2: 0.79,  elapsed 5.6s (compute 4.3)\n",
      "[6, 3200] Loss: 1.01e-03  runR2: 0.79,  elapsed 4.7s (compute 4.3)\n",
      "[6, 4000] Loss: 1.03e-03  runR2: 0.80,  elapsed 6.6s (compute 4.3)\n",
      "[6, 4800] Loss: 1.05e-03  runR2: 0.80,  elapsed 4.4s (compute 4.1)\n",
      "[6, 5600] Loss: 1.04e-03  runR2: 0.80,  elapsed 6.0s (compute 4.2)\n",
      "[6, 6400] Loss: 1.01e-03  runR2: 0.79,  elapsed 4.5s (compute 4.2)\n",
      "[6, 7200] Loss: 1.09e-03  runR2: 0.79,  elapsed 5.5s (compute 4.2)\n",
      "[6, 8000] Loss: 1.09e-03  runR2: 0.79,  elapsed 5.7s (compute 4.3)\n",
      "[6, 8800] Loss: 1.15e-03  runR2: 0.79,  elapsed 5.1s (compute 4.2)\n",
      "[6, 9600] Loss: 1.16e-03  runR2: 0.79,  elapsed 5.0s (compute 4.3)\n",
      "[6, 10400] Loss: 1.02e-03  runR2: 0.80,  elapsed 4.9s (compute 4.3)\n",
      "[6, 11200] Loss: 1.13e-03  runR2: 0.80,  elapsed 5.6s (compute 4.3)\n",
      "[6, 12000] Loss: 9.53e-04  runR2: 0.80,  elapsed 4.9s (compute 4.4)\n",
      "[6, 12800] Loss: 1.05e-03  runR2: 0.80,  elapsed 5.5s (compute 4.5)\n",
      "[6, 13600] Loss: 1.07e-03  runR2: 0.80,  elapsed 5.1s (compute 4.5)\n",
      "[6, 14400] Loss: 9.86e-04  runR2: 0.80,  elapsed 5.5s (compute 4.1)\n",
      "[6, 15200] Loss: 1.12e-03  runR2: 0.80,  elapsed 6.0s (compute 4.1)\n",
      "[6, 16000] Loss: 1.12e-03  runR2: 0.80,  elapsed 4.8s (compute 4.6)\n",
      "[6, 16800] Loss: 1.01e-03  runR2: 0.80,  elapsed 4.7s (compute 4.5)\n",
      "[6, 17600] Loss: 9.27e-04  runR2: 0.80,  elapsed 8.0s (compute 4.2)\n",
      "[6, 18400] Loss: 1.06e-03  runR2: 0.80,  elapsed 6.1s (compute 4.1)\n",
      "[6, 19200] Loss: 1.24e-03  runR2: 0.80,  elapsed 4.6s (compute 4.3)\n",
      "[6, 20000] Loss: 1.07e-03  runR2: 0.80,  elapsed 5.9s (compute 4.1)\n",
      "[6, 20800] Loss: 1.11e-03  runR2: 0.80,  elapsed 4.8s (compute 4.2)\n",
      "[6, 21600] Loss: 1.12e-03  runR2: 0.80,  elapsed 6.5s (compute 4.1)\n",
      "[6, 22400] Loss: 1.06e-03  runR2: 0.80,  elapsed 4.6s (compute 4.3)\n",
      "[6, 23200] Loss: 1.02e-03  runR2: 0.80,  elapsed 4.7s (compute 4.3)\n",
      "[6, 24000] Loss: 9.85e-04  runR2: 0.80,  elapsed 5.7s (compute 4.0)\n",
      "Epoch 6 TRAIN loss: 1.07e-03  MSE: 9.16e-04  h-con:  1.52e+03   R2: 0.80  R2-dT/dt: 0.63   R2-precc: 0.969\n",
      "Epoch 6/10 complete, took 171.08 seconds, autoreg window was 4\n",
      "Epoch 7 Training rollout timesteps: 5 \n",
      "[7, 800] Loss: 1.39e-03  runR2: 0.78,  elapsed 14.1s (compute 3.8)\n",
      "[7, 1600] Loss: 1.19e-03  runR2: 0.79,  elapsed 6.1s (compute 4.4)\n",
      "[7, 2400] Loss: 1.05e-03  runR2: 0.79,  elapsed 4.8s (compute 4.5)\n",
      "[7, 3200] Loss: 9.97e-04  runR2: 0.79,  elapsed 6.0s (compute 4.1)\n",
      "[7, 4000] Loss: 1.01e-03  runR2: 0.80,  elapsed 4.9s (compute 3.9)\n",
      "[7, 4800] Loss: 1.03e-03  runR2: 0.80,  elapsed 6.2s (compute 4.3)\n",
      "[7, 5600] Loss: 1.02e-03  runR2: 0.80,  elapsed 5.3s (compute 4.5)\n",
      "[7, 6400] Loss: 9.93e-04  runR2: 0.80,  elapsed 4.3s (compute 3.9)\n",
      "[7, 7200] Loss: 1.08e-03  runR2: 0.79,  elapsed 6.5s (compute 4.1)\n",
      "[7, 8000] Loss: 1.07e-03  runR2: 0.80,  elapsed 5.8s (compute 4.2)\n",
      "[7, 8800] Loss: 1.13e-03  runR2: 0.80,  elapsed 5.5s (compute 4.1)\n",
      "[7, 9600] Loss: 1.14e-03  runR2: 0.80,  elapsed 4.5s (compute 4.2)\n",
      "[7, 10400] Loss: 1.01e-03  runR2: 0.80,  elapsed 5.6s (compute 4.1)\n",
      "[7, 11200] Loss: 1.11e-03  runR2: 0.80,  elapsed 4.4s (compute 4.1)\n",
      "[7, 12000] Loss: 9.36e-04  runR2: 0.80,  elapsed 5.4s (compute 3.9)\n",
      "[7, 12800] Loss: 1.02e-03  runR2: 0.80,  elapsed 4.5s (compute 3.9)\n",
      "[7, 13600] Loss: 1.05e-03  runR2: 0.80,  elapsed 5.7s (compute 4.1)\n",
      "[7, 14400] Loss: 9.70e-04  runR2: 0.80,  elapsed 4.5s (compute 4.2)\n",
      "[7, 15200] Loss: 1.10e-03  runR2: 0.80,  elapsed 6.9s (compute 4.7)\n",
      "[7, 16000] Loss: 1.10e-03  runR2: 0.80,  elapsed 4.5s (compute 4.2)\n",
      "[7, 16800] Loss: 9.97e-04  runR2: 0.80,  elapsed 4.4s (compute 4.2)\n",
      "[7, 17600] Loss: 9.14e-04  runR2: 0.80,  elapsed 7.7s (compute 4.2)\n",
      "[7, 18400] Loss: 1.05e-03  runR2: 0.80,  elapsed 4.9s (compute 4.1)\n",
      "[7, 19200] Loss: 1.22e-03  runR2: 0.80,  elapsed 5.0s (compute 4.2)\n",
      "[7, 20000] Loss: 1.05e-03  runR2: 0.80,  elapsed 5.1s (compute 4.2)\n",
      "[7, 20800] Loss: 1.09e-03  runR2: 0.80,  elapsed 4.5s (compute 4.2)\n",
      "[7, 21600] Loss: 1.11e-03  runR2: 0.80,  elapsed 5.5s (compute 4.1)\n",
      "[7, 22400] Loss: 1.04e-03  runR2: 0.80,  elapsed 5.5s (compute 4.2)\n",
      "[7, 23200] Loss: 1.01e-03  runR2: 0.80,  elapsed 4.2s (compute 3.9)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     timewindoww\u001b[38;5;241m=\u001b[39mtimewindow\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Training rollout timesteps: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, timewindoww))\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimewindoww\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_wandb: wandb\u001b[38;5;241m.\u001b[39mlog(train_runner\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_val:\n",
      "Cell \u001b[0;32mIn[35], line 84\u001b[0m, in \u001b[0;36mmodel_train_eval.eval_one_epoch\u001b[0;34m(self, epoch, timewindow)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mp_autocast:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, dtype\u001b[38;5;241m=\u001b[39mdtype):\n\u001b[0;32m---> 84\u001b[0m         pred_lay, pred_sfc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_lay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_sfc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     pred_lay, pred_sfc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs_lay, inputs_sfc)\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 189\u001b[0m, in \u001b[0;36mRNN_autoreg.forward\u001b[0;34m(self, inputs_main, inputs_sfc)\u001b[0m\n\u001b[1;32m    185\u001b[0m     rnn1_input \u001b[38;5;241m=\u001b[39m inputs_main \n\u001b[1;32m    187\u001b[0m rnn1_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((rnn1_input,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn1_mem), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 189\u001b[0m rnn1out, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn1_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m rnn1out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflip(rnn1out, [\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    193\u001b[0m hx2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnh_rnn2),dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (batch, hidden_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 128 128\n",
    "# autoreg, hybrid-loss\n",
    "num_epochs = 10\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd4506db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n",
      "[1, 800] Loss: 4.16e-03  runR2: -0.57,  elapsed 11.6s (compute 5.8)\n",
      "[1, 1600] Loss: 2.27e-03  runR2: -0.05,  elapsed 5.7s (compute 5.4)\n",
      "[1, 2400] Loss: 2.02e-03  runR2: 0.19,  elapsed 6.6s (compute 6.3)\n",
      "[1, 3200] Loss: 1.93e-03  runR2: 0.31,  elapsed 6.8s (compute 6.4)\n",
      "[1, 4000] Loss: 1.74e-03  runR2: 0.40,  elapsed 7.1s (compute 6.8)\n",
      "[1, 4800] Loss: 1.97e-03  runR2: 0.44,  elapsed 6.9s (compute 6.6)\n",
      "[1, 5600] Loss: 1.93e-03  runR2: 0.47,  elapsed 7.1s (compute 6.8)\n",
      "[1, 6400] Loss: 1.67e-03  runR2: 0.49,  elapsed 5.9s (compute 5.6)\n",
      "[1, 7200] Loss: 1.51e-03  runR2: 0.51,  elapsed 7.6s (compute 7.0)\n",
      "[1, 8000] Loss: 1.52e-03  runR2: 0.52,  elapsed 5.9s (compute 5.6)\n",
      "[1, 8800] Loss: 1.61e-03  runR2: 0.54,  elapsed 5.9s (compute 5.6)\n",
      "[1, 9600] Loss: 1.50e-03  runR2: 0.55,  elapsed 5.9s (compute 5.6)\n",
      "[1, 10400] Loss: 1.70e-03  runR2: 0.55,  elapsed 5.9s (compute 5.6)\n",
      "[1, 11200] Loss: 1.58e-03  runR2: 0.56,  elapsed 5.5s (compute 5.2)\n",
      "[1, 12000] Loss: 1.42e-03  runR2: 0.57,  elapsed 5.5s (compute 5.2)\n",
      "[1, 12800] Loss: 1.54e-03  runR2: 0.57,  elapsed 6.4s (compute 6.1)\n",
      "[1, 13600] Loss: 1.41e-03  runR2: 0.58,  elapsed 7.0s (compute 6.7)\n",
      "[1, 14400] Loss: 1.26e-03  runR2: 0.58,  elapsed 7.3s (compute 6.6)\n",
      "[1, 15200] Loss: 1.37e-03  runR2: 0.59,  elapsed 6.5s (compute 6.3)\n",
      "[1, 16000] Loss: 1.57e-03  runR2: 0.59,  elapsed 6.6s (compute 6.2)\n",
      "[1, 16800] Loss: 1.40e-03  runR2: 0.60,  elapsed 6.3s (compute 6.0)\n",
      "[1, 17600] Loss: 1.20e-03  runR2: 0.60,  elapsed 6.1s (compute 5.8)\n",
      "[1, 18400] Loss: 1.31e-03  runR2: 0.61,  elapsed 5.9s (compute 5.6)\n",
      "[1, 19200] Loss: 1.33e-03  runR2: 0.61,  elapsed 5.5s (compute 5.2)\n",
      "[1, 20000] Loss: 1.26e-03  runR2: 0.62,  elapsed 5.4s (compute 5.2)\n",
      "[1, 20800] Loss: 1.42e-03  runR2: 0.62,  elapsed 5.5s (compute 5.1)\n",
      "[1, 21600] Loss: 1.23e-03  runR2: 0.63,  elapsed 5.7s (compute 5.1)\n",
      "[1, 22400] Loss: 1.34e-03  runR2: 0.63,  elapsed 5.5s (compute 5.2)\n",
      "[1, 23200] Loss: 1.20e-03  runR2: 0.63,  elapsed 5.4s (compute 5.1)\n",
      "[1, 24000] Loss: 1.33e-03  runR2: 0.64,  elapsed 5.4s (compute 5.1)\n",
      "[1, 24800] Loss: 1.33e-03  runR2: 0.64,  elapsed 5.4s (compute 5.2)\n",
      "[1, 25600] Loss: 1.22e-03  runR2: 0.64,  elapsed 5.4s (compute 5.1)\n",
      "[1, 26400] Loss: 1.26e-03  runR2: 0.65,  elapsed 5.4s (compute 5.1)\n",
      "[1, 27200] Loss: 1.18e-03  runR2: 0.65,  elapsed 5.4s (compute 5.2)\n",
      "[1, 28000] Loss: 1.30e-03  runR2: 0.65,  elapsed 6.4s (compute 6.1)\n",
      "[1, 28800] Loss: 1.47e-03  runR2: 0.66,  elapsed 6.1s (compute 5.5)\n",
      "[1, 29600] Loss: 1.23e-03  runR2: 0.66,  elapsed 5.9s (compute 5.6)\n",
      "[1, 30400] Loss: 1.18e-03  runR2: 0.66,  elapsed 7.0s (compute 6.6)\n",
      "[1, 31200] Loss: 1.26e-03  runR2: 0.66,  elapsed 6.3s (compute 5.9)\n",
      "[1, 32000] Loss: 1.19e-03  runR2: 0.67,  elapsed 5.4s (compute 5.1)\n",
      "[1, 32800] Loss: 1.34e-03  runR2: 0.67,  elapsed 6.0s (compute 5.7)\n",
      "[1, 33600] Loss: 1.35e-03  runR2: 0.67,  elapsed 6.9s (compute 6.5)\n",
      "[1, 34400] Loss: 1.29e-03  runR2: 0.67,  elapsed 6.9s (compute 6.6)\n",
      "[1, 35200] Loss: 1.42e-03  runR2: 0.67,  elapsed 6.9s (compute 6.6)\n",
      "[1, 36000] Loss: 1.28e-03  runR2: 0.67,  elapsed 7.8s (compute 7.2)\n",
      "[1, 36800] Loss: 1.23e-03  runR2: 0.68,  elapsed 6.9s (compute 6.6)\n",
      "[1, 37600] Loss: 1.30e-03  runR2: 0.68,  elapsed 6.6s (compute 6.3)\n",
      "[1, 38400] Loss: 1.23e-03  runR2: 0.68,  elapsed 6.3s (compute 6.0)\n",
      "[1, 39200] Loss: 1.19e-03  runR2: 0.68,  elapsed 6.3s (compute 6.0)\n",
      "[1, 40000] Loss: 1.05e-03  runR2: 0.68,  elapsed 6.4s (compute 6.1)\n",
      "[1, 40800] Loss: 1.19e-03  runR2: 0.68,  elapsed 6.5s (compute 6.2)\n",
      "[1, 41600] Loss: 1.27e-03  runR2: 0.68,  elapsed 7.8s (compute 7.5)\n",
      "[1, 42400] Loss: 1.19e-03  runR2: 0.69,  elapsed 6.3s (compute 6.0)\n",
      "[1, 43200] Loss: 1.21e-03  runR2: 0.69,  elapsed 5.8s (compute 5.2)\n",
      "[1, 44000] Loss: 1.13e-03  runR2: 0.69,  elapsed 6.3s (compute 5.9)\n",
      "[1, 44800] Loss: 1.41e-03  runR2: 0.69,  elapsed 5.9s (compute 5.6)\n",
      "[1, 45600] Loss: 1.23e-03  runR2: 0.69,  elapsed 6.9s (compute 6.5)\n",
      "[1, 46400] Loss: 1.13e-03  runR2: 0.69,  elapsed 6.4s (compute 6.0)\n",
      "[1, 47200] Loss: 1.13e-03  runR2: 0.69,  elapsed 6.6s (compute 6.3)\n",
      "[1, 48000] Loss: 1.15e-03  runR2: 0.69,  elapsed 6.7s (compute 6.4)\n",
      "[1, 48800] Loss: 1.17e-03  runR2: 0.70,  elapsed 6.0s (compute 5.7)\n",
      "[1, 49600] Loss: 1.30e-03  runR2: 0.70,  elapsed 6.4s (compute 6.1)\n",
      "Epoch 1 TRAIN loss: 1.40e-03  MSE: 1.16e-03  h-con:  2.44e+03   R2: 0.70  R2-dT/dt: -0.64   R2-dq/dt: 0.50   R2-precc: 0.916\n",
      "Epoch 1/10 complete, took 400.21 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.31e-03  runR2: 0.76,  elapsed 12.6s (compute 6.6)\n",
      "[2, 1600] Loss: 1.15e-03  runR2: 0.76,  elapsed 5.9s (compute 5.6)\n",
      "[2, 2400] Loss: 1.12e-03  runR2: 0.77,  elapsed 6.7s (compute 6.3)\n",
      "[2, 3200] Loss: 1.12e-03  runR2: 0.77,  elapsed 6.7s (compute 6.4)\n",
      "[2, 4000] Loss: 1.11e-03  runR2: 0.78,  elapsed 6.2s (compute 5.9)\n",
      "[2, 4800] Loss: 1.32e-03  runR2: 0.78,  elapsed 6.1s (compute 5.8)\n",
      "[2, 5600] Loss: 1.38e-03  runR2: 0.78,  elapsed 6.4s (compute 6.0)\n",
      "[2, 6400] Loss: 1.18e-03  runR2: 0.78,  elapsed 5.9s (compute 5.6)\n",
      "[2, 7200] Loss: 1.11e-03  runR2: 0.78,  elapsed 6.4s (compute 5.8)\n",
      "[2, 8000] Loss: 1.13e-03  runR2: 0.78,  elapsed 6.3s (compute 6.0)\n",
      "[2, 8800] Loss: 1.22e-03  runR2: 0.78,  elapsed 5.6s (compute 5.4)\n",
      "[2, 9600] Loss: 1.18e-03  runR2: 0.78,  elapsed 6.0s (compute 5.7)\n",
      "[2, 10400] Loss: 1.35e-03  runR2: 0.78,  elapsed 6.2s (compute 5.8)\n",
      "[2, 11200] Loss: 1.27e-03  runR2: 0.78,  elapsed 5.9s (compute 5.6)\n",
      "[2, 12000] Loss: 1.18e-03  runR2: 0.78,  elapsed 5.7s (compute 5.4)\n",
      "[2, 12800] Loss: 1.26e-03  runR2: 0.78,  elapsed 5.5s (compute 5.2)\n",
      "[2, 13600] Loss: 1.17e-03  runR2: 0.78,  elapsed 5.5s (compute 5.2)\n",
      "[2, 14400] Loss: 1.03e-03  runR2: 0.78,  elapsed 6.0s (compute 5.3)\n",
      "[2, 15200] Loss: 1.14e-03  runR2: 0.78,  elapsed 5.5s (compute 5.2)\n",
      "[2, 16000] Loss: 1.32e-03  runR2: 0.78,  elapsed 5.5s (compute 5.2)\n",
      "[2, 16800] Loss: 1.19e-03  runR2: 0.78,  elapsed 5.7s (compute 5.4)\n",
      "[2, 17600] Loss: 1.03e-03  runR2: 0.78,  elapsed 6.0s (compute 5.6)\n",
      "[2, 18400] Loss: 1.12e-03  runR2: 0.79,  elapsed 6.5s (compute 6.1)\n",
      "[2, 19200] Loss: 1.16e-03  runR2: 0.79,  elapsed 6.8s (compute 6.4)\n",
      "[2, 20000] Loss: 1.10e-03  runR2: 0.79,  elapsed 6.4s (compute 6.0)\n",
      "[2, 20800] Loss: 1.26e-03  runR2: 0.79,  elapsed 6.5s (compute 6.0)\n",
      "[2, 21600] Loss: 1.09e-03  runR2: 0.79,  elapsed 6.9s (compute 6.1)\n",
      "[2, 22400] Loss: 1.19e-03  runR2: 0.79,  elapsed 6.4s (compute 6.1)\n",
      "[2, 23200] Loss: 1.07e-03  runR2: 0.79,  elapsed 5.8s (compute 5.5)\n",
      "[2, 24000] Loss: 1.20e-03  runR2: 0.79,  elapsed 6.1s (compute 5.8)\n",
      "[2, 24800] Loss: 1.21e-03  runR2: 0.79,  elapsed 5.9s (compute 5.6)\n",
      "[2, 25600] Loss: 1.10e-03  runR2: 0.79,  elapsed 6.1s (compute 5.8)\n",
      "[2, 26400] Loss: 1.14e-03  runR2: 0.79,  elapsed 6.1s (compute 5.8)\n",
      "[2, 27200] Loss: 1.07e-03  runR2: 0.79,  elapsed 6.4s (compute 6.0)\n",
      "[2, 28000] Loss: 1.18e-03  runR2: 0.79,  elapsed 6.9s (compute 6.6)\n",
      "[2, 28800] Loss: 1.34e-03  runR2: 0.79,  elapsed 6.7s (compute 6.0)\n",
      "[2, 29600] Loss: 1.12e-03  runR2: 0.79,  elapsed 6.2s (compute 5.8)\n",
      "[2, 30400] Loss: 1.08e-03  runR2: 0.79,  elapsed 5.8s (compute 5.4)\n",
      "[2, 31200] Loss: 1.17e-03  runR2: 0.79,  elapsed 6.1s (compute 5.7)\n",
      "[2, 32000] Loss: 1.09e-03  runR2: 0.79,  elapsed 6.9s (compute 6.6)\n",
      "[2, 32800] Loss: 1.24e-03  runR2: 0.79,  elapsed 7.1s (compute 6.7)\n",
      "[2, 33600] Loss: 1.25e-03  runR2: 0.79,  elapsed 7.0s (compute 6.6)\n",
      "[2, 34400] Loss: 1.19e-03  runR2: 0.79,  elapsed 7.0s (compute 6.6)\n",
      "[2, 35200] Loss: 1.31e-03  runR2: 0.79,  elapsed 7.2s (compute 6.8)\n",
      "[2, 36000] Loss: 1.20e-03  runR2: 0.79,  elapsed 7.4s (compute 6.6)\n",
      "[2, 36800] Loss: 1.15e-03  runR2: 0.79,  elapsed 6.8s (compute 6.4)\n",
      "[2, 37600] Loss: 1.22e-03  runR2: 0.79,  elapsed 6.7s (compute 6.2)\n",
      "[2, 38400] Loss: 1.15e-03  runR2: 0.79,  elapsed 7.0s (compute 6.6)\n",
      "[2, 39200] Loss: 1.11e-03  runR2: 0.79,  elapsed 6.3s (compute 6.0)\n",
      "[2, 40000] Loss: 9.87e-04  runR2: 0.79,  elapsed 6.1s (compute 5.7)\n",
      "[2, 40800] Loss: 1.11e-03  runR2: 0.79,  elapsed 6.1s (compute 5.7)\n",
      "[2, 41600] Loss: 1.19e-03  runR2: 0.79,  elapsed 6.6s (compute 6.2)\n",
      "[2, 42400] Loss: 1.11e-03  runR2: 0.79,  elapsed 6.1s (compute 5.8)\n",
      "[2, 43200] Loss: 1.14e-03  runR2: 0.79,  elapsed 6.8s (compute 6.0)\n",
      "[2, 44000] Loss: 1.06e-03  runR2: 0.79,  elapsed 5.8s (compute 5.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 44800] Loss: 1.32e-03  runR2: 0.79,  elapsed 6.0s (compute 5.5)\n",
      "[2, 45600] Loss: 1.16e-03  runR2: 0.79,  elapsed 6.7s (compute 6.3)\n",
      "[2, 46400] Loss: 1.06e-03  runR2: 0.79,  elapsed 6.7s (compute 6.2)\n",
      "[2, 47200] Loss: 1.06e-03  runR2: 0.79,  elapsed 6.3s (compute 6.0)\n",
      "[2, 48000] Loss: 1.08e-03  runR2: 0.79,  elapsed 6.1s (compute 5.8)\n",
      "[2, 48800] Loss: 1.11e-03  runR2: 0.79,  elapsed 6.8s (compute 6.4)\n",
      "[2, 49600] Loss: 1.22e-03  runR2: 0.79,  elapsed 6.9s (compute 6.5)\n",
      "Epoch 2 TRAIN loss: 1.17e-03  MSE: 9.77e-04  h-con:  1.90e+03   R2: 0.80  R2-dT/dt: 0.68   R2-dq/dt: 0.65   R2-precc: 0.959\n",
      "Epoch 2/10 complete, took 404.83 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.41e-03  runR2: 0.79,  elapsed 12.6s (compute 6.7)\n",
      "[3, 1600] Loss: 1.09e-03  runR2: 0.79,  elapsed 6.3s (compute 5.9)\n",
      "[3, 2400] Loss: 1.06e-03  runR2: 0.80,  elapsed 6.6s (compute 6.2)\n",
      "[3, 3200] Loss: 1.06e-03  runR2: 0.80,  elapsed 6.4s (compute 6.0)\n",
      "[3, 4000] Loss: 1.06e-03  runR2: 0.81,  elapsed 6.3s (compute 5.9)\n",
      "[3, 4800] Loss: 1.27e-03  runR2: 0.81,  elapsed 5.9s (compute 5.6)\n",
      "[3, 5600] Loss: 1.32e-03  runR2: 0.81,  elapsed 6.0s (compute 5.6)\n",
      "[3, 6400] Loss: 1.13e-03  runR2: 0.81,  elapsed 5.8s (compute 5.5)\n",
      "[3, 7200] Loss: 1.06e-03  runR2: 0.80,  elapsed 6.3s (compute 5.5)\n",
      "[3, 8000] Loss: 1.08e-03  runR2: 0.80,  elapsed 5.9s (compute 5.5)\n",
      "[3, 8800] Loss: 1.16e-03  runR2: 0.81,  elapsed 5.9s (compute 5.5)\n",
      "[3, 9600] Loss: 1.13e-03  runR2: 0.81,  elapsed 5.8s (compute 5.5)\n",
      "[3, 10400] Loss: 1.29e-03  runR2: 0.81,  elapsed 5.8s (compute 5.5)\n",
      "[3, 11200] Loss: 1.22e-03  runR2: 0.81,  elapsed 5.8s (compute 5.5)\n",
      "[3, 12000] Loss: 1.13e-03  runR2: 0.81,  elapsed 5.8s (compute 5.5)\n",
      "[3, 12800] Loss: 1.21e-03  runR2: 0.80,  elapsed 5.5s (compute 5.2)\n",
      "[3, 13600] Loss: 1.13e-03  runR2: 0.80,  elapsed 5.5s (compute 5.2)\n",
      "[3, 14400] Loss: 9.84e-04  runR2: 0.80,  elapsed 5.9s (compute 5.2)\n",
      "[3, 15200] Loss: 1.10e-03  runR2: 0.81,  elapsed 5.5s (compute 5.2)\n",
      "[3, 16000] Loss: 1.27e-03  runR2: 0.81,  elapsed 5.9s (compute 5.5)\n",
      "[3, 16800] Loss: 1.14e-03  runR2: 0.81,  elapsed 6.1s (compute 5.8)\n",
      "[3, 17600] Loss: 9.86e-04  runR2: 0.81,  elapsed 5.5s (compute 5.2)\n",
      "[3, 18400] Loss: 1.08e-03  runR2: 0.81,  elapsed 6.1s (compute 5.7)\n",
      "[3, 19200] Loss: 1.11e-03  runR2: 0.81,  elapsed 6.0s (compute 5.7)\n",
      "[3, 20000] Loss: 1.06e-03  runR2: 0.81,  elapsed 5.9s (compute 5.6)\n",
      "[3, 20800] Loss: 1.21e-03  runR2: 0.81,  elapsed 6.1s (compute 5.7)\n",
      "[3, 21600] Loss: 1.05e-03  runR2: 0.81,  elapsed 7.0s (compute 6.3)\n",
      "[3, 22400] Loss: 1.15e-03  runR2: 0.81,  elapsed 6.0s (compute 5.7)\n",
      "[3, 23200] Loss: 1.03e-03  runR2: 0.81,  elapsed 6.2s (compute 5.9)\n",
      "[3, 24000] Loss: 1.16e-03  runR2: 0.81,  elapsed 6.2s (compute 5.9)\n",
      "[3, 24800] Loss: 1.16e-03  runR2: 0.81,  elapsed 6.4s (compute 6.1)\n",
      "[3, 25600] Loss: 1.06e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 26400] Loss: 1.11e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 27200] Loss: 1.04e-03  runR2: 0.81,  elapsed 6.3s (compute 6.0)\n",
      "[3, 28000] Loss: 1.15e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 28800] Loss: 1.30e-03  runR2: 0.81,  elapsed 6.1s (compute 5.4)\n",
      "[3, 29600] Loss: 1.09e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 30400] Loss: 1.05e-03  runR2: 0.81,  elapsed 6.8s (compute 6.4)\n",
      "[3, 31200] Loss: 1.13e-03  runR2: 0.81,  elapsed 6.7s (compute 6.3)\n",
      "[3, 32000] Loss: 1.06e-03  runR2: 0.81,  elapsed 6.1s (compute 5.8)\n",
      "[3, 32800] Loss: 1.20e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 33600] Loss: 1.22e-03  runR2: 0.81,  elapsed 6.1s (compute 5.7)\n",
      "[3, 34400] Loss: 1.15e-03  runR2: 0.81,  elapsed 6.6s (compute 6.3)\n",
      "[3, 35200] Loss: 1.27e-03  runR2: 0.81,  elapsed 6.1s (compute 5.8)\n",
      "[3, 36000] Loss: 1.16e-03  runR2: 0.81,  elapsed 6.1s (compute 5.4)\n",
      "[3, 36800] Loss: 1.12e-03  runR2: 0.81,  elapsed 5.9s (compute 5.5)\n",
      "[3, 37600] Loss: 1.19e-03  runR2: 0.81,  elapsed 6.2s (compute 5.8)\n",
      "[3, 38400] Loss: 1.12e-03  runR2: 0.81,  elapsed 5.8s (compute 5.4)\n",
      "[3, 39200] Loss: 1.08e-03  runR2: 0.81,  elapsed 5.8s (compute 5.3)\n",
      "[3, 40000] Loss: 9.57e-04  runR2: 0.81,  elapsed 6.1s (compute 5.8)\n",
      "[3, 40800] Loss: 1.08e-03  runR2: 0.81,  elapsed 6.0s (compute 5.7)\n",
      "[3, 41600] Loss: 1.16e-03  runR2: 0.81,  elapsed 6.5s (compute 6.1)\n",
      "[3, 42400] Loss: 1.08e-03  runR2: 0.81,  elapsed 5.5s (compute 5.2)\n",
      "[3, 43200] Loss: 1.11e-03  runR2: 0.81,  elapsed 6.6s (compute 5.9)\n",
      "[3, 44000] Loss: 1.02e-03  runR2: 0.81,  elapsed 6.1s (compute 5.7)\n",
      "[3, 44800] Loss: 1.29e-03  runR2: 0.81,  elapsed 6.5s (compute 6.1)\n",
      "[3, 45600] Loss: 1.13e-03  runR2: 0.81,  elapsed 6.3s (compute 6.0)\n",
      "[3, 46400] Loss: 1.03e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 47200] Loss: 1.03e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 48000] Loss: 1.06e-03  runR2: 0.81,  elapsed 6.0s (compute 5.7)\n",
      "[3, 48800] Loss: 1.08e-03  runR2: 0.81,  elapsed 6.7s (compute 6.3)\n",
      "[3, 49600] Loss: 1.19e-03  runR2: 0.81,  elapsed 6.0s (compute 5.7)\n",
      "Epoch 3 TRAIN loss: 1.12e-03  MSE: 9.48e-04  h-con:  1.77e+03   R2: 0.81  R2-dT/dt: 0.71   R2-dq/dt: 0.70   R2-precc: 0.964\n",
      "Epoch 3/10 complete, took 391.93 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 1.57e-03  runR2: 0.79,  elapsed 10.9s (compute 4.8)\n",
      "[4, 1600] Loss: 1.06e-03  runR2: 0.80,  elapsed 5.4s (compute 5.0)\n",
      "[4, 2400] Loss: 1.03e-03  runR2: 0.80,  elapsed 4.9s (compute 4.6)\n",
      "[4, 3200] Loss: 1.02e-03  runR2: 0.81,  elapsed 4.7s (compute 4.3)\n",
      "[4, 4000] Loss: 1.01e-03  runR2: 0.81,  elapsed 5.0s (compute 4.7)\n",
      "[4, 4800] Loss: 1.22e-03  runR2: 0.81,  elapsed 5.7s (compute 4.9)\n",
      "[4, 5600] Loss: 1.26e-03  runR2: 0.81,  elapsed 4.7s (compute 4.4)\n",
      "[4, 6400] Loss: 1.09e-03  runR2: 0.81,  elapsed 4.9s (compute 4.5)\n",
      "[4, 7200] Loss: 1.01e-03  runR2: 0.81,  elapsed 5.1s (compute 4.4)\n",
      "[4, 8000] Loss: 1.04e-03  runR2: 0.81,  elapsed 4.7s (compute 4.4)\n",
      "[4, 8800] Loss: 1.11e-03  runR2: 0.81,  elapsed 4.8s (compute 4.5)\n",
      "[4, 9600] Loss: 1.08e-03  runR2: 0.81,  elapsed 4.6s (compute 4.3)\n",
      "[4, 10400] Loss: 1.23e-03  runR2: 0.81,  elapsed 5.4s (compute 5.1)\n",
      "[4, 11200] Loss: 1.16e-03  runR2: 0.81,  elapsed 5.2s (compute 4.9)\n",
      "[4, 12000] Loss: 1.09e-03  runR2: 0.81,  elapsed 5.2s (compute 5.0)\n",
      "[4, 12800] Loss: 1.16e-03  runR2: 0.81,  elapsed 4.6s (compute 4.3)\n",
      "[4, 13600] Loss: 1.08e-03  runR2: 0.81,  elapsed 5.3s (compute 5.0)\n",
      "[4, 14400] Loss: 9.38e-04  runR2: 0.81,  elapsed 5.7s (compute 5.0)\n",
      "[4, 15200] Loss: 1.04e-03  runR2: 0.81,  elapsed 5.3s (compute 5.0)\n",
      "[4, 16000] Loss: 1.21e-03  runR2: 0.81,  elapsed 5.3s (compute 4.9)\n",
      "[4, 16800] Loss: 1.08e-03  runR2: 0.81,  elapsed 4.9s (compute 4.6)\n",
      "[4, 17600] Loss: 9.40e-04  runR2: 0.81,  elapsed 4.7s (compute 4.4)\n",
      "[4, 18400] Loss: 1.03e-03  runR2: 0.81,  elapsed 4.9s (compute 4.6)\n",
      "[4, 19200] Loss: 1.06e-03  runR2: 0.81,  elapsed 4.8s (compute 4.5)\n",
      "[4, 20000] Loss: 1.01e-03  runR2: 0.81,  elapsed 5.1s (compute 4.8)\n",
      "[4, 20800] Loss: 1.15e-03  runR2: 0.81,  elapsed 4.8s (compute 4.4)\n",
      "[4, 21600] Loss: 9.96e-04  runR2: 0.81,  elapsed 5.4s (compute 4.7)\n",
      "[4, 22400] Loss: 1.10e-03  runR2: 0.81,  elapsed 4.9s (compute 4.5)\n",
      "[4, 23200] Loss: 9.76e-04  runR2: 0.81,  elapsed 5.5s (compute 5.0)\n",
      "[4, 24000] Loss: 1.10e-03  runR2: 0.81,  elapsed 5.5s (compute 5.0)\n",
      "[4, 24800] Loss: 1.11e-03  runR2: 0.81,  elapsed 5.3s (compute 5.0)\n",
      "[4, 25600] Loss: 1.01e-03  runR2: 0.82,  elapsed 5.4s (compute 5.1)\n",
      "[4, 26400] Loss: 1.06e-03  runR2: 0.82,  elapsed 5.4s (compute 5.0)\n",
      "[4, 27200] Loss: 9.92e-04  runR2: 0.82,  elapsed 5.3s (compute 4.9)\n",
      "[4, 28000] Loss: 1.09e-03  runR2: 0.82,  elapsed 5.3s (compute 5.0)\n",
      "[4, 28800] Loss: 1.23e-03  runR2: 0.82,  elapsed 6.2s (compute 5.5)\n",
      "[4, 29600] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.2s (compute 4.9)\n",
      "[4, 30400] Loss: 1.00e-03  runR2: 0.82,  elapsed 5.1s (compute 4.8)\n",
      "[4, 31200] Loss: 1.07e-03  runR2: 0.82,  elapsed 5.2s (compute 4.9)\n",
      "[4, 32000] Loss: 1.02e-03  runR2: 0.82,  elapsed 5.3s (compute 4.9)\n",
      "[4, 32800] Loss: 1.15e-03  runR2: 0.82,  elapsed 5.3s (compute 4.9)\n",
      "[4, 33600] Loss: 1.16e-03  runR2: 0.82,  elapsed 5.6s (compute 5.1)\n",
      "[4, 34400] Loss: 1.11e-03  runR2: 0.82,  elapsed 5.4s (compute 5.1)\n",
      "[4, 35200] Loss: 1.21e-03  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[4, 36000] Loss: 1.11e-03  runR2: 0.82,  elapsed 5.0s (compute 4.3)\n",
      "[4, 36800] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 37600] Loss: 1.13e-03  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[4, 38400] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[4, 39200] Loss: 1.02e-03  runR2: 0.82,  elapsed 5.0s (compute 4.6)\n",
      "[4, 40000] Loss: 9.09e-04  runR2: 0.82,  elapsed 5.3s (compute 4.9)\n",
      "[4, 40800] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[4, 41600] Loss: 1.10e-03  runR2: 0.82,  elapsed 4.6s (compute 4.3)\n",
      "[4, 42400] Loss: 1.03e-03  runR2: 0.82,  elapsed 5.1s (compute 4.7)\n",
      "[4, 43200] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.9s (compute 4.3)\n",
      "[4, 44000] Loss: 9.72e-04  runR2: 0.82,  elapsed 5.0s (compute 4.7)\n",
      "[4, 44800] Loss: 1.23e-03  runR2: 0.82,  elapsed 5.1s (compute 4.7)\n",
      "[4, 45600] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.9s (compute 4.6)\n",
      "[4, 46400] Loss: 9.80e-04  runR2: 0.82,  elapsed 5.0s (compute 4.6)\n",
      "[4, 47200] Loss: 9.80e-04  runR2: 0.81,  elapsed 5.3s (compute 4.9)\n",
      "[4, 48000] Loss: 1.00e-03  runR2: 0.81,  elapsed 4.9s (compute 4.5)\n",
      "[4, 48800] Loss: 1.03e-03  runR2: 0.81,  elapsed 5.2s (compute 4.9)\n",
      "[4, 49600] Loss: 1.13e-03  runR2: 0.81,  elapsed 4.9s (compute 4.6)\n",
      "Epoch 4 TRAIN loss: 1.07e-03  MSE: 9.16e-04  h-con:  1.58e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.73   R2-precc: 0.970\n",
      "Epoch 4/10 complete, took 326.71 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 1.63e-03  runR2: 0.80,  elapsed 10.9s (compute 4.4)\n",
      "[5, 1600] Loss: 1.01e-03  runR2: 0.81,  elapsed 5.0s (compute 4.6)\n",
      "[5, 2400] Loss: 9.79e-04  runR2: 0.81,  elapsed 5.0s (compute 4.6)\n",
      "[5, 3200] Loss: 9.69e-04  runR2: 0.81,  elapsed 5.4s (compute 5.0)\n",
      "[5, 4000] Loss: 9.67e-04  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[5, 4800] Loss: 1.17e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 5600] Loss: 1.21e-03  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[5, 6400] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.0s (compute 4.7)\n",
      "[5, 7200] Loss: 9.74e-04  runR2: 0.81,  elapsed 5.4s (compute 4.7)\n",
      "[5, 8000] Loss: 9.96e-04  runR2: 0.81,  elapsed 5.1s (compute 4.6)\n",
      "[5, 8800] Loss: 1.07e-03  runR2: 0.82,  elapsed 5.0s (compute 4.7)\n",
      "[5, 9600] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.0s (compute 4.5)\n",
      "[5, 10400] Loss: 1.18e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 11200] Loss: 1.12e-03  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[5, 12000] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 12800] Loss: 1.11e-03  runR2: 0.81,  elapsed 4.9s (compute 4.5)\n",
      "[5, 13600] Loss: 1.05e-03  runR2: 0.81,  elapsed 4.8s (compute 4.4)\n",
      "[5, 14400] Loss: 9.08e-04  runR2: 0.81,  elapsed 5.3s (compute 4.6)\n",
      "[5, 15200] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[5, 16000] Loss: 1.17e-03  runR2: 0.81,  elapsed 3.9s (compute 3.6)\n",
      "[5, 16800] Loss: 1.05e-03  runR2: 0.81,  elapsed 4.5s (compute 3.9)\n",
      "[5, 17600] Loss: 9.02e-04  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[5, 18400] Loss: 9.89e-04  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 19200] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[5, 20000] Loss: 9.69e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[5, 20800] Loss: 1.12e-03  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[5, 21600] Loss: 9.59e-04  runR2: 0.82,  elapsed 4.7s (compute 4.0)\n",
      "[5, 22400] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[5, 23200] Loss: 9.44e-04  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[5, 24000] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[5, 24800] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[5, 25600] Loss: 9.78e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[5, 26400] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[5, 27200] Loss: 9.54e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[5, 28000] Loss: 1.05e-03  runR2: 0.82,  elapsed 5.4s (compute 5.1)\n",
      "[5, 28800] Loss: 1.20e-03  runR2: 0.82,  elapsed 4.4s (compute 3.7)\n",
      "[5, 29600] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[5, 30400] Loss: 9.67e-04  runR2: 0.82,  elapsed 5.3s (compute 5.0)\n",
      "[5, 31200] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.2s (compute 4.8)\n",
      "[5, 32000] Loss: 9.78e-04  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "[5, 32800] Loss: 1.12e-03  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[5, 33600] Loss: 1.12e-03  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[5, 34400] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[5, 35200] Loss: 1.18e-03  runR2: 0.82,  elapsed 3.9s (compute 3.7)\n",
      "[5, 36000] Loss: 1.08e-03  runR2: 0.82,  elapsed 4.3s (compute 3.7)\n",
      "[5, 36800] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.0s (compute 3.7)\n",
      "[5, 37600] Loss: 1.10e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 38400] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 39200] Loss: 9.91e-04  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[5, 40000] Loss: 8.83e-04  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[5, 40800] Loss: 9.93e-04  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[5, 41600] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[5, 42400] Loss: 9.97e-04  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[5, 43200] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.7s (compute 4.0)\n",
      "[5, 44000] Loss: 9.41e-04  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[5, 44800] Loss: 1.20e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[5, 45600] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[5, 46400] Loss: 9.53e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[5, 47200] Loss: 9.56e-04  runR2: 0.82,  elapsed 4.6s (compute 4.3)\n",
      "[5, 48000] Loss: 9.78e-04  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[5, 48800] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[5, 49600] Loss: 1.11e-03  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "Epoch 5 TRAIN loss: 1.04e-03  MSE: 8.93e-04  h-con:  1.44e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.75   R2-precc: 0.973\n",
      "Epoch 5/10 complete, took 300.52 seconds, autoreg window was 3\n",
      "Epoch 6 Training rollout timesteps: 4 \n",
      "[6, 800] Loss: 1.66e-03  runR2: 0.80,  elapsed 10.3s (compute 3.4)\n",
      "[6, 1600] Loss: 9.83e-04  runR2: 0.81,  elapsed 3.9s (compute 3.5)\n",
      "[6, 2400] Loss: 9.55e-04  runR2: 0.81,  elapsed 4.4s (compute 4.0)\n",
      "[6, 3200] Loss: 9.47e-04  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[6, 4000] Loss: 9.44e-04  runR2: 0.82,  elapsed 4.6s (compute 4.1)\n",
      "[6, 4800] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.5s (compute 4.0)\n",
      "[6, 5600] Loss: 1.18e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 6400] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 7200] Loss: 9.49e-04  runR2: 0.82,  elapsed 4.9s (compute 4.1)\n",
      "[6, 8000] Loss: 9.74e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 8800] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 9600] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 10400] Loss: 1.16e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 11200] Loss: 1.09e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 12000] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 12800] Loss: 1.09e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[6, 13600] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 14400] Loss: 8.86e-04  runR2: 0.82,  elapsed 5.0s (compute 4.3)\n",
      "[6, 15200] Loss: 9.81e-04  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[6, 16000] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[6, 16800] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 17600] Loss: 8.84e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[6, 18400] Loss: 9.68e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[6, 19200] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 20000] Loss: 9.53e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 20800] Loss: 1.09e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 21600] Loss: 9.38e-04  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[6, 22400] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 23200] Loss: 9.24e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 24000] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 24800] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 25600] Loss: 9.57e-04  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[6, 26400] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.6s (compute 4.1)\n",
      "[6, 27200] Loss: 9.39e-04  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[6, 28000] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 28800] Loss: 1.17e-03  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[6, 29600] Loss: 9.79e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 30400] Loss: 9.46e-04  runR2: 0.82,  elapsed 4.6s (compute 4.1)\n",
      "[6, 31200] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[6, 32000] Loss: 9.64e-04  runR2: 0.82,  elapsed 4.4s (compute 3.9)\n",
      "[6, 32800] Loss: 1.09e-03  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[6, 33600] Loss: 1.10e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 34400] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 35200] Loss: 1.15e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 36000] Loss: 1.06e-03  runR2: 0.82,  elapsed 5.1s (compute 4.3)\n",
      "[6, 36800] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[6, 37600] Loss: 1.08e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 38400] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[6, 39200] Loss: 9.76e-04  runR2: 0.82,  elapsed 4.0s (compute 3.7)\n",
      "[6, 40000] Loss: 8.68e-04  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[6, 40800] Loss: 9.74e-04  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[6, 41600] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[6, 42400] Loss: 9.76e-04  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[6, 43200] Loss: 1.00e-03  runR2: 0.82,  elapsed 5.2s (compute 4.4)\n",
      "[6, 44000] Loss: 9.22e-04  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[6, 44800] Loss: 1.17e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[6, 45600] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[6, 46400] Loss: 9.40e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[6, 47200] Loss: 9.36e-04  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[6, 48000] Loss: 9.60e-04  runR2: 0.82,  elapsed 4.7s (compute 4.2)\n",
      "[6, 48800] Loss: 9.86e-04  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[6, 49600] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "Epoch 6 TRAIN loss: 1.02e-03  MSE: 8.79e-04  h-con:  1.36e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.76   R2-precc: 0.976\n",
      "Epoch 6/10 complete, took 289.85 seconds, autoreg window was 4\n",
      "Epoch 7 Training rollout timesteps: 5 \n",
      "[7, 800] Loss: 1.72e-03  runR2: 0.81,  elapsed 10.2s (compute 4.0)\n",
      "[7, 1600] Loss: 9.69e-04  runR2: 0.81,  elapsed 3.6s (compute 3.3)\n",
      "[7, 2400] Loss: 9.41e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[7, 3200] Loss: 9.31e-04  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[7, 4000] Loss: 9.25e-04  runR2: 0.82,  elapsed 3.9s (compute 3.6)\n",
      "[7, 4800] Loss: 1.12e-03  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[7, 5600] Loss: 1.17e-03  runR2: 0.82,  elapsed 4.0s (compute 3.7)\n",
      "[7, 6400] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[7, 7200] Loss: 9.37e-04  runR2: 0.82,  elapsed 4.8s (compute 4.1)\n",
      "[7, 8000] Loss: 9.59e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 8800] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[7, 9600] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.4s (compute 3.9)\n",
      "[7, 10400] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[7, 11200] Loss: 1.08e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 12000] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.9s (compute 4.1)\n",
      "[7, 12800] Loss: 1.08e-03  runR2: 0.82,  elapsed 4.5s (compute 4.0)\n",
      "[7, 13600] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[7, 14400] Loss: 8.69e-04  runR2: 0.82,  elapsed 4.7s (compute 3.9)\n",
      "[7, 15200] Loss: 9.68e-04  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[7, 16000] Loss: 1.13e-03  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[7, 16800] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 17600] Loss: 8.78e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 18400] Loss: 9.63e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 19200] Loss: 9.90e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 20000] Loss: 9.41e-04  runR2: 0.82,  elapsed 4.9s (compute 4.5)\n",
      "[7, 20800] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 21600] Loss: 9.28e-04  runR2: 0.82,  elapsed 5.0s (compute 4.2)\n",
      "[7, 22400] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.7s (compute 4.2)\n",
      "[7, 23200] Loss: 9.14e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 24000] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 24800] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 25600] Loss: 9.48e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 26400] Loss: 9.97e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 27200] Loss: 9.30e-04  runR2: 0.82,  elapsed 4.7s (compute 4.2)\n",
      "[7, 28000] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 28800] Loss: 1.16e-03  runR2: 0.82,  elapsed 5.0s (compute 4.2)\n",
      "[7, 29600] Loss: 9.69e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 30400] Loss: 9.38e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 31200] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 32000] Loss: 9.49e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 32800] Loss: 1.08e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[7, 33600] Loss: 1.09e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[7, 34400] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 35200] Loss: 1.13e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 36000] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.8s (compute 4.1)\n",
      "[7, 36800] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 37600] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 38400] Loss: 9.96e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 39200] Loss: 9.63e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[7, 40000] Loss: 8.55e-04  runR2: 0.82,  elapsed 4.5s (compute 4.0)\n",
      "[7, 40800] Loss: 9.63e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 41600] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 42400] Loss: 9.65e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 43200] Loss: 9.89e-04  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[7, 44000] Loss: 9.11e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[7, 44800] Loss: 1.16e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[7, 45600] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[7, 46400] Loss: 9.29e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[7, 47200] Loss: 9.25e-04  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[7, 48000] Loss: 9.51e-04  runR2: 0.82,  elapsed 3.7s (compute 3.4)\n",
      "[7, 48800] Loss: 9.78e-04  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[7, 49600] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "Epoch 7 TRAIN loss: 1.00e-03  MSE: 8.71e-04  h-con:  1.31e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.77   R2-precc: 0.977\n",
      "Epoch 7/10 complete, took 289.88 seconds, autoreg window was 5\n",
      "Epoch 8 Training rollout timesteps: 5 \n",
      "[8, 800] Loss: 1.76e-03  runR2: 0.81,  elapsed 10.1s (compute 4.2)\n",
      "[8, 1600] Loss: 9.60e-04  runR2: 0.81,  elapsed 4.6s (compute 4.2)\n",
      "[8, 2400] Loss: 9.32e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 3200] Loss: 9.23e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 4000] Loss: 9.17e-04  runR2: 0.83,  elapsed 4.6s (compute 4.2)\n",
      "[8, 4800] Loss: 1.11e-03  runR2: 0.83,  elapsed 4.2s (compute 3.8)\n",
      "[8, 5600] Loss: 1.15e-03  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[8, 6400] Loss: 9.92e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 7200] Loss: 9.28e-04  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[8, 8000] Loss: 9.52e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 8800] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 9600] Loss: 9.95e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 10400] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 11200] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[8, 12000] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 12800] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 13600] Loss: 9.96e-04  runR2: 0.82,  elapsed 5.1s (compute 4.7)\n",
      "[8, 14400] Loss: 8.63e-04  runR2: 0.82,  elapsed 4.6s (compute 3.9)\n",
      "[8, 15200] Loss: 9.61e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[8, 16000] Loss: 1.12e-03  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[8, 16800] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.6s (compute 4.3)\n",
      "[8, 17600] Loss: 8.74e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[8, 18400] Loss: 9.58e-04  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[8, 19200] Loss: 9.83e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 20000] Loss: 9.34e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 20800] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 21600] Loss: 9.20e-04  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[8, 22400] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 23200] Loss: 9.08e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[8, 24000] Loss: 1.02e-03  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[8, 24800] Loss: 1.03e-03  runR2: 0.82,  elapsed 3.9s (compute 3.5)\n",
      "[8, 25600] Loss: 9.42e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[8, 26400] Loss: 9.92e-04  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[8, 27200] Loss: 9.26e-04  runR2: 0.83,  elapsed 4.1s (compute 3.8)\n",
      "[8, 28000] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[8, 28800] Loss: 1.15e-03  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[8, 29600] Loss: 9.64e-04  runR2: 0.83,  elapsed 4.6s (compute 4.2)\n",
      "[8, 30400] Loss: 9.33e-04  runR2: 0.83,  elapsed 4.6s (compute 4.2)\n",
      "[8, 31200] Loss: 1.00e-03  runR2: 0.83,  elapsed 4.6s (compute 4.2)\n",
      "[8, 32000] Loss: 9.45e-04  runR2: 0.83,  elapsed 4.7s (compute 4.2)\n",
      "[8, 32800] Loss: 1.07e-03  runR2: 0.83,  elapsed 4.6s (compute 4.2)\n",
      "[8, 33600] Loss: 1.08e-03  runR2: 0.82,  elapsed 5.0s (compute 4.6)\n",
      "[8, 34400] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[8, 35200] Loss: 1.13e-03  runR2: 0.83,  elapsed 4.5s (compute 4.1)\n",
      "[8, 36000] Loss: 1.04e-03  runR2: 0.82,  elapsed 4.6s (compute 3.9)\n",
      "[8, 36800] Loss: 1.00e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[8, 37600] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[8, 38400] Loss: 9.90e-04  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[8, 39200] Loss: 9.57e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[8, 40000] Loss: 8.50e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[8, 40800] Loss: 9.57e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[8, 41600] Loss: 1.03e-03  runR2: 0.82,  elapsed 3.9s (compute 3.6)\n",
      "[8, 42400] Loss: 9.59e-04  runR2: 0.82,  elapsed 3.9s (compute 3.6)\n",
      "[8, 43200] Loss: 9.84e-04  runR2: 0.82,  elapsed 4.2s (compute 3.6)\n",
      "[8, 44000] Loss: 9.06e-04  runR2: 0.82,  elapsed 4.0s (compute 3.6)\n",
      "[8, 44800] Loss: 1.15e-03  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[8, 45600] Loss: 1.01e-03  runR2: 0.82,  elapsed 3.9s (compute 3.6)\n",
      "[8, 46400] Loss: 9.24e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[8, 47200] Loss: 9.21e-04  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[8, 48000] Loss: 9.46e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[8, 48800] Loss: 9.74e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[8, 49600] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.0s (compute 3.7)\n",
      "Epoch 8 TRAIN loss: 9.96e-04  MSE: 8.67e-04  h-con:  1.29e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.77   R2-precc: 0.978\n",
      "Epoch 8/10 complete, took 283.65 seconds, autoreg window was 5\n",
      "Epoch 9 Training rollout timesteps: 5 \n",
      "[9, 800] Loss: 1.77e-03  runR2: 0.81,  elapsed 9.4s (compute 3.6)\n",
      "[9, 1600] Loss: 9.54e-04  runR2: 0.81,  elapsed 4.4s (compute 4.1)\n",
      "[9, 2400] Loss: 9.26e-04  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[9, 3200] Loss: 9.18e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[9, 4000] Loss: 9.14e-04  runR2: 0.83,  elapsed 4.5s (compute 4.1)\n",
      "[9, 4800] Loss: 1.10e-03  runR2: 0.83,  elapsed 4.5s (compute 4.1)\n",
      "[9, 5600] Loss: 1.15e-03  runR2: 0.83,  elapsed 4.1s (compute 3.7)\n",
      "[9, 6400] Loss: 9.86e-04  runR2: 0.82,  elapsed 4.0s (compute 3.6)\n",
      "[9, 7200] Loss: 9.22e-04  runR2: 0.82,  elapsed 4.6s (compute 3.9)\n",
      "[9, 8000] Loss: 9.46e-04  runR2: 0.82,  elapsed 3.8s (compute 3.5)\n",
      "[9, 8800] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 9600] Loss: 9.90e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 10400] Loss: 1.13e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[9, 11200] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "[9, 12000] Loss: 9.97e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[9, 12800] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "[9, 13600] Loss: 9.90e-04  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[9, 14400] Loss: 8.60e-04  runR2: 0.82,  elapsed 4.0s (compute 3.3)\n",
      "[9, 15200] Loss: 9.59e-04  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[9, 16000] Loss: 1.12e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[9, 16800] Loss: 1.00e-03  runR2: 0.82,  elapsed 3.9s (compute 3.6)\n",
      "[9, 17600] Loss: 8.76e-04  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "[9, 18400] Loss: 9.58e-04  runR2: 0.82,  elapsed 4.0s (compute 3.7)\n",
      "[9, 19200] Loss: 9.79e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[9, 20000] Loss: 9.30e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[9, 20800] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.8s (compute 4.4)\n",
      "[9, 21600] Loss: 9.15e-04  runR2: 0.82,  elapsed 5.5s (compute 4.7)\n",
      "[9, 22400] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[9, 23200] Loss: 9.04e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 24000] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 24800] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 25600] Loss: 9.41e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 26400] Loss: 9.90e-04  runR2: 0.82,  elapsed 4.7s (compute 4.2)\n",
      "[9, 27200] Loss: 9.24e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 28000] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[9, 28800] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.1s (compute 3.4)\n",
      "[9, 29600] Loss: 9.63e-04  runR2: 0.83,  elapsed 4.0s (compute 3.7)\n",
      "[9, 30400] Loss: 9.29e-04  runR2: 0.82,  elapsed 3.8s (compute 3.4)\n",
      "[9, 31200] Loss: 9.99e-04  runR2: 0.83,  elapsed 4.0s (compute 3.7)\n",
      "[9, 32000] Loss: 9.41e-04  runR2: 0.83,  elapsed 4.0s (compute 3.7)\n",
      "[9, 32800] Loss: 1.07e-03  runR2: 0.83,  elapsed 4.1s (compute 3.8)\n",
      "[9, 33600] Loss: 1.08e-03  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "[9, 34400] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[9, 35200] Loss: 1.13e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 36000] Loss: 1.03e-03  runR2: 0.82,  elapsed 5.0s (compute 4.2)\n",
      "[9, 36800] Loss: 9.99e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 37600] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 38400] Loss: 9.88e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[9, 39200] Loss: 9.53e-04  runR2: 0.82,  elapsed 4.3s (compute 3.8)\n",
      "[9, 40000] Loss: 8.47e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[9, 40800] Loss: 9.53e-04  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[9, 41600] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.7s (compute 4.3)\n",
      "[9, 42400] Loss: 9.55e-04  runR2: 0.82,  elapsed 4.7s (compute 4.4)\n",
      "[9, 43200] Loss: 9.80e-04  runR2: 0.82,  elapsed 5.2s (compute 4.3)\n",
      "[9, 44000] Loss: 9.03e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 44800] Loss: 1.15e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[9, 45600] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[9, 46400] Loss: 9.20e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[9, 47200] Loss: 9.18e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[9, 48000] Loss: 9.42e-04  runR2: 0.82,  elapsed 4.3s (compute 3.9)\n",
      "[9, 48800] Loss: 9.71e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[9, 49600] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "Epoch 9 TRAIN loss: 9.92e-04  MSE: 8.64e-04  h-con:  1.28e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.78   R2-precc: 0.978\n",
      "Epoch 9/10 complete, took 281.50 seconds, autoreg window was 5\n",
      "Epoch 10 Training rollout timesteps: 5 \n",
      "[10, 800] Loss: 1.78e-03  runR2: 0.81,  elapsed 10.1s (compute 4.2)\n",
      "[10, 1600] Loss: 9.51e-04  runR2: 0.81,  elapsed 5.0s (compute 4.7)\n",
      "[10, 2400] Loss: 9.23e-04  runR2: 0.82,  elapsed 4.0s (compute 3.7)\n",
      "[10, 3200] Loss: 9.15e-04  runR2: 0.82,  elapsed 3.9s (compute 3.5)\n",
      "[10, 4000] Loss: 9.11e-04  runR2: 0.83,  elapsed 4.2s (compute 3.9)\n",
      "[10, 4800] Loss: 1.10e-03  runR2: 0.82,  elapsed 3.9s (compute 3.6)\n",
      "[10, 5600] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 6400] Loss: 9.83e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 7200] Loss: 9.18e-04  runR2: 0.82,  elapsed 4.5s (compute 3.8)\n",
      "[10, 8000] Loss: 9.43e-04  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[10, 8800] Loss: 1.01e-03  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[10, 9600] Loss: 9.86e-04  runR2: 0.82,  elapsed 3.5s (compute 3.3)\n",
      "[10, 10400] Loss: 1.13e-03  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[10, 11200] Loss: 1.06e-03  runR2: 0.82,  elapsed 3.5s (compute 3.3)\n",
      "[10, 12000] Loss: 9.94e-04  runR2: 0.82,  elapsed 3.5s (compute 3.3)\n",
      "[10, 12800] Loss: 1.06e-03  runR2: 0.82,  elapsed 3.5s (compute 3.3)\n",
      "[10, 13600] Loss: 9.86e-04  runR2: 0.82,  elapsed 4.1s (compute 3.8)\n",
      "[10, 14400] Loss: 8.58e-04  runR2: 0.82,  elapsed 4.2s (compute 3.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 15200] Loss: 9.57e-04  runR2: 0.82,  elapsed 3.6s (compute 3.3)\n",
      "[10, 16000] Loss: 1.12e-03  runR2: 0.82,  elapsed 3.5s (compute 3.3)\n",
      "[10, 16800] Loss: 9.99e-04  runR2: 0.82,  elapsed 3.8s (compute 3.6)\n",
      "[10, 17600] Loss: 8.77e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 18400] Loss: 9.58e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 19200] Loss: 9.78e-04  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[10, 20000] Loss: 9.30e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[10, 20800] Loss: 1.06e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[10, 21600] Loss: 9.13e-04  runR2: 0.82,  elapsed 4.9s (compute 3.9)\n",
      "[10, 22400] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[10, 23200] Loss: 9.04e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[10, 24000] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.4s (compute 4.0)\n",
      "[10, 24800] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[10, 25600] Loss: 9.42e-04  runR2: 0.82,  elapsed 4.5s (compute 4.2)\n",
      "[10, 26400] Loss: 9.91e-04  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[10, 27200] Loss: 9.24e-04  runR2: 0.82,  elapsed 4.6s (compute 4.3)\n",
      "[10, 28000] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[10, 28800] Loss: 1.14e-03  runR2: 0.82,  elapsed 4.8s (compute 4.1)\n",
      "[10, 29600] Loss: 9.64e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 30400] Loss: 9.27e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 31200] Loss: 9.96e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 32000] Loss: 9.42e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 32800] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 33600] Loss: 1.07e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 34400] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 35200] Loss: 1.13e-03  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 36000] Loss: 1.03e-03  runR2: 0.82,  elapsed 4.6s (compute 3.9)\n",
      "[10, 36800] Loss: 9.97e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[10, 37600] Loss: 1.05e-03  runR2: 0.82,  elapsed 4.8s (compute 4.5)\n",
      "[10, 38400] Loss: 9.87e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 39200] Loss: 9.51e-04  runR2: 0.82,  elapsed 4.3s (compute 4.0)\n",
      "[10, 40000] Loss: 8.45e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[10, 40800] Loss: 9.50e-04  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 41600] Loss: 1.02e-03  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[10, 42400] Loss: 9.53e-04  runR2: 0.82,  elapsed 4.1s (compute 3.7)\n",
      "[10, 43200] Loss: 9.78e-04  runR2: 0.82,  elapsed 4.9s (compute 4.2)\n",
      "[10, 44000] Loss: 9.03e-04  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[10, 44800] Loss: 1.15e-03  runR2: 0.82,  elapsed 4.4s (compute 4.1)\n",
      "[10, 45600] Loss: 1.01e-03  runR2: 0.82,  elapsed 4.5s (compute 4.1)\n",
      "[10, 46400] Loss: 9.18e-04  runR2: 0.82,  elapsed 4.6s (compute 4.2)\n",
      "[10, 47200] Loss: 9.17e-04  runR2: 0.82,  elapsed 4.2s (compute 3.9)\n",
      "[10, 48000] Loss: 9.40e-04  runR2: 0.82,  elapsed 4.2s (compute 3.8)\n",
      "[10, 48800] Loss: 9.70e-04  runR2: 0.82,  elapsed 4.0s (compute 3.6)\n",
      "[10, 49600] Loss: 1.06e-03  runR2: 0.82,  elapsed 3.5s (compute 3.3)\n",
      "Epoch 10 TRAIN loss: 9.90e-04  MSE: 8.63e-04  h-con:  1.28e+03   R2: 0.82  R2-dT/dt: 0.71   R2-dq/dt: 0.78   R2-precc: 0.978\n",
      "Epoch 10/10 complete, took 275.09 seconds, autoreg window was 5\n"
     ]
    }
   ],
   "source": [
    "# 128 128\n",
    "# autoreg, hybrid-loss, 2 years instead of 1\n",
    "num_epochs = 10\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0283add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n",
      "[1, 800] Loss: 3.69e-03  runR2: -0.13,  elapsed 13.3s (compute 7.2)\n",
      "[1, 1600] Loss: 2.17e-03  runR2: 0.21,  elapsed 7.0s (compute 6.6)\n",
      "[1, 2400] Loss: 1.93e-03  runR2: 0.36,  elapsed 6.8s (compute 6.4)\n",
      "[1, 3200] Loss: 1.84e-03  runR2: 0.44,  elapsed 6.5s (compute 6.1)\n",
      "[1, 4000] Loss: 1.64e-03  runR2: 0.49,  elapsed 7.3s (compute 6.9)\n",
      "[1, 4800] Loss: 1.85e-03  runR2: 0.52,  elapsed 7.2s (compute 6.7)\n",
      "[1, 5600] Loss: 1.83e-03  runR2: 0.54,  elapsed 7.3s (compute 6.9)\n",
      "[1, 6400] Loss: 1.57e-03  runR2: 0.55,  elapsed 7.5s (compute 7.0)\n",
      "[1, 7200] Loss: 1.47e-03  runR2: 0.56,  elapsed 7.0s (compute 6.3)\n",
      "[1, 8000] Loss: 1.48e-03  runR2: 0.57,  elapsed 6.5s (compute 6.1)\n",
      "[1, 8800] Loss: 1.54e-03  runR2: 0.58,  elapsed 7.4s (compute 7.0)\n",
      "[1, 9600] Loss: 1.46e-03  runR2: 0.59,  elapsed 6.7s (compute 6.4)\n",
      "[1, 10400] Loss: 1.65e-03  runR2: 0.59,  elapsed 6.7s (compute 6.3)\n",
      "[1, 11200] Loss: 1.55e-03  runR2: 0.60,  elapsed 6.7s (compute 6.4)\n",
      "[1, 12000] Loss: 1.40e-03  runR2: 0.60,  elapsed 8.3s (compute 7.8)\n",
      "[1, 12800] Loss: 1.49e-03  runR2: 0.61,  elapsed 7.7s (compute 7.4)\n",
      "[1, 13600] Loss: 1.38e-03  runR2: 0.61,  elapsed 7.5s (compute 7.1)\n",
      "[1, 14400] Loss: 1.24e-03  runR2: 0.62,  elapsed 7.8s (compute 7.0)\n",
      "[1, 15200] Loss: 1.33e-03  runR2: 0.62,  elapsed 6.7s (compute 6.4)\n",
      "[1, 16000] Loss: 1.52e-03  runR2: 0.63,  elapsed 7.3s (compute 7.0)\n",
      "[1, 16800] Loss: 1.36e-03  runR2: 0.63,  elapsed 7.0s (compute 6.7)\n",
      "[1, 17600] Loss: 1.17e-03  runR2: 0.64,  elapsed 7.4s (compute 7.1)\n",
      "[1, 18400] Loss: 1.28e-03  runR2: 0.64,  elapsed 7.8s (compute 7.4)\n",
      "[1, 19200] Loss: 1.30e-03  runR2: 0.65,  elapsed 8.0s (compute 7.7)\n",
      "[1, 20000] Loss: 1.24e-03  runR2: 0.65,  elapsed 8.0s (compute 7.6)\n",
      "[1, 20800] Loss: 1.39e-03  runR2: 0.65,  elapsed 7.4s (compute 7.1)\n",
      "[1, 21600] Loss: 1.21e-03  runR2: 0.66,  elapsed 7.4s (compute 6.8)\n",
      "[1, 22400] Loss: 1.32e-03  runR2: 0.66,  elapsed 6.7s (compute 6.2)\n",
      "[1, 23200] Loss: 1.18e-03  runR2: 0.67,  elapsed 7.1s (compute 6.8)\n",
      "[1, 24000] Loss: 1.31e-03  runR2: 0.67,  elapsed 7.6s (compute 7.2)\n",
      "[1, 24800] Loss: 1.32e-03  runR2: 0.67,  elapsed 7.9s (compute 7.4)\n",
      "[1, 25600] Loss: 1.21e-03  runR2: 0.67,  elapsed 7.5s (compute 7.1)\n",
      "[1, 26400] Loss: 1.25e-03  runR2: 0.68,  elapsed 6.9s (compute 6.5)\n",
      "[1, 27200] Loss: 1.17e-03  runR2: 0.68,  elapsed 8.2s (compute 7.8)\n",
      "[1, 28000] Loss: 1.28e-03  runR2: 0.68,  elapsed 7.0s (compute 6.7)\n",
      "[1, 28800] Loss: 1.45e-03  runR2: 0.69,  elapsed 6.9s (compute 6.3)\n",
      "[1, 29600] Loss: 1.21e-03  runR2: 0.69,  elapsed 7.0s (compute 6.7)\n",
      "[1, 30400] Loss: 1.16e-03  runR2: 0.69,  elapsed 7.5s (compute 7.2)\n",
      "[1, 31200] Loss: 1.25e-03  runR2: 0.69,  elapsed 8.3s (compute 8.0)\n",
      "[1, 32000] Loss: 1.18e-03  runR2: 0.70,  elapsed 7.5s (compute 7.2)\n",
      "[1, 32800] Loss: 1.32e-03  runR2: 0.70,  elapsed 7.1s (compute 6.8)\n",
      "[1, 33600] Loss: 1.34e-03  runR2: 0.70,  elapsed 7.6s (compute 7.3)\n",
      "[1, 34400] Loss: 1.27e-03  runR2: 0.70,  elapsed 6.9s (compute 6.6)\n",
      "[1, 35200] Loss: 1.40e-03  runR2: 0.70,  elapsed 6.5s (compute 6.2)\n",
      "[1, 36000] Loss: 1.27e-03  runR2: 0.70,  elapsed 8.2s (compute 7.6)\n",
      "[1, 36800] Loss: 1.22e-03  runR2: 0.71,  elapsed 8.0s (compute 7.7)\n",
      "[1, 37600] Loss: 1.29e-03  runR2: 0.71,  elapsed 8.2s (compute 7.9)\n",
      "[1, 38400] Loss: 1.21e-03  runR2: 0.71,  elapsed 7.2s (compute 6.9)\n",
      "[1, 39200] Loss: 1.18e-03  runR2: 0.71,  elapsed 6.8s (compute 6.5)\n",
      "[1, 40000] Loss: 1.05e-03  runR2: 0.71,  elapsed 7.0s (compute 6.7)\n",
      "[1, 40800] Loss: 1.17e-03  runR2: 0.71,  elapsed 8.1s (compute 7.7)\n",
      "[1, 41600] Loss: 1.26e-03  runR2: 0.71,  elapsed 7.3s (compute 7.0)\n",
      "[1, 42400] Loss: 1.17e-03  runR2: 0.71,  elapsed 7.0s (compute 6.7)\n",
      "[1, 43200] Loss: 1.20e-03  runR2: 0.72,  elapsed 7.4s (compute 6.8)\n",
      "[1, 44000] Loss: 1.12e-03  runR2: 0.72,  elapsed 8.9s (compute 8.6)\n",
      "[1, 44800] Loss: 1.39e-03  runR2: 0.72,  elapsed 8.5s (compute 8.2)\n",
      "[1, 45600] Loss: 1.22e-03  runR2: 0.72,  elapsed 8.1s (compute 7.8)\n",
      "[1, 46400] Loss: 1.12e-03  runR2: 0.72,  elapsed 8.0s (compute 7.8)\n",
      "[1, 47200] Loss: 1.12e-03  runR2: 0.72,  elapsed 7.6s (compute 7.3)\n",
      "[1, 48000] Loss: 1.13e-03  runR2: 0.72,  elapsed 6.8s (compute 6.5)\n",
      "[1, 48800] Loss: 1.15e-03  runR2: 0.72,  elapsed 6.6s (compute 6.3)\n",
      "[1, 49600] Loss: 1.28e-03  runR2: 0.72,  elapsed 6.5s (compute 6.3)\n",
      "Epoch 1 TRAIN loss: 1.37e-03  MSE: 1.14e-03  h-con:  2.37e+03   R2: 0.73  R2-dT/dt: -0.10   R2-dq/dt: 0.60   R2-precc: 0.918\n",
      "Epoch 1/5 complete, took 468.27 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.38e-03  runR2: 0.78,  elapsed 12.4s (compute 6.6)\n",
      "[2, 1600] Loss: 1.14e-03  runR2: 0.78,  elapsed 6.7s (compute 6.4)\n",
      "[2, 2400] Loss: 1.10e-03  runR2: 0.79,  elapsed 7.3s (compute 7.1)\n",
      "[2, 3200] Loss: 1.10e-03  runR2: 0.79,  elapsed 7.0s (compute 6.7)\n",
      "[2, 4000] Loss: 1.10e-03  runR2: 0.80,  elapsed 7.2s (compute 7.0)\n",
      "[2, 4800] Loss: 1.32e-03  runR2: 0.80,  elapsed 8.0s (compute 7.8)\n",
      "[2, 5600] Loss: 1.36e-03  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[2, 6400] Loss: 1.17e-03  runR2: 0.80,  elapsed 7.2s (compute 6.9)\n",
      "[2, 7200] Loss: 1.09e-03  runR2: 0.79,  elapsed 6.7s (compute 6.2)\n",
      "[2, 8000] Loss: 1.12e-03  runR2: 0.79,  elapsed 7.5s (compute 7.3)\n",
      "[2, 8800] Loss: 1.21e-03  runR2: 0.80,  elapsed 6.9s (compute 6.5)\n",
      "[2, 9600] Loss: 1.17e-03  runR2: 0.80,  elapsed 7.3s (compute 7.0)\n",
      "[2, 10400] Loss: 1.33e-03  runR2: 0.80,  elapsed 6.8s (compute 6.5)\n",
      "[2, 11200] Loss: 1.25e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 12000] Loss: 1.16e-03  runR2: 0.80,  elapsed 6.8s (compute 6.5)\n",
      "[2, 12800] Loss: 1.25e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 13600] Loss: 1.16e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 14400] Loss: 1.02e-03  runR2: 0.80,  elapsed 7.1s (compute 6.6)\n",
      "[2, 15200] Loss: 1.13e-03  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[2, 16000] Loss: 1.31e-03  runR2: 0.80,  elapsed 6.8s (compute 6.5)\n",
      "[2, 16800] Loss: 1.17e-03  runR2: 0.80,  elapsed 7.2s (compute 6.9)\n",
      "[2, 17600] Loss: 1.01e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 18400] Loss: 1.11e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 19200] Loss: 1.14e-03  runR2: 0.80,  elapsed 6.9s (compute 6.7)\n",
      "[2, 20000] Loss: 1.08e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 20800] Loss: 1.24e-03  runR2: 0.80,  elapsed 7.1s (compute 6.8)\n",
      "[2, 21600] Loss: 1.07e-03  runR2: 0.80,  elapsed 7.0s (compute 6.4)\n",
      "[2, 22400] Loss: 1.18e-03  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[2, 23200] Loss: 1.05e-03  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[2, 24000] Loss: 1.18e-03  runR2: 0.80,  elapsed 6.4s (compute 6.1)\n",
      "[2, 24800] Loss: 1.19e-03  runR2: 0.80,  elapsed 6.7s (compute 6.4)\n",
      "[2, 25600] Loss: 1.09e-03  runR2: 0.80,  elapsed 6.4s (compute 6.0)\n",
      "[2, 26400] Loss: 1.13e-03  runR2: 0.80,  elapsed 7.2s (compute 6.9)\n",
      "[2, 27200] Loss: 1.06e-03  runR2: 0.80,  elapsed 6.4s (compute 6.2)\n",
      "[2, 28000] Loss: 1.17e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 28800] Loss: 1.32e-03  runR2: 0.80,  elapsed 7.2s (compute 6.7)\n",
      "[2, 29600] Loss: 1.10e-03  runR2: 0.80,  elapsed 7.7s (compute 7.4)\n",
      "[2, 30400] Loss: 1.07e-03  runR2: 0.80,  elapsed 7.6s (compute 7.3)\n",
      "[2, 31200] Loss: 1.15e-03  runR2: 0.80,  elapsed 6.8s (compute 6.5)\n",
      "[2, 32000] Loss: 1.08e-03  runR2: 0.80,  elapsed 6.6s (compute 6.3)\n",
      "[2, 32800] Loss: 1.22e-03  runR2: 0.80,  elapsed 6.8s (compute 6.5)\n",
      "[2, 33600] Loss: 1.24e-03  runR2: 0.80,  elapsed 6.5s (compute 6.2)\n",
      "[2, 34400] Loss: 1.17e-03  runR2: 0.80,  elapsed 7.4s (compute 7.1)\n",
      "[2, 35200] Loss: 1.30e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 36000] Loss: 1.18e-03  runR2: 0.80,  elapsed 7.3s (compute 6.7)\n",
      "[2, 36800] Loss: 1.14e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 37600] Loss: 1.21e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 38400] Loss: 1.13e-03  runR2: 0.81,  elapsed 7.3s (compute 7.0)\n",
      "[2, 39200] Loss: 1.10e-03  runR2: 0.81,  elapsed 7.2s (compute 6.9)\n",
      "[2, 40000] Loss: 9.75e-04  runR2: 0.81,  elapsed 7.0s (compute 6.7)\n",
      "[2, 40800] Loss: 1.10e-03  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[2, 41600] Loss: 1.18e-03  runR2: 0.80,  elapsed 7.0s (compute 6.7)\n",
      "[2, 42400] Loss: 1.10e-03  runR2: 0.81,  elapsed 7.0s (compute 6.8)\n",
      "[2, 43200] Loss: 1.12e-03  runR2: 0.80,  elapsed 7.9s (compute 7.3)\n",
      "[2, 44000] Loss: 1.04e-03  runR2: 0.81,  elapsed 7.3s (compute 7.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 44800] Loss: 1.31e-03  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[2, 45600] Loss: 1.14e-03  runR2: 0.81,  elapsed 7.0s (compute 6.7)\n",
      "[2, 46400] Loss: 1.05e-03  runR2: 0.81,  elapsed 7.7s (compute 7.5)\n",
      "[2, 47200] Loss: 1.05e-03  runR2: 0.81,  elapsed 6.6s (compute 6.3)\n",
      "[2, 48000] Loss: 1.07e-03  runR2: 0.81,  elapsed 7.3s (compute 7.0)\n",
      "[2, 48800] Loss: 1.09e-03  runR2: 0.81,  elapsed 7.5s (compute 7.2)\n",
      "[2, 49600] Loss: 1.20e-03  runR2: 0.81,  elapsed 6.9s (compute 6.6)\n",
      "Epoch 2 TRAIN loss: 1.15e-03  MSE: 9.65e-04  h-con:  1.87e+03   R2: 0.81  R2-dT/dt: 0.70   R2-dq/dt: 0.70   R2-precc: 0.959\n",
      "Epoch 2/5 complete, took 446.51 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.46e-03  runR2: 0.80,  elapsed 12.4s (compute 6.1)\n",
      "[3, 1600] Loss: 1.08e-03  runR2: 0.80,  elapsed 6.7s (compute 6.4)\n",
      "[3, 2400] Loss: 1.05e-03  runR2: 0.81,  elapsed 7.3s (compute 7.0)\n",
      "[3, 3200] Loss: 1.05e-03  runR2: 0.81,  elapsed 7.3s (compute 6.9)\n",
      "[3, 4000] Loss: 1.04e-03  runR2: 0.82,  elapsed 7.0s (compute 6.7)\n",
      "[3, 4800] Loss: 1.25e-03  runR2: 0.82,  elapsed 6.4s (compute 6.2)\n",
      "[3, 5600] Loss: 1.30e-03  runR2: 0.82,  elapsed 8.0s (compute 7.7)\n",
      "[3, 6400] Loss: 1.11e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[3, 7200] Loss: 1.04e-03  runR2: 0.81,  elapsed 7.8s (compute 7.3)\n",
      "[3, 8000] Loss: 1.07e-03  runR2: 0.81,  elapsed 7.2s (compute 6.9)\n",
      "[3, 8800] Loss: 1.15e-03  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[3, 9600] Loss: 1.11e-03  runR2: 0.81,  elapsed 6.4s (compute 6.0)\n",
      "[3, 10400] Loss: 1.28e-03  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "[3, 11200] Loss: 1.20e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[3, 12000] Loss: 1.11e-03  runR2: 0.82,  elapsed 6.5s (compute 6.2)\n",
      "[3, 12800] Loss: 1.20e-03  runR2: 0.81,  elapsed 7.3s (compute 7.0)\n",
      "[3, 13600] Loss: 1.11e-03  runR2: 0.81,  elapsed 6.3s (compute 6.0)\n",
      "[3, 14400] Loss: 9.69e-04  runR2: 0.81,  elapsed 7.5s (compute 7.0)\n",
      "[3, 15200] Loss: 1.08e-03  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "[3, 16000] Loss: 1.25e-03  runR2: 0.81,  elapsed 6.3s (compute 6.0)\n",
      "[3, 16800] Loss: 1.13e-03  runR2: 0.81,  elapsed 6.8s (compute 6.5)\n",
      "[3, 17600] Loss: 9.73e-04  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[3, 18400] Loss: 1.06e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[3, 19200] Loss: 1.10e-03  runR2: 0.82,  elapsed 6.5s (compute 6.2)\n",
      "[3, 20000] Loss: 1.04e-03  runR2: 0.82,  elapsed 7.3s (compute 7.0)\n",
      "[3, 20800] Loss: 1.19e-03  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "[3, 21600] Loss: 1.03e-03  runR2: 0.82,  elapsed 7.5s (compute 7.0)\n",
      "[3, 22400] Loss: 1.14e-03  runR2: 0.82,  elapsed 6.7s (compute 6.4)\n",
      "[3, 23200] Loss: 1.01e-03  runR2: 0.82,  elapsed 6.9s (compute 6.6)\n",
      "[3, 24000] Loss: 1.14e-03  runR2: 0.82,  elapsed 6.8s (compute 6.6)\n",
      "[3, 24800] Loss: 1.15e-03  runR2: 0.82,  elapsed 6.7s (compute 6.5)\n",
      "[3, 25600] Loss: 1.05e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[3, 26400] Loss: 1.09e-03  runR2: 0.82,  elapsed 6.5s (compute 6.2)\n",
      "[3, 27200] Loss: 1.02e-03  runR2: 0.82,  elapsed 7.2s (compute 7.0)\n",
      "[3, 28000] Loss: 1.13e-03  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "[3, 28800] Loss: 1.28e-03  runR2: 0.82,  elapsed 7.5s (compute 7.0)\n",
      "[3, 29600] Loss: 1.06e-03  runR2: 0.82,  elapsed 6.7s (compute 6.4)\n",
      "[3, 30400] Loss: 1.03e-03  runR2: 0.82,  elapsed 6.9s (compute 6.6)\n",
      "[3, 31200] Loss: 1.11e-03  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[3, 32000] Loss: 1.04e-03  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[3, 32800] Loss: 1.19e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[3, 33600] Loss: 1.20e-03  runR2: 0.82,  elapsed 6.5s (compute 6.2)\n",
      "[3, 34400] Loss: 1.14e-03  runR2: 0.82,  elapsed 7.8s (compute 7.4)\n",
      "[3, 35200] Loss: 1.25e-03  runR2: 0.82,  elapsed 6.5s (compute 6.2)\n",
      "[3, 36000] Loss: 1.14e-03  runR2: 0.82,  elapsed 7.8s (compute 7.2)\n",
      "[3, 36800] Loss: 1.10e-03  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[3, 37600] Loss: 1.17e-03  runR2: 0.82,  elapsed 8.3s (compute 8.0)\n",
      "[3, 38400] Loss: 1.10e-03  runR2: 0.82,  elapsed 7.4s (compute 7.1)\n",
      "[3, 39200] Loss: 1.06e-03  runR2: 0.82,  elapsed 8.2s (compute 7.9)\n",
      "[3, 40000] Loss: 9.43e-04  runR2: 0.82,  elapsed 6.7s (compute 6.4)\n",
      "[3, 40800] Loss: 1.06e-03  runR2: 0.82,  elapsed 7.3s (compute 7.0)\n",
      "[3, 41600] Loss: 1.14e-03  runR2: 0.82,  elapsed 6.5s (compute 6.3)\n",
      "[3, 42400] Loss: 1.07e-03  runR2: 0.82,  elapsed 7.9s (compute 7.7)\n",
      "[3, 43200] Loss: 1.09e-03  runR2: 0.82,  elapsed 6.8s (compute 6.3)\n",
      "[3, 44000] Loss: 1.01e-03  runR2: 0.82,  elapsed 7.4s (compute 7.2)\n",
      "[3, 44800] Loss: 1.27e-03  runR2: 0.82,  elapsed 7.7s (compute 7.4)\n",
      "[3, 45600] Loss: 1.11e-03  runR2: 0.82,  elapsed 7.2s (compute 6.8)\n",
      "[3, 46400] Loss: 1.02e-03  runR2: 0.82,  elapsed 7.4s (compute 7.1)\n",
      "[3, 47200] Loss: 1.02e-03  runR2: 0.82,  elapsed 7.4s (compute 7.2)\n",
      "[3, 48000] Loss: 1.04e-03  runR2: 0.82,  elapsed 6.5s (compute 6.2)\n",
      "[3, 48800] Loss: 1.06e-03  runR2: 0.82,  elapsed 6.4s (compute 6.1)\n",
      "[3, 49600] Loss: 1.17e-03  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "Epoch 3 TRAIN loss: 1.11e-03  MSE: 9.35e-04  h-con:  1.74e+03   R2: 0.82  R2-dT/dt: 0.72   R2-dq/dt: 0.74   R2-precc: 0.965\n",
      "Epoch 3/5 complete, took 444.67 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 1.66e-03  runR2: 0.81,  elapsed 12.3s (compute 6.0)\n",
      "[4, 1600] Loss: 1.05e-03  runR2: 0.81,  elapsed 7.0s (compute 6.7)\n",
      "[4, 2400] Loss: 1.01e-03  runR2: 0.81,  elapsed 6.0s (compute 5.8)\n",
      "[4, 3200] Loss: 1.00e-03  runR2: 0.82,  elapsed 5.7s (compute 5.5)\n",
      "[4, 4000] Loss: 9.98e-04  runR2: 0.82,  elapsed 5.2s (compute 4.9)\n",
      "[4, 4800] Loss: 1.20e-03  runR2: 0.82,  elapsed 6.2s (compute 5.5)\n",
      "[4, 5600] Loss: 1.25e-03  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 6400] Loss: 1.07e-03  runR2: 0.82,  elapsed 5.9s (compute 5.6)\n",
      "[4, 7200] Loss: 9.97e-04  runR2: 0.82,  elapsed 5.9s (compute 5.4)\n",
      "[4, 8000] Loss: 1.02e-03  runR2: 0.82,  elapsed 5.7s (compute 5.5)\n",
      "[4, 8800] Loss: 1.10e-03  runR2: 0.82,  elapsed 6.0s (compute 5.7)\n",
      "[4, 9600] Loss: 1.07e-03  runR2: 0.82,  elapsed 6.6s (compute 6.3)\n",
      "[4, 10400] Loss: 1.22e-03  runR2: 0.82,  elapsed 6.0s (compute 5.7)\n",
      "[4, 11200] Loss: 1.15e-03  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 12000] Loss: 1.07e-03  runR2: 0.82,  elapsed 5.8s (compute 5.5)\n",
      "[4, 12800] Loss: 1.14e-03  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 13600] Loss: 1.06e-03  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 14400] Loss: 9.23e-04  runR2: 0.82,  elapsed 5.8s (compute 5.3)\n",
      "[4, 15200] Loss: 1.03e-03  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 16000] Loss: 1.20e-03  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 16800] Loss: 1.07e-03  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 17600] Loss: 9.26e-04  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 18400] Loss: 1.01e-03  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 19200] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 20000] Loss: 9.92e-04  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 20800] Loss: 1.14e-03  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 21600] Loss: 9.79e-04  runR2: 0.82,  elapsed 5.8s (compute 5.3)\n",
      "[4, 22400] Loss: 1.08e-03  runR2: 0.82,  elapsed 5.5s (compute 5.3)\n",
      "[4, 23200] Loss: 9.63e-04  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 24000] Loss: 1.08e-03  runR2: 0.82,  elapsed 5.9s (compute 5.6)\n",
      "[4, 24800] Loss: 1.09e-03  runR2: 0.82,  elapsed 6.1s (compute 5.8)\n",
      "[4, 25600] Loss: 9.98e-04  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "[4, 26400] Loss: 1.04e-03  runR2: 0.82,  elapsed 5.7s (compute 5.5)\n",
      "[4, 27200] Loss: 9.80e-04  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 28000] Loss: 1.07e-03  runR2: 0.82,  elapsed 5.2s (compute 4.9)\n",
      "[4, 28800] Loss: 1.22e-03  runR2: 0.82,  elapsed 5.5s (compute 4.9)\n",
      "[4, 29600] Loss: 1.02e-03  runR2: 0.82,  elapsed 6.0s (compute 5.7)\n",
      "[4, 30400] Loss: 9.86e-04  runR2: 0.82,  elapsed 5.7s (compute 5.4)\n",
      "[4, 31200] Loss: 1.06e-03  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[4, 32000] Loss: 1.00e-03  runR2: 0.82,  elapsed 5.8s (compute 5.5)\n",
      "[4, 32800] Loss: 1.13e-03  runR2: 0.82,  elapsed 6.2s (compute 5.9)\n",
      "[4, 33600] Loss: 1.14e-03  runR2: 0.82,  elapsed 7.2s (compute 6.9)\n",
      "[4, 34400] Loss: 1.09e-03  runR2: 0.82,  elapsed 6.7s (compute 6.4)\n",
      "[4, 35200] Loss: 1.20e-03  runR2: 0.82,  elapsed 6.6s (compute 6.3)\n",
      "[4, 36000] Loss: 1.09e-03  runR2: 0.82,  elapsed 7.3s (compute 6.8)\n",
      "[4, 36800] Loss: 1.05e-03  runR2: 0.82,  elapsed 6.0s (compute 5.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 37600] Loss: 1.12e-03  runR2: 0.82,  elapsed 6.7s (compute 6.3)\n",
      "[4, 38400] Loss: 1.05e-03  runR2: 0.82,  elapsed 6.3s (compute 6.0)\n",
      "[4, 39200] Loss: 1.01e-03  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[4, 40000] Loss: 8.96e-04  runR2: 0.82,  elapsed 6.9s (compute 6.6)\n",
      "[4, 40800] Loss: 1.01e-03  runR2: 0.82,  elapsed 6.3s (compute 6.1)\n",
      "[4, 41600] Loss: 1.09e-03  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[4, 42400] Loss: 1.01e-03  runR2: 0.82,  elapsed 6.7s (compute 6.5)\n",
      "[4, 43200] Loss: 1.03e-03  runR2: 0.82,  elapsed 5.9s (compute 5.4)\n",
      "[4, 44000] Loss: 9.56e-04  runR2: 0.82,  elapsed 6.1s (compute 5.9)\n",
      "[4, 44800] Loss: 1.21e-03  runR2: 0.82,  elapsed 5.2s (compute 4.9)\n",
      "[4, 45600] Loss: 1.06e-03  runR2: 0.82,  elapsed 5.5s (compute 5.2)\n",
      "[4, 46400] Loss: 9.68e-04  runR2: 0.82,  elapsed 5.7s (compute 5.4)\n",
      "[4, 47200] Loss: 9.66e-04  runR2: 0.82,  elapsed 6.6s (compute 6.3)\n",
      "[4, 48000] Loss: 9.91e-04  runR2: 0.82,  elapsed 5.5s (compute 5.2)\n",
      "[4, 48800] Loss: 1.01e-03  runR2: 0.82,  elapsed 6.4s (compute 6.1)\n",
      "[4, 49600] Loss: 1.12e-03  runR2: 0.82,  elapsed 5.7s (compute 5.5)\n",
      "Epoch 4 TRAIN loss: 1.06e-03  MSE: 9.03e-04  h-con:  1.55e+03   R2: 0.82  R2-dT/dt: 0.72   R2-dq/dt: 0.76   R2-precc: 0.970\n",
      "Epoch 4/5 complete, took 381.51 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 1.72e-03  runR2: 0.81,  elapsed 10.7s (compute 5.0)\n",
      "[5, 1600] Loss: 9.98e-04  runR2: 0.82,  elapsed 6.0s (compute 5.7)\n",
      "[5, 2400] Loss: 9.66e-04  runR2: 0.82,  elapsed 5.7s (compute 5.4)\n",
      "[5, 3200] Loss: 9.54e-04  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[5, 4000] Loss: 9.54e-04  runR2: 0.83,  elapsed 5.4s (compute 5.1)\n",
      "[5, 4800] Loss: 1.15e-03  runR2: 0.83,  elapsed 5.9s (compute 5.6)\n",
      "[5, 5600] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.0s (compute 5.7)\n",
      "[5, 6400] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.3s (compute 6.0)\n",
      "[5, 7200] Loss: 9.62e-04  runR2: 0.82,  elapsed 6.5s (compute 6.0)\n",
      "[5, 8000] Loss: 9.83e-04  runR2: 0.82,  elapsed 5.6s (compute 5.3)\n",
      "[5, 8800] Loss: 1.06e-03  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 9600] Loss: 1.03e-03  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 10400] Loss: 1.17e-03  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 11200] Loss: 1.10e-03  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 12000] Loss: 1.03e-03  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 12800] Loss: 1.10e-03  runR2: 0.82,  elapsed 6.1s (compute 5.8)\n",
      "[5, 13600] Loss: 1.03e-03  runR2: 0.82,  elapsed 5.4s (compute 5.1)\n",
      "[5, 14400] Loss: 8.93e-04  runR2: 0.82,  elapsed 5.5s (compute 4.9)\n",
      "[5, 15200] Loss: 9.89e-04  runR2: 0.83,  elapsed 5.1s (compute 4.8)\n",
      "[5, 16000] Loss: 1.15e-03  runR2: 0.82,  elapsed 5.1s (compute 4.8)\n",
      "[5, 16800] Loss: 1.03e-03  runR2: 0.82,  elapsed 5.4s (compute 5.1)\n",
      "[5, 17600] Loss: 8.90e-04  runR2: 0.83,  elapsed 5.6s (compute 5.2)\n",
      "[5, 18400] Loss: 9.76e-04  runR2: 0.83,  elapsed 5.7s (compute 5.4)\n",
      "[5, 19200] Loss: 1.01e-03  runR2: 0.83,  elapsed 5.8s (compute 5.5)\n",
      "[5, 20000] Loss: 9.53e-04  runR2: 0.83,  elapsed 6.2s (compute 6.0)\n",
      "[5, 20800] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.2s (compute 5.9)\n",
      "[5, 21600] Loss: 9.46e-04  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[5, 22400] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.7s (compute 5.4)\n",
      "[5, 23200] Loss: 9.34e-04  runR2: 0.83,  elapsed 5.6s (compute 5.2)\n",
      "[5, 24000] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.7s (compute 5.4)\n",
      "[5, 24800] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.0s (compute 5.7)\n",
      "[5, 25600] Loss: 9.66e-04  runR2: 0.83,  elapsed 5.8s (compute 5.5)\n",
      "[5, 26400] Loss: 1.01e-03  runR2: 0.83,  elapsed 5.6s (compute 5.2)\n",
      "[5, 27200] Loss: 9.44e-04  runR2: 0.83,  elapsed 4.9s (compute 4.6)\n",
      "[5, 28000] Loss: 1.04e-03  runR2: 0.83,  elapsed 4.7s (compute 4.4)\n",
      "[5, 28800] Loss: 1.19e-03  runR2: 0.83,  elapsed 5.1s (compute 4.5)\n",
      "[5, 29600] Loss: 9.86e-04  runR2: 0.83,  elapsed 4.7s (compute 4.5)\n",
      "[5, 30400] Loss: 9.55e-04  runR2: 0.83,  elapsed 5.0s (compute 4.7)\n",
      "[5, 31200] Loss: 1.03e-03  runR2: 0.83,  elapsed 5.0s (compute 4.7)\n",
      "[5, 32000] Loss: 9.64e-04  runR2: 0.83,  elapsed 5.1s (compute 4.7)\n",
      "[5, 32800] Loss: 1.10e-03  runR2: 0.83,  elapsed 5.2s (compute 4.9)\n",
      "[5, 33600] Loss: 1.11e-03  runR2: 0.83,  elapsed 5.2s (compute 4.9)\n",
      "[5, 34400] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.1s (compute 4.8)\n",
      "[5, 35200] Loss: 1.17e-03  runR2: 0.83,  elapsed 5.1s (compute 4.8)\n",
      "[5, 36000] Loss: 1.06e-03  runR2: 0.83,  elapsed 5.5s (compute 4.9)\n",
      "[5, 36800] Loss: 1.02e-03  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 37600] Loss: 1.09e-03  runR2: 0.83,  elapsed 5.2s (compute 4.9)\n",
      "[5, 38400] Loss: 1.01e-03  runR2: 0.83,  elapsed 5.3s (compute 5.0)\n",
      "[5, 39200] Loss: 9.78e-04  runR2: 0.83,  elapsed 5.3s (compute 5.0)\n",
      "[5, 40000] Loss: 8.71e-04  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[5, 40800] Loss: 9.80e-04  runR2: 0.83,  elapsed 5.5s (compute 5.3)\n",
      "[5, 41600] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.2s (compute 4.9)\n",
      "[5, 42400] Loss: 9.83e-04  runR2: 0.83,  elapsed 5.3s (compute 5.0)\n",
      "[5, 43200] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.2s (compute 5.6)\n",
      "[5, 44000] Loss: 9.29e-04  runR2: 0.83,  elapsed 5.9s (compute 5.5)\n",
      "[5, 44800] Loss: 1.18e-03  runR2: 0.83,  elapsed 6.0s (compute 5.7)\n",
      "[5, 45600] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.0s (compute 5.7)\n",
      "[5, 46400] Loss: 9.40e-04  runR2: 0.83,  elapsed 5.9s (compute 5.7)\n",
      "[5, 47200] Loss: 9.42e-04  runR2: 0.83,  elapsed 5.9s (compute 5.7)\n",
      "[5, 48000] Loss: 9.63e-04  runR2: 0.83,  elapsed 6.0s (compute 5.7)\n",
      "[5, 48800] Loss: 9.86e-04  runR2: 0.83,  elapsed 5.9s (compute 5.6)\n",
      "[5, 49600] Loss: 1.09e-03  runR2: 0.83,  elapsed 5.2s (compute 4.9)\n",
      "Epoch 5 TRAIN loss: 1.02e-03  MSE: 8.81e-04  h-con:  1.42e+03   R2: 0.83  R2-dT/dt: 0.73   R2-dq/dt: 0.77   R2-precc: 0.974\n",
      "Epoch 5/5 complete, took 355.43 seconds, autoreg window was 3\n"
     ]
    }
   ],
   "source": [
    "# 160 160\n",
    "# autoreg, hybrid-loss, 2 years instead of 1\n",
    "num_epochs = 5\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f68e1932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.0010232454466328606,\n",
       " 'mean_squared_error': 0.0008814633375946937,\n",
       " 'mean_absolute_error': 0,\n",
       " 'R2': tensor(0.8270, device='cuda:0'),\n",
       " 'R2_heating': tensor(0.7330, device='cuda:0'),\n",
       " 'R2_moistening': tensor(0.7725, device='cuda:0'),\n",
       " 'R2_precc': np.float64(0.9739614277496498),\n",
       " 'h_conservation': 1417.821073812898}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_runner.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6baad8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/28272/ipykernel_3268844/4217634886.py:28: RuntimeWarning: divide by zero encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_3268844/4217634886.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_3268844/1891790002.py:176: RuntimeWarning: invalid value encountered in add\n",
      "  epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 800] Loss: 1.72e-02  runR2: -0.60,  elapsed 19.8s (compute 9.3)\n",
      "[1, 1600] Loss: 6.34e-03  runR2: 0.00,  elapsed 8.4s (compute 8.0)\n",
      "[1, 2400] Loss: 6.85e-03  runR2: 0.25,  elapsed 8.3s (compute 7.9)\n",
      "[1, 3200] Loss: 5.22e-03  runR2: 0.34,  elapsed 7.7s (compute 7.3)\n",
      "[1, 4000] Loss: 5.70e-03  runR2: 0.39,  elapsed 8.2s (compute 7.8)\n",
      "[1, 4800] Loss: 5.90e-03  runR2: 0.41,  elapsed 8.4s (compute 8.0)\n",
      "[1, 5600] Loss: 5.30e-03  runR2: 0.43,  elapsed 8.0s (compute 7.6)\n",
      "[1, 6400] Loss: 5.07e-03  runR2: 0.44,  elapsed 8.5s (compute 8.0)\n",
      "[1, 7200] Loss: 4.82e-03  runR2: 0.45,  elapsed 7.8s (compute 7.5)\n",
      "[1, 8000] Loss: 4.92e-03  runR2: 0.46,  elapsed 8.9s (compute 8.1)\n",
      "[1, 8800] Loss: 4.44e-03  runR2: 0.47,  elapsed 8.0s (compute 7.6)\n",
      "[1, 9600] Loss: 4.84e-03  runR2: 0.48,  elapsed 7.9s (compute 7.5)\n",
      "[1, 10400] Loss: 4.64e-03  runR2: 0.49,  elapsed 8.2s (compute 7.8)\n",
      "[1, 11200] Loss: 4.14e-03  runR2: 0.50,  elapsed 8.5s (compute 8.1)\n",
      "[1, 12000] Loss: 4.20e-03  runR2: 0.50,  elapsed 8.1s (compute 7.7)\n",
      "[1, 12800] Loss: 4.61e-03  runR2: 0.50,  elapsed 7.7s (compute 7.3)\n",
      "[1, 13600] Loss: 4.20e-03  runR2: 0.51,  elapsed 8.2s (compute 7.8)\n",
      "[1, 14400] Loss: 5.14e-03  runR2: 0.52,  elapsed 8.8s (compute 8.4)\n",
      "[1, 15200] Loss: 3.87e-03  runR2: 0.52,  elapsed 10.1s (compute 9.4)\n",
      "[1, 16000] Loss: 4.22e-03  runR2: 0.52,  elapsed 7.8s (compute 7.4)\n",
      "[1, 16800] Loss: 4.01e-03  runR2: 0.53,  elapsed 7.6s (compute 7.1)\n",
      "[1, 17600] Loss: 4.17e-03  runR2: 0.53,  elapsed 7.6s (compute 7.2)\n",
      "[1, 18400] Loss: 3.63e-03  runR2: 0.54,  elapsed 7.6s (compute 7.2)\n",
      "[1, 19200] Loss: 3.82e-03  runR2: 0.54,  elapsed 7.7s (compute 7.3)\n",
      "[1, 20000] Loss: 3.98e-03  runR2: 0.54,  elapsed 8.1s (compute 7.7)\n",
      "[1, 20800] Loss: 3.79e-03  runR2: 0.55,  elapsed 8.0s (compute 7.6)\n",
      "[1, 21600] Loss: 3.55e-03  runR2: 0.55,  elapsed 8.4s (compute 8.0)\n",
      "[1, 22400] Loss: 3.55e-03  runR2: 0.56,  elapsed 8.2s (compute 7.4)\n",
      "[1, 23200] Loss: 4.09e-03  runR2: 0.56,  elapsed 7.7s (compute 7.3)\n",
      "[1, 24000] Loss: 3.71e-03  runR2: 0.56,  elapsed 7.7s (compute 7.3)\n",
      "[1, 24800] Loss: 3.88e-03  runR2: 0.56,  elapsed 8.0s (compute 7.7)\n",
      "[1, 25600] Loss: 3.99e-03  runR2: 0.57,  elapsed 7.4s (compute 7.1)\n",
      "[1, 26400] Loss: 3.97e-03  runR2: 0.57,  elapsed 7.3s (compute 6.9)\n",
      "[1, 27200] Loss: 3.59e-03  runR2: 0.57,  elapsed 7.2s (compute 6.8)\n",
      "[1, 28000] Loss: 3.79e-03  runR2: 0.57,  elapsed 7.1s (compute 6.8)\n",
      "[1, 28800] Loss: 4.34e-03  runR2: 0.58,  elapsed 7.1s (compute 6.7)\n",
      "[1, 29600] Loss: 3.57e-03  runR2: 0.58,  elapsed 7.5s (compute 6.8)\n",
      "[1, 30400] Loss: 4.04e-03  runR2: 0.58,  elapsed 7.1s (compute 6.8)\n",
      "[1, 31200] Loss: 3.74e-03  runR2: 0.58,  elapsed 7.4s (compute 7.0)\n",
      "[1, 32000] Loss: 3.93e-03  runR2: 0.59,  elapsed 7.2s (compute 6.8)\n",
      "[1, 32800] Loss: 4.43e-03  runR2: 0.59,  elapsed 7.9s (compute 7.4)\n",
      "[1, 33600] Loss: 3.59e-03  runR2: 0.59,  elapsed 8.4s (compute 8.0)\n",
      "[1, 34400] Loss: 3.44e-03  runR2: 0.59,  elapsed 8.5s (compute 8.1)\n",
      "[1, 35200] Loss: 4.00e-03  runR2: 0.60,  elapsed 7.8s (compute 7.4)\n",
      "[1, 36000] Loss: 3.44e-03  runR2: 0.60,  elapsed 8.8s (compute 8.3)\n",
      "[1, 36800] Loss: 3.18e-03  runR2: 0.60,  elapsed 9.5s (compute 8.8)\n",
      "[1, 37600] Loss: 2.88e-03  runR2: 0.60,  elapsed 9.1s (compute 8.7)\n",
      "[1, 38400] Loss: 3.30e-03  runR2: 0.60,  elapsed 7.7s (compute 7.4)\n",
      "[1, 39200] Loss: 3.25e-03  runR2: 0.60,  elapsed 7.7s (compute 7.4)\n",
      "[1, 40000] Loss: 3.25e-03  runR2: 0.61,  elapsed 8.7s (compute 8.3)\n",
      "[1, 40800] Loss: 3.43e-03  runR2: 0.61,  elapsed 8.4s (compute 8.1)\n",
      "[1, 41600] Loss: 3.47e-03  runR2: 0.61,  elapsed 9.1s (compute 8.7)\n",
      "[1, 42400] Loss: 4.10e-03  runR2: 0.61,  elapsed 8.4s (compute 8.0)\n",
      "[1, 43200] Loss: 3.18e-03  runR2: 0.61,  elapsed 8.3s (compute 7.9)\n",
      "[1, 44000] Loss: 3.61e-03  runR2: 0.62,  elapsed 8.1s (compute 7.4)\n",
      "[1, 44800] Loss: 3.51e-03  runR2: 0.62,  elapsed 7.7s (compute 7.4)\n",
      "[1, 45600] Loss: 3.87e-03  runR2: 0.62,  elapsed 8.1s (compute 7.7)\n",
      "[1, 46400] Loss: 3.53e-03  runR2: 0.62,  elapsed 8.1s (compute 7.7)\n",
      "[1, 47200] Loss: 3.59e-03  runR2: 0.62,  elapsed 8.1s (compute 7.6)\n",
      "[1, 48000] Loss: 3.15e-03  runR2: 0.62,  elapsed 8.5s (compute 8.1)\n",
      "[1, 48800] Loss: 3.27e-03  runR2: 0.63,  elapsed 8.5s (compute 8.1)\n",
      "[1, 49600] Loss: 3.34e-03  runR2: 0.63,  elapsed 7.1s (compute 6.8)\n",
      "Epoch 1 TRAIN loss: 4.13e-03  MSE: 1.48e-03  h-con:  2.65e+03   R2: 0.63  R2-dT/dt: -1.16   R2-dq/dt: 0.19   R2-precc: 0.837\n",
      "Epoch 1/2 complete, took 519.36 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 4.54e-03  runR2: 0.68,  elapsed 15.0s (compute 7.8)\n",
      "[2, 1600] Loss: 3.44e-03  runR2: 0.69,  elapsed 8.7s (compute 8.3)\n",
      "[2, 2400] Loss: 3.88e-03  runR2: 0.71,  elapsed 8.1s (compute 7.7)\n",
      "[2, 3200] Loss: 3.16e-03  runR2: 0.71,  elapsed 8.6s (compute 8.2)\n",
      "[2, 4000] Loss: 3.34e-03  runR2: 0.72,  elapsed 8.7s (compute 8.4)\n",
      "[2, 4800] Loss: 3.70e-03  runR2: 0.72,  elapsed 8.4s (compute 8.1)\n",
      "[2, 5600] Loss: 3.27e-03  runR2: 0.72,  elapsed 8.7s (compute 8.4)\n",
      "[2, 6400] Loss: 3.27e-03  runR2: 0.71,  elapsed 8.9s (compute 8.5)\n",
      "[2, 7200] Loss: 3.29e-03  runR2: 0.71,  elapsed 8.5s (compute 8.1)\n",
      "[2, 8000] Loss: 3.27e-03  runR2: 0.71,  elapsed 8.7s (compute 8.0)\n",
      "[2, 8800] Loss: 3.02e-03  runR2: 0.71,  elapsed 8.8s (compute 8.5)\n",
      "[2, 9600] Loss: 3.47e-03  runR2: 0.71,  elapsed 8.7s (compute 8.3)\n",
      "[2, 10400] Loss: 3.31e-03  runR2: 0.71,  elapsed 8.7s (compute 8.3)\n",
      "[2, 11200] Loss: 2.96e-03  runR2: 0.71,  elapsed 8.5s (compute 8.1)\n",
      "[2, 12000] Loss: 3.09e-03  runR2: 0.71,  elapsed 8.2s (compute 7.9)\n",
      "[2, 12800] Loss: 3.38e-03  runR2: 0.71,  elapsed 7.6s (compute 7.2)\n",
      "[2, 13600] Loss: 3.01e-03  runR2: 0.71,  elapsed 8.4s (compute 8.1)\n",
      "[2, 14400] Loss: 3.88e-03  runR2: 0.72,  elapsed 8.6s (compute 8.2)\n",
      "[2, 15200] Loss: 2.87e-03  runR2: 0.72,  elapsed 9.2s (compute 8.5)\n",
      "[2, 16000] Loss: 3.22e-03  runR2: 0.72,  elapsed 8.7s (compute 8.4)\n",
      "[2, 16800] Loss: 3.09e-03  runR2: 0.72,  elapsed 9.2s (compute 8.8)\n",
      "[2, 17600] Loss: 3.30e-03  runR2: 0.72,  elapsed 8.5s (compute 8.1)\n",
      "[2, 18400] Loss: 2.97e-03  runR2: 0.72,  elapsed 8.2s (compute 7.8)\n",
      "[2, 19200] Loss: 3.10e-03  runR2: 0.72,  elapsed 8.3s (compute 8.0)\n",
      "[2, 20000] Loss: 3.15e-03  runR2: 0.72,  elapsed 8.3s (compute 7.9)\n",
      "[2, 20800] Loss: 3.06e-03  runR2: 0.72,  elapsed 8.7s (compute 8.3)\n",
      "[2, 21600] Loss: 2.90e-03  runR2: 0.72,  elapsed 8.6s (compute 8.2)\n",
      "[2, 22400] Loss: 2.85e-03  runR2: 0.72,  elapsed 9.3s (compute 8.5)\n",
      "[2, 23200] Loss: 3.38e-03  runR2: 0.72,  elapsed 8.7s (compute 8.3)\n",
      "[2, 24000] Loss: 3.14e-03  runR2: 0.72,  elapsed 8.8s (compute 8.4)\n",
      "[2, 24800] Loss: 3.35e-03  runR2: 0.72,  elapsed 8.9s (compute 8.5)\n",
      "[2, 25600] Loss: 3.35e-03  runR2: 0.72,  elapsed 9.0s (compute 8.6)\n",
      "[2, 26400] Loss: 3.39e-03  runR2: 0.72,  elapsed 8.9s (compute 8.5)\n",
      "[2, 27200] Loss: 3.06e-03  runR2: 0.72,  elapsed 8.7s (compute 8.4)\n",
      "[2, 28000] Loss: 3.30e-03  runR2: 0.72,  elapsed 8.9s (compute 8.6)\n",
      "[2, 28800] Loss: 3.66e-03  runR2: 0.72,  elapsed 8.9s (compute 8.5)\n",
      "[2, 29600] Loss: 3.09e-03  runR2: 0.72,  elapsed 9.3s (compute 8.5)\n",
      "[2, 30400] Loss: 3.53e-03  runR2: 0.72,  elapsed 8.9s (compute 8.6)\n",
      "[2, 31200] Loss: 3.27e-03  runR2: 0.72,  elapsed 9.0s (compute 8.6)\n",
      "[2, 32000] Loss: 3.46e-03  runR2: 0.72,  elapsed 8.7s (compute 8.3)\n",
      "[2, 32800] Loss: 3.93e-03  runR2: 0.72,  elapsed 8.8s (compute 8.4)\n",
      "[2, 33600] Loss: 3.16e-03  runR2: 0.72,  elapsed 8.9s (compute 8.5)\n",
      "[2, 34400] Loss: 3.03e-03  runR2: 0.72,  elapsed 8.9s (compute 8.6)\n",
      "[2, 35200] Loss: 3.54e-03  runR2: 0.72,  elapsed 8.8s (compute 8.4)\n",
      "[2, 36000] Loss: 3.11e-03  runR2: 0.72,  elapsed 8.9s (compute 8.5)\n",
      "[2, 36800] Loss: 2.82e-03  runR2: 0.72,  elapsed 9.0s (compute 8.3)\n",
      "[2, 37600] Loss: 2.56e-03  runR2: 0.72,  elapsed 8.8s (compute 8.4)\n",
      "[2, 38400] Loss: 2.97e-03  runR2: 0.72,  elapsed 8.6s (compute 8.2)\n",
      "[2, 39200] Loss: 2.94e-03  runR2: 0.72,  elapsed 8.1s (compute 7.7)\n",
      "[2, 40000] Loss: 2.95e-03  runR2: 0.72,  elapsed 7.7s (compute 7.3)\n",
      "[2, 40800] Loss: 3.07e-03  runR2: 0.72,  elapsed 8.6s (compute 8.2)\n",
      "[2, 41600] Loss: 3.13e-03  runR2: 0.73,  elapsed 8.6s (compute 8.2)\n",
      "[2, 42400] Loss: 3.71e-03  runR2: 0.73,  elapsed 8.9s (compute 8.5)\n",
      "[2, 43200] Loss: 2.86e-03  runR2: 0.73,  elapsed 8.9s (compute 8.6)\n",
      "[2, 44000] Loss: 3.28e-03  runR2: 0.73,  elapsed 9.2s (compute 8.5)\n",
      "[2, 44800] Loss: 3.18e-03  runR2: 0.73,  elapsed 8.6s (compute 8.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 45600] Loss: 3.49e-03  runR2: 0.73,  elapsed 8.7s (compute 8.3)\n",
      "[2, 46400] Loss: 3.14e-03  runR2: 0.73,  elapsed 8.7s (compute 8.3)\n",
      "[2, 47200] Loss: 3.25e-03  runR2: 0.73,  elapsed 8.5s (compute 8.2)\n",
      "[2, 48000] Loss: 2.84e-03  runR2: 0.73,  elapsed 8.3s (compute 8.0)\n",
      "[2, 48800] Loss: 2.98e-03  runR2: 0.73,  elapsed 8.9s (compute 8.5)\n",
      "[2, 49600] Loss: 3.05e-03  runR2: 0.73,  elapsed 8.3s (compute 8.0)\n",
      "Epoch 2 TRAIN loss: 3.23e-03  MSE: 1.15e-03  h-con:  2.08e+03   R2: 0.73  R2-dT/dt: 0.59   R2-dq/dt: 0.46   R2-precc: 0.928\n",
      "VALIDATION..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALIDATION..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mval_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimewindoww\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     losses_val \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mk: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m val_runner\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_wandb: wandb\u001b[38;5;241m.\u001b[39mlog(losses_val)\n",
      "Cell \u001b[0;32mIn[23], line 62\u001b[0m, in \u001b[0;36mmodel_train_eval.eval_one_epoch\u001b[0;34m(self, epoch, timewindow)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     loss_update_start_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 62\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_lay_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_sfc_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_lay_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_sfc_chunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_lay_chunks\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_lay_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    945\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/nobackups/miniforge/envs/py311_torch_cu121/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 160 160\n",
    "# autoreg, hybrid-loss, 2 years instead of 1\n",
    "num_epochs = 2\n",
    "save_model = False\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "if use_val: val_runner = model_train_eval(val_loader, model, autoregressive, train=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80d8f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/28272/ipykernel_3268844/3348240014.py:28: RuntimeWarning: divide by zero encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_3268844/3348240014.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_3268844/1891790002.py:176: RuntimeWarning: invalid value encountered in add\n",
      "  epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 800] Loss: 3.84e-03  runR2: -39.14,  elapsed 24.4s (compute 3.2)\n",
      "[2, 1600] Loss: 2.91e-03  runR2: -31.61,  elapsed 12.1s (compute 4.1)\n",
      "[2, 2400] Loss: 3.43e-03  runR2: -35.92,  elapsed 4.2s (compute 3.8)\n",
      "[2, 3200] Loss: 3.46e-03  runR2: -34.63,  elapsed 15.5s (compute 3.3)\n",
      "[2, 4000] Loss: 3.00e-03  runR2: -31.95,  elapsed 3.9s (compute 3.6)\n",
      "[2, 4800] Loss: 3.11e-03  runR2: -29.17,  elapsed 19.0s (compute 4.3)\n",
      "[2, 5600] Loss: 3.34e-03  runR2: -29.34,  elapsed 4.2s (compute 3.8)\n",
      "[2, 6400] Loss: 3.37e-03  runR2: -30.01,  elapsed 7.5s (compute 3.9)\n",
      "[2, 7200] Loss: 2.88e-03  runR2: -29.75,  elapsed 6.0s (compute 4.0)\n",
      "[2, 8000] Loss: 3.32e-03  runR2: -29.91,  elapsed 15.7s (compute 3.4)\n",
      "[2, 8800] Loss: 3.34e-03  runR2: -29.47,  elapsed 15.6s (compute 3.3)\n",
      "[2, 9600] Loss: 3.32e-03  runR2: -29.15,  elapsed 3.7s (compute 3.3)\n",
      "[2, 10400] Loss: 3.26e-03  runR2: -29.16,  elapsed 12.3s (compute 3.3)\n",
      "[2, 11200] Loss: 3.27e-03  runR2: -29.70,  elapsed 3.9s (compute 3.5)\n",
      "[2, 12000] Loss: 3.36e-03  runR2: -30.14,  elapsed 10.8s (compute 3.7)\n",
      "[2, 12800] Loss: 3.02e-03  runR2: -30.24,  elapsed 3.8s (compute 3.5)\n",
      "[2, 13600] Loss: 3.31e-03  runR2: -29.61,  elapsed 9.3s (compute 3.8)\n",
      "[2, 14400] Loss: 3.16e-03  runR2: -29.34,  elapsed 9.8s (compute 3.4)\n",
      "[2, 15200] Loss: 3.42e-03  runR2: -28.72,  elapsed 10.2s (compute 3.9)\n",
      "[2, 16000] Loss: 3.48e-03  runR2: -29.32,  elapsed 4.1s (compute 3.7)\n",
      "[2, 16800] Loss: 3.25e-03  runR2: -29.68,  elapsed 14.8s (compute 3.7)\n",
      "[2, 17600] Loss: 3.04e-03  runR2: -29.73,  elapsed 4.0s (compute 3.7)\n",
      "[2, 18400] Loss: 3.05e-03  runR2: -29.65,  elapsed 10.0s (compute 3.6)\n",
      "[2, 19200] Loss: 3.17e-03  runR2: -29.63,  elapsed 12.6s (compute 3.6)\n",
      "[2, 20000] Loss: 3.38e-03  runR2: -28.97,  elapsed 3.8s (compute 3.6)\n",
      "[2, 20800] Loss: 3.35e-03  runR2: -28.76,  elapsed 14.3s (compute 3.4)\n",
      "[2, 21600] Loss: 3.33e-03  runR2: -28.88,  elapsed 3.9s (compute 3.5)\n",
      "[2, 22400] Loss: 3.39e-03  runR2: -28.84,  elapsed 10.6s (compute 3.6)\n",
      "[2, 23200] Loss: 3.40e-03  runR2: -28.83,  elapsed 3.6s (compute 3.3)\n",
      "[2, 24000] Loss: 3.33e-03  runR2: -28.60,  elapsed 9.7s (compute 3.4)\n",
      "[2, 24800] Loss: 3.31e-03  runR2: -28.66,  elapsed 6.6s (compute 3.4)\n",
      "[2, 25600] Loss: 3.37e-03  runR2: -28.39,  elapsed 9.6s (compute 3.6)\n",
      "Epoch 2 VAL loss: 3.26e-03  MSE: 3.07e-03  h-con:  1.93e+03   R2: -28.36  R2-dT/dt: -3094.39   R2-dq/dt: -0.62   R2-precc: 0.958\n"
     ]
    }
   ],
   "source": [
    "if use_val: val_runner = model_train_eval(val_loader, model, autoregressive, train=False)\n",
    "\n",
    "if epoch%2:\n",
    "    print(\"VALIDATION..\")\n",
    "    val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "    losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "    if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "    val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "    # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "    if save_model and val_loss < best_val_loss:\n",
    "      torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss,\n",
    "                  }, SAVE_PATH)  \n",
    "      best_val_loss = val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "229dd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd674e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = train_runner.metrics[\"R2_lev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fccba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f15a7c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(R2[:,1],np.arange(60))\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95a5140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(60),R2[:,1])\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec4a3176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGiCAYAAAA1LsZRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCVElEQVR4nO3deXiU9b3//9dkJpnsCVnIQkIIOxIWSWQJAoqKB5eKrUqrFfeKohSxHov+vnU5bWk9ra0tgnWvLSpVweoRhbQqOwViEASUnbBkIQkkISGTZOb+/ZFkJCaBTAjcM5nn47rmktxzT+bNR5K88lkthmEYAgAAMEmA2QUAAAD/RhgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKbyOIysXLlS1157rZKTk2WxWPT++++f8TUrVqxQZmamgoOD1bt3b73wwgsdqRUAAHRBHoeRqqoqDRs2TPPmzWvX/fv27dNVV12lcePGKS8vT4899phmzpyp9957z+NiAQBA12M5m4PyLBaLlixZoilTprR5z6OPPqoPPvhAO3bscF+bPn26vvzyS61bt66jbw0AALoI27l+g3Xr1mnSpEnNrl155ZV65ZVXVFdXp8DAwBavcTgccjgc7o9dLpfKysoUGxsri8VyrksGAACdwDAMVVZWKjk5WQEBbQ/GnPMwUlhYqISEhGbXEhISVF9fr5KSEiUlJbV4zdy5c/XUU0+d69IAAMB5cPDgQaWkpLT5/DkPI5Ja9GY0jQy11csxZ84czZ492/1xeXm5evbsqYMHDyoyMvLcFeohwzC0q7hSa3aXaO3uMm3KP6a6elezewJtAYoJDVR0aJBiQoMUHRqomLAgRYcEKSzYquBAq0ICAxRss8oeZFWw1argoADZbQ0J0ukyZBiS0zAa/2zI5ZIcTpdO1NSrvKZOlTV1qjhZ3/jfOpXX1MtR51RtvUu1Tpdq611y1LtUW++Uw+mSo95QvdOljg/QtS0qxKYe0aHqGROinjFhSokJUc+YUKXFhCouwk7PFgD4kYqKCqWmpioiIuK0953zMJKYmKjCwsJm14qLi2Wz2RQbG9vqa+x2u+x2e4vrkZGRXhNGcg8c06xFeTpYdvLbi9ZgpcQEa8KAeI3vF6/RvWMVHRrotT+AnS5DdU5X46MhoNQ6G4LLyVqnqmudqq6t18lap07Wffvxseo6lVQ6dPSEQyUnHCqprFXJCYfqXYYqXdLXZfX6uqxSUmWz9wsJtKp3fJiG9IjSkJQoDe0Rrf6J4bLbrOY0AADgvDjTz8FzHkbGjBmjDz/8sNm15cuXKysrq9X5Ir5g7Z4S3f3XTaqudcpuC9Do3rEa3z9eE/rHqU98uNeGj++yBlhkDWjonTlbhmGo/GSdCitqlF9arQOl1TpQVqUDpdXaX1qlw8dO6mSdU9uOVGjbkQq9vfGgJCnQatHAxEhl9IjS0JQoZfeJVVps2FnXAwDwHR6HkRMnTmj37t3uj/ft26fNmzcrJiZGPXv21Jw5c3T48GG98cYbkhpWzsybN0+zZ8/WPffco3Xr1umVV17RW2+91Xl/i/Pos2+KNf1vuXLUuzSuX5xe+HGmwuznZbTLq1ksFkWHBik6NEgDE1v2XtXWu3ToWLV2FlVqy6FybT3c8DheXef+81sbGu4dkBChKwcnaNLgRA1OjvSZcAcA6BiPl/Z+/vnnuvTSS1tcv+222/T666/r9ttv1/79+/X555+7n1uxYoUeeughbdu2TcnJyXr00Uc1ffr0dr9nRUWFoqKiVF5ebuowzSdfFejBt/JU5zR0+aAEzbv5wk7pVfBXhmHo0LGT2nq4XFsOlSsv/5g2HTgmp+vbf5I9okM0aXCCrhycqKy0brJZ2TQYAHxFe39+n9U+I+eLN4SR9/MO6+F3vpTTZeiaoUn6w9ThCuQHY6c7Xl2rT78u1rJthVqx86hq6r6dEBwbFqSZl/XTj0enyRpAbwkAeDvCSCd6a0O+HluyVYYh3ZCZot/+YCg/DM+Dk7VOrdp1VMu2FenfXxfpeHWdJCmjR6R+OWWIhqdGm1sgAOC0CCOd5NXV+/T0/22XJE0bk6Ynrx2sAILIeVfvdOmtDfl6Ztk3qqypl8Ui/WhkT/33lQMUHRpkdnkAgFa09+c34wyn8ZcVe9xB5N7xvfXU9wgiZrFZA3TrmF769OFL9P0RPWQY0pv/ydfE36/QO5sOyuXy+kwNAGgDPSNtOHL8pC7+7adyGdKsy/vpp5f1Y1WHF1m/t1T/7/2vtKv4hCTpol7d9MspQzQg8fQb6wAAzh96Rs7Su7mH5DKkUekxmnV5f4KIlxndO1ZLfzpOj101UKFBVm3cf0w3vLBWpSccZ34xAMCrEEZa4XIZ+semhk25fjgy1eRq0JZAa4B+Mr6P/jV7gvonhKuypt69mRoAwHcQRlqxbm+pDh07qYhgmyZntDzID94lOTpE913SR5L0t3UHVOd0neEVAABvQhhpxaLG366vG57MpmY+4qohSYoLt6uwokaffFV45hcAALwGYeQ7yqvr9Mm2hh9mU7N6mlwN2stus+qWUQ3/v15fu9/cYgAAHiGMfMf7mw+rtt6lQUmRyujhHScEo31uGd1TgVaLcg8c05ZDx80uBwDQToSR72gaopmalcIKGh/TPSJY1wxNliS9vma/ucUAANqNMHKKrw6Xa3tBhYJsAZpyYQ+zy0EH3J7dS5L04ZYjKq6sMbcYAEC7EEZO8fbGfEnSlYMT2WLcRw1LjdaIntGqcxp68z/5ZpcDAGgHwkijmjqn/rn5iCRpahZ7i/iy28emS5L+vj5ftfUs8/U1hmGo3unSyVqnKmrqVFZVq6KKGh06Vq19JVXKL63m/yvQxdjMLsBbfPxVgSpr6pXSLUTZfWLNLgdnYXJGohIi7SqqcGjp1gKG3LzcCUe9vjhwTBv3l2nDvjJ9eei4aupOHzYCLFJSVIh6xoQ2PGJDldr4577dwxVu51sb4Ev4im3UNHH1xsxUDsPzcYHWAN06Ok2/W75Tr63Zp+uGJzMZ2YuUV9dp3d4SbdjXEEC2F1TIeYaDDq0BFgVaLQoMCFCt0yVHvUuHj5/U4eMntW5vabN7AyzS4OQojUyP0cj0GF3UK0YxYQy7At6MMCLpQGmV1u8tk8Ui3ZCVYnY56AQ/GtlTf/p0t748VK68g8c1omc3s0vye8UVNXpx5V4t/E++TtY5mz2X0i1EI3s1hIesXt0UHx4sm9WiQGuAbAGWZr8gGIahkhO1yi+rUn5ZtfJLTyq/rFoHy6q1v7RKxZUObT1crq2Hy/XK6n2SpP4J4Y3hJFbj+8UxJwzwMoQRyX0Ozbh+8eoRHWJyNegMseF2fW9Yst7NPaTX1+wnjJjo8PGT+suKPXp740H3XI/e8WHK7hOrixoDSFJU+7/uLBaL4iPsio+wKzMtpsXzBeUntWFfmfuxq/iEdhY1PP6+Pl+2AIvG9o3T1UOSNGlwAsEE8AIWwzBO3z/qBdp7BHFH1DtdGvvbT1VU4dD8W0boqiGcRdNVfHW4XNf8ebVsARatfnSiEqOCzS7Jr+SXVmv+57v13heHVOds+DaTmdZND07sqwn948/b0FnpCYc27j+mDfvKtGZ3ib4pqnQ/ZwuwKLtvnK4ekqhJFySqG8M5QKdq789vvw8jn35dpDtf36SYsCCtn3OZgmwsMOpKbnphnTbsL9ODE/vq4UkDzC7HLxw6Vq1nl+/UP7884p4Lkt0nVg9M7KsxvWNNn7+z5+gJLd1SoI+2Fujrwm+DiTXAoov7xukn43sru4/5dQJdQXt/fvv9ME3TxNXrL+xBEOmCbh/bSxv2l+nN/+RrxqV9OfjwHPvkqwI98u4WVdbUS5IuGRCvByf2bXU4xSx94sP14GX99OBl/bT36Akt3Vqgj7YWakdBhVbsPKoVO49qeGq0HpzYVxMHdieUAOeBX/eMHK10aMzcf6veZWj5Q+PVPyGi0z43vEO906Xxz3ymI+U1+t8bhupG9pA5J2rqnJq7dIf+uu6AJOnCntF6+nsZGpISZXJl7bf36Am9se6A3tqQL0fj3JZBSZGacWkfTc5IkpVVdoDH2vvz26+7ApbkHVK9y9Dw1GiCSBdlswboltFpkqSlWwtMrqZr2l9SpR8sWOsOIvdO6K1/3DvGp4KIJPWOD9eT3xus1Y9O1L0TeissyKodBRV64M08XfGHFXo395DqnGy2BpwLfh1Glm4tlCRNvYjflruyYSnRkhpWdaBzffDlEV3z59XadqRCMWFBeu2OizRn8iAFWn33W0t8hF1zJg/Smp9P1E8v66eokEDtPVqln73zpa7840qt21N65k8CwCN+PUxzstapj78q0BUXJCgiOLDTPi+8y+7iE7r82RWKDLZpy5NXml1Ol1BT59RTH27XWxsazv8Z2StGf/rRhV1yxdIJR73+vv6AXlq5V6VVtZKkGzNT9NhVg1h9A5wBwzTtEBJk1fdHpBBEurimH5AVNfWqctSbXI3vK66o0ZTn1+itDfmyWKSZE/vqzXtGdckgIknhdpumT+ijT392iW4e1VOS9E7uIV3+7AotyTskH/h9DvB6fh1G4B/C7TZFBDcsHCusqDG5Gt9WXVuvu9/YpK8LKxUXbtff7hyl2ZMGyObDwzLtFRUSqF9fP0TvTh+jft3DVVpVq4cWfalpr27QgdIqs8sDfFrX/w4CSEpq/K29sJww0lFOl6FZb2/WlkPligkL0nv3jdHF/eLMLuu8y+oVo49mjtMjVw5QkC1Aq3aVaNIfVur5z3YzwRXoIMII/EJi43bjBYSRDvvNxzu0fHuRgqwBevHWTKXFhpldkmmCbAGacWlfLZ81XmP7xspR79L/LvtGNyxYq/0l9JIAniKMwC8kRtolSYXlrKjpiL+vP6CXVjUcOve/Nw5VVi/v2cTMTL3iwvT3u0bp2ZuGKTLYpi8PlevqP63Se7nMJQE8QRiBX6BnpONW7DyqJz7YJkl6+Ir+um54D5Mr8i4Wi0XfH5GiT2aN18j0GFXVOvXwO19q1qLNqqipM7s8wCcQRuAXmDPSMV8XVmjGwi/kdBn6wYgUPTCxr9klea3k6BC9dc9oPXxFf1kDLPrn5iO6+k+r9EX+MbNLA7weYQR+oWnZKT0j7VdcUaO7Xt+kE456je4do7nfH8I5LWdgDbDowcv66R/3jlFKtxAdLDupG19Yp3mf7nIfGgigJcII/IK7Z4Slve3StIT38PGT6h0Xphd+nMlBkh7ITOumpT8dp+8NS5bTZeh3y3fq5pfWq5h/f0Cr+O4Cv5AU2TBnpKyqVjV1TpOr8W6GYeiRd7a4l/C+dsdFig5lp1FPRQYH6rkfDtfvbxymsCCr/rOvTFf/ebU27CszuzTA6xBG4BciQ2wKCbRKkor47fS0Pv6qUB9tLVCg1eL3S3jPlsVi0Q8yU/R/M8dpQEKEjlY69KOX1uvlVXtZbQOcgjACv2CxWNxDNcwbaVtFTZ2ebFw5c98lfVnC20nS48K0ZEa2pgxvGLb55Uc79MCbeTrB8QSAJMII/EgiK2rO6HfLvlFxpUPpcWG6/5I+ZpfTpYQG2fSHqcP19HWDFWi16KOtBbpu3mrtLq40uzTAdIQR+A1W1Jze5oPH9bf1ByRJv5qSoeDGYS10HovFomljeuntn4xRYmSw9hyt0nXz1uijLQVmlwaYijACv/HtXiPswvpd9U6X5izeKsOQvn9hD2X39b8zZ86nzLRu+r+ZF2tM71hV1To1480v9KuPtrP8F36LMAK/wS6sbXttzX7tKKhQdGigHr96kNnl+IW4cLv+dtdI3TuhtyTppVX7dOfrG9m1FX6JMAK/kRTJXiOtOXSsWs/m7JQkPTZ5kGLD7SZX5D9s1gDNmTxIz988QsGBAVqx86iuf36N9nHYHvwMYQR+gzkjLRmGoSf+uU0n65wamR6jG7NSzC7JL109NEnvTs9WUlTDPJIpz6/R6l0lZpcFnDeEEfiNpjkjJSccqq13mVyNd1i2rVD//rpYgVaLfn19Btu9myijR5T++cBYXdgzWuUn63Tbaxv017X72Y8EfoEwAr8RExakIGuADEMqrqR3pLKmzn0a7/QJfdS3e4TJFaF7RLDeume0vn9hDzldhp74YJseW/IV4RldHmEEfsNisbiHatiFVfr98p0qqnCoV2yoZlzKabzeIjjQqt/fNExzJg+UxSK9tSFft77yH5VV1ZpdGnDOEEbgV5g30mDLoeP667r9kqRfThnCniJexmKx6N4JffTytCyF2236z74yXT9/jXYXnzC7NOCcIIzAryRGsgurJP166Q4ZhnTd8GRd3I89RbzVZYMStPj+bKV0C9GB0mp9f/4ard3NxFZ0PYQR+BXOp5HW7SnV+r1lCrIG6NH/Gmh2OTiD/gkRen/GWI3oGa2KmnpNe3WDFm3MN7ssdCG5B8q0bk+pqWclEUbgVzifRvrjvxr2FJl6UaqSo0NMrgbtERdu15v3jNb3hiWr3mXo0fe2au7HO+Rix1Z0gnmf7taPXlqvxV8cMq0Gwgj8yrc9I/65JfzaPSX6z76GXpH7L+UgPF8SHGjVcz8crp9e1k+S9JcVe3X/wi90stZpcmXwZYZhaOvhCkkNy8vNQhiBX2naEt4fe0YMw9Af/7VLkvTDkalKiqJXxNdYLBY9dEV//XHqcAVZA/TJtkJNfXGdilkdhg4qqnCo5IRDARZpUGKkaXUQRuBXmnpGiiodfnco2bo9pdrQ1CtyCUt5fdmUC3to4T2jFBMWpC2HynXd82v0TWGl2WXBB209XC5J6tc9QiFB5q2qI4zAr8SF22UNsMjpMlRywmF2OefNqb0iPxqZ6p47A991Ua8YLbk/W33iw1RQXqMbXlirdXtKzS4LPqYpjJg5RCMRRuBnrAEWJUQ0HATnTytq1u4p1Yb9ZQqyBeg+ekW6jLTYML13X7Yu6tVNlTX1uu3VDfrgyyNmlwUfsq0xjAzpYd4QjUQYgR/6dkWNf0xibegVaVhBc/PInvSKdDHRoUH6212jdNWQRNU6XZr5Vp5eXLmHM23QLk09I0NS6BkBzqumiZv+0jOyZnepNu4/1tgrwgqarig40Kp5PxqhO8emS5J+vfRrPfXhdr+bFwXPFFfUqLiycfJqEj0jwHnlT3uNfLdXJCGSXpGuKiDAol9ce4H+v6sHSZJeX7tfMxZ+oZo6lv6idU29In3iwxUaZDO1FsII/I4/7cK6eneJNh04Jju9In7j7nG9Ne/mC91Lf295+T86xiF7aIV7iMbkyasSYQR+yF96Rk5dQXPzKHpF/Mk1Q5P1t7tGKjLYptwDxzhkD636yktW0kiEEfghd89IRdeewLpqV4lym3pFJtAr4m9G9Y7Ve/dlq0d0iPaXVuv6+Wu0cudRs8uCF/mqcedVsyevSoQR+KGmXViLyh1d9myPU+eK3DIqTd3pFfFL/RIi9M8HxiorrWHp7+2vbdBra/ax0gY6WulQYUWNLBbpApMnr0qEEfih7hF2WSxSrdOlsuquOZa+bm+pvsg/LrstQNMn9Da7HJgoLtyuhfeM0g2ZKXIZ0lMfbtdjS75SndNldmkwUdMQTe+4MIXZzZ28KnUwjMyfP1/p6ekKDg5WZmamVq1addr7Fy5cqGHDhik0NFRJSUm64447VFrKToEwR6A1QPHhDRufddV5I6+v2S9JujErhV4RyG6z6n9vGKrHrxoki0V6a0O+bn2Fia3+zJsmr0odCCOLFi3SrFmz9PjjjysvL0/jxo3T5MmTlZ+f3+r9q1ev1rRp03TXXXdp27Zteuedd7Rx40bdfffdZ1080FFdeUXNwbJq/WtHkSTp9uxe5hYDr2GxWHTP+N565bYshdttWr+3TNc9v0a7ijjTxh95yzbwTTwOI88++6zuuusu3X333Ro0aJD++Mc/KjU1VQsWLGj1/vXr16tXr16aOXOm0tPTdfHFF+vee+/Vpk2bzrp4oKOaVpYUdsHTTv+2/oBchjSuX5z6do8wuxx4mYkDE7T4/mylxoQov6xa189fq6VbC8wuC+fZV77cM1JbW6vc3FxNmjSp2fVJkyZp7dq1rb4mOztbhw4d0tKlS2UYhoqKivTuu+/q6quvbvN9HA6HKioqmj2AzpTURbeEr66t19sbGnop7xjby9xi4LX6J0TonzMu1sj0GJ1w1Ov+hV/o5+9tUXVtvdml4TwoOeFQQXnD5NXBvhhGSkpK5HQ6lZCQ0Ox6QkKCCgsLW31Ndna2Fi5cqKlTpyooKEiJiYmKjo7Wn//85zbfZ+7cuYqKinI/UlNTPSkTOKPELrol/JK8w6qoqVdabKgu6d/d7HLgxWLCgrTw7lGacWkfWSzS2xsP6po/r3b/xoyuq+n/cXpcmMK9YPKq1MEJrBaLpdnHhmG0uNZk+/btmjlzpn7xi18oNzdXn3zyifbt26fp06e3+fnnzJmj8vJy9+PgwYMdKRNoU1IX3PjMMAz3xNXbxvRSQEDrX5NAk0BrgB65cqAW3j1KiZHB2nu0St+fv1Yvr9rbZZe945TNzpK9o1dEkjyKRHFxcbJarS16QYqLi1v0ljSZO3euxo4dq0ceeUSSNHToUIWFhWncuHH65S9/qaSkpBavsdvtstvtnpQGeKQr7sK6dk+pdhWfUFiQVTdkpZhdDnxIdp84ffzTcfrv97YoZ3uRfvnRDq3aVaLf3ThM8RF8L+5qvG0ljeRhz0hQUJAyMzOVk5PT7HpOTo6ys7NbfU11dbUCApq/jdVqlSQ23oFpTl1N01X+Hb7W2CtyQ2aKIoMDzS0GPqdbWJBevDVTv5ySIbstQCt2HtXk51bqs2+KzS4Nnaxp51VvWUkjdWCYZvbs2Xr55Zf16quvaseOHXrooYeUn5/vHnaZM2eOpk2b5r7/2muv1eLFi7VgwQLt3btXa9as0cyZMzVy5EglJyd33t8E8EDTapqTdU5VnPT9SXv5pdX699cNy3mnsZwXHWSxWPTj0Wn68MGLNTAxQiUnanXHaxs16+08lZxwmF0eOkFZVa0OH2+YuD+4h/k7rzbxeObK1KlTVVpaqqeffloFBQXKyMjQ0qVLlZaWJkkqKChotufI7bffrsrKSs2bN08PP/ywoqOjNXHiRP32t7/tvL8F4KHgQKtiwoJUVlWrgoqTigr17Z6EN9btl2FIE/rHq098uNnlwMf1T4jQ+zPG6plPvtFra/fp/c1H9Nk3R/XYVQN1U1Zqm3ME4f1OnbzqTT2oFsMH+qgrKioUFRWl8vJyRUZ6T5KDb7vquVXaXlCh1+64SJcO8N2VJ1WOeo2e+29V1tT7/N8F3ufLg8c1Z/FWbS9o6NofmR6jX18/RH27E3p90fOf7db/LvtG1wxN0rybR5zz92vvz2/OpoHf6iorahbnHVZlTb3S48I0oV+82eWgixmWGq0PHhirx68apJBAqzbsK9Pk51bq2Zydqqlzml0ePORtm501IYzAbyV2gS3hG5bz7pMk3TYmjeW8OCds1gDdM763lj80XpcOiFed09Cf/r1LVz23Smt3l5hdHjzgjStpJMII/FhX2IV19e4S7TlapXC7TT/IZDkvzq3UmFC9evtFev7mEYqPsGtvSZVufvk/evCtPJ/vYfQHx6pqdehY0+RVwgjgFbrCLqyvn7KcN8KLJqOh67JYLLp6aJL+NXuCpo1JU4BF+vDLI7rs95/rpZV7Ved0mV0i2vDVkYZekbTYUEWFeNf3C8II/JavzxnZX1KlTxv3gLiN5bw4z6JCAvX0dRn64IGLdWHPaFXVOvWrpTt09Z9Waf3eUrPLQyu8cX+RJoQR+C1f34X1jXUHZBjSpQPilR4XZnY58FMZPaL03vRsPfODoYoJC9LOohP64YvrNevtPBVX+ubXVlfljdvANyGMwG8lNm58VumoV2VNncnVeKamzql3chvObKJXBGYLCLDopotS9enDE3TLqJ6yWKT3Nx/RZb9boXdzD3WZXY59nbdOXpUII/BjYXabIoMb9v0rqvCt3+CWbStUZU29ekSHaDzLeeElokOD9Kvrh+ifM8ZqWEqUKh31+tk7X2rGm1/oWFWt2eX5tfLqOuWXVUuSMrxo59UmhBH4tSQfncT6bu4hSdIPMlNYzguvMzQlWovvH6tHrhwgW4BFS7cW6r+eW6lVu46aXZrfapq8mhoToujQIJOraYkwAr+W4IN7jRw+flKrG/d2uJHlvPBS1gCLZlzaV0vuH6ve8WEqqnDo1lc26KkPt7FZmgm8eYhGIozAzyVF+t4k1vdyD8kwpNG9Y5QaE2p2OcBpDUmJ0kcPjtO0MQ3nl722Zr++N2+1th+pMLky/+KevEoYAbyPe0WNj8wZcbkM9xDNjZmpJlcDtE9IkFVPX5eh126/SHHhdu0sOqHrnl+tl1buZXLreeLNK2kkwgj8nK/tNbJhf5nyy6oVbrdp8pBEs8sBPHLpwO5aNmucrrggQXVOQ79aukMPvJmnKke92aV1aRU1ddpf2jB5lWEawAv52vk072xq6BW5ekiSQoNsJlcDeC423K4Xb83U/0zJUKDVoo+2Fuj6+Wu0v6TK7NK6rK8ONfSK9IgOUbcw75u8KhFG4OeaVtP4wvk0Jxz1Wrq1QJJ000VMXIXvslgsunV0mt7+yWjFRzQM21w7b7U+/brI7NK6HJfL0J8/3S1JykzrZnI1bSOMwK819Ywcq67z+hn+S7cU6GSdU73jwzSip/d+UwHaKzMtRh89eLEy07qpsqZed/11k5771y65XMwj6SxvbsjXur2lCg4M0Owr+ptdTpsII/BrkcE2hQZZJXn/vJGmHVdvyEyRxcLeIugaukcG6617RuvW0WkyDOkP/9qpn/wtVxU+tiuyNzpYVq25S3dIkv77yoHq5cXHRhBG4NcsFotPzBvZV1KljfuPKcAi/WAEQzToWoJsAfqfKRl65oahCrIF6F87ijRl3hrtLq40uzSfZRiGHn1vi6pqnRrZK0a3e/mxEYQR+L0kdxjx3nkj7zb2iozvH6+Exr1RgK7mpqxUvXPvGCVHBWtvSZWum7dGy7YVml2WT3pzQ77W7mkYnvntDUO9fqdmwgj8XkJEww/3ogqHyZW0zuky9F7uYUnsLYKub1hqtD548GKN7h2jqlqn7v1brn6//BvmkXjg0LFq/fqjhuGZR64c6BOnehNG4PeatoT31sPyVu06qsKKGkWHBuryC7qbXQ5wzsWF2/W3u0bpzrHpkqQ/f7pbd/11o8pPMo/kTAzD0M/f26qqWqcu6tVNd3j58EwTwgj8XmKkd4eRdxp3XJ0yvIfsNqvJ1QDnR6A1QL+49gL9Yeow2W0B+uybo7pu3mrtLGIeyem8teGgVu8ukd0WoGduGOb1wzNNCCPwewmRdkneuSX88epa5Wxr2HvhBg7Fgx+6/sIUvXdftnpEh2h/abWmPL9GHzfut4PmDh2r1q8+2i5JeuTKAT4xPNOEMAK/1zQhtMgLV9N88OUR1TpdGpQU6bUHXAHnWkaPKH344MXK7hOr6lqn7lv4hX77ydeqc7rMLs1rGIahOYsbhmey0rrpjsYhLl9BGIHfa1raW1zp8LpJck3bv99Irwj8XExYkN64c6TuGdfwQ3bB53t07Z9XK/fAMZMrM59hGPr7+gNatatpeGaorD4yPNOEwy3g9+LC7bJYpHqXodKqWsVH2M0uSZL0dWGFth4uV6DVoikX9jC7HMB0NmuAHr/6Ag1Ljdb/e/8rfV1YqRteWKtbRvXUI1cOVFRIoNklnjfl1XVavbtEK3YWa+XOEvcw8yNXDlDv+HCTq/McYQR+L9AaoLhwu45WOlRUUeM1YWRJXsNy3ssGJijGSw+3AsxwzdBkZfeJ06+X7tC7uYf09/X5WratSE9ce4GuHpLUJXcodrkMfXnouFbubAggmw8e16kduXZbgL4/oofPDc80IYwAalhR0xRGvGFuhmEYWt44cfXaYckmVwN4n5iwIP3uxmH6wYgUPb5kq/aWVOmBN/P07oBD+p/rMpQaE2p2iZ1m1a6j+s3HX2vbkYpm1/t1D9eE/vEa3z9eI9NjFBzou6vtCCOAGlbUbD3sPStq9hw9oX0lVQqyBmjCgHizywG81pg+sfp41jgt+HyP5n+2R59/c1RX/GGF7r+kr24f20uRwb47dLPl0HH99pOvtWZ3qSQpLMiqCQPiNb5fQwBJjg4xucLOQxgB5H0rapY19opk941VuJ0vU+B07DarZl3eX9cMTdbjS7bqP/vK9GzOTr28aq9uH5uuO8f2UnSo7wx17iup0u+Wf6OPtjQsYQ6yBujWMWmacWnfLjtky3c5QKdufOYdW8LnbG8II5MuSDS5EsB39O0errd/MloffHlE8z7drV3FJ/Snf+/SK6v26sdj0nTPuN6KC/eOOWGtKa6s0Z/+vUtvbzioepchi0W6/sIemn1Ff6V06zrDTq0hjAD6tmfEG4ZpiipqtPngcVksYvt3wEMWi0XXDe+ha4cma9m2Qv35093aXlChv6zYq7+u3a8fjeype8f3cS/pN9uxqlr9a0eRlm0r0qpdR+Wob9g7ZeLA7nrkygEalBRpcoXnB2EEkHedT9PUKzI8NVrdI7zjGybgawICLJo8JEn/lZGoT78u1p8+3a0vDx7Xa2v26+/rD2h071j35M9+3cPP6wqcI8dPavm2Qi3bVqQN+8vkPGVZzPDUaP188kCN7h173urxBoQRQN51Ps1yhmiATmOxWHTZoARNHNhdq3eX6M+f7taGfWVatatEq3aVSB/tUFJUsMb1i9P4/vG6uG9cp80vMQxDRRUO7Sup0v7SKu0vqdK6vaXacqi82X2DkiJ15eAEXTk4UQMTI7rk0uQzIYwA+vZ8mmPVdaqpc5q2RK6ypk7r9pRIkiYNTjClBqArslgsGtcvXuP6xWtXUaVW7DyqFTuPasO+MhWU1+gfmw7pH5sOKcAi9eseofgIu2LCgpo9YsOC1C0sSBZJJ+ucqqlz6mSdUydrXe4/V9TUKb+0WvtKqnSgtFon65yt1CJlpXXTlYMTNemCRPWM7drzQdqDMAJIigoJlN0WIEe9S8UVDtO+OXz+zVHVOQ31iQ9THx/cRRHwBf0SItQvIUJ3j+utmjqn/rOvTCt3HtXKnUe1q/iEvimq1DeddDqwNcCilG4h6hUbpvS4MA1KitDEgQles7mityCMAGr4rSkxKlgHSqtVVFljWhhpGqK5giEa4LwIDrRqQv94TejfsJ/PkeMn9U1RpY5V1aqsqlalVbU61vjfssY/yyKFBFoVHGj99r9BVoUEBig0yKbUmFClx4WqV2yYUmNCFWjlGLgzIYwAjRIiGsJIoUl7jdTWu/T518WSGKIBzJIcHdKlNhPzFcQ1oJHZK2rW7y1VpaNe8RF2DU+JNqUGADADYQRolNg4idWsMLJ8e6Ek6fJBCQrwseO/AeBsEEaARt9ufHb+d2F1uYxvd11liAaAnyGMAI3MPJ9m6+FyFVU4FBZkVXYf/9rsCAAII0Cjpu2hiyrPfxhpGqK5ZGB32W2+eww4AHQEYQRolNC49XpheY0MwzjD3Z1r+bamXVcZogHgfwgjQKPujRNYHfUulZ+sO2/vu6+kSruKT8gWYNElAzgYD4D/IYwAjYIDreoWGihJKjqPk1hzGodoxvSJVVRI4Hl7XwDwFoQR4BTfrqg5f/NGmoZormCIBoCfIowApzjfK2qOVjqUm39MUsP+IgDgjwgjwCkSI8/vLqyffl0kw5CGpkSxBTUAv0UYAU6R0DiJ9XwN07CKBgAII0Az5/N8mipHvVbtLpHEKb0A/BthBDhF014j52M1zeffHFVtvUtpsaHqnxB+zt8PALwVYQQ4RdMurOdjmObjrwokSf+VkSiLhYPxAPgvwghwiqbVNCUnHKpzus7Z+9TUOfXp18WSpMkZSefsfQDAFxBGgFPEhgXJFmCRYTQEknNl5c6jqq51KjkqWMNSos7Z+wCALyCMAKcICLCoe0TjippzuNfIJ1817Lr6XxlJDNEA8HuEEeA7zvWKmtp6l3J2NCzpnTyEVTQAQBgBvuNcr6hZu6dElTX1io+wK7Nnt3PyHgDgSwgjwHec6xU1H29tHKIZnKiAAIZoAIAwAnzHuTyfpt7p0vLGU3onZzBEAwASYQRooWlL+KLKzg8jG/aV6Vh1nbqFBmpkekynf34A8EWEEeA7mg7LOxeraT5uXEUz6YJE2ax8+QGARBgBWvh2NU3nTmB1uQx9sq1xvgiraADArUNhZP78+UpPT1dwcLAyMzO1atWq097vcDj0+OOPKy0tTXa7XX369NGrr77aoYKBc61pzsgJR71OOOo77fPm5h/T0UqHIoJtGtsnrtM+LwD4OpunL1i0aJFmzZql+fPna+zYsfrLX/6iyZMna/v27erZs2err7nppptUVFSkV155RX379lVxcbHq6zvvmzzQmcLtNoXbbTrhqFdRRY3C4zvnELumVTRXDEpQkI1OSQBo4nEYefbZZ3XXXXfp7rvvliT98Y9/1LJly7RgwQLNnTu3xf2ffPKJVqxYob179yompmHCXq9evc6uauAcS4i068TRehWV16hPJ4QRwzC0rGmIhlU0ANCMR7+e1dbWKjc3V5MmTWp2fdKkSVq7dm2rr/nggw+UlZWlZ555Rj169FD//v31s5/9TCdPnmzzfRwOhyoqKpo9gPPJvby3k1bUbDlUrsPHTyo0yKrx/eM75XMCQFfhUc9ISUmJnE6nEhISml1PSEhQYWFhq6/Zu3evVq9ereDgYC1ZskQlJSW6//77VVZW1ua8kblz5+qpp57ypDSgU327oqZzJrEu/apAknTpwO4KDrR2yucEgK6iQwPX3z3YyzCMNg/7crlcslgsWrhwoUaOHKmrrrpKzz77rF5//fU2e0fmzJmj8vJy9+PgwYMdKRPosM48n8YwDPfBeFdlJJ315wOArsajnpG4uDhZrdYWvSDFxcUtekuaJCUlqUePHoqK+vaY9EGDBskwDB06dEj9+vVr8Rq73S673e5JaUCnSmg8ubczwsiOgkodKK2W3RagSwYwRAMA3+VRz0hQUJAyMzOVk5PT7HpOTo6ys7Nbfc3YsWN15MgRnThxwn1t586dCggIUEpKSgdKBs69zjyf5pPGIZoJ/eMVZvd4zjgAdHkeD9PMnj1bL7/8sl599VXt2LFDDz30kPLz8zV9+nRJDUMs06ZNc99/8803KzY2VnfccYe2b9+ulStX6pFHHtGdd96pkJCQzvubAJ2oM8+nWdo4RDOZjc4AoFUe/5o2depUlZaW6umnn1ZBQYEyMjK0dOlSpaWlSZIKCgqUn5/vvj88PFw5OTl68MEHlZWVpdjYWN1000365S9/2Xl/C6CTNYWR4kqHXC6jw6fr7i6u1O7iEwq0WnTZoNaHMgHA33Woz/j+++/X/fff3+pzr7/+eotrAwcObDG0A3iz+Ai7LBap3mWotKpW8REdm8PUtNHZxX3jFBkc2JklAkCXwTaQQCsCrQGKCz/7Saz/2lEkSZrMKhoAaBNhBGhDQuTZhZETjnptPVwuSRrXn7NoAKAthBGgDe6NzzoYRjbnH5fLkHpEhygpisnaANAWwgjQhrNdUbPpQJkk6aJe3TqtJgDoiggjQBvcYaSiY1vCb9p/TJKU1Sum02oCgK6IMAK04WyGaeqdLn2R3xBGLiKMAMBpEUaANpzN+TQ7CipVXetUZLBN/bqHd3ZpANClEEaANpzNapqN+xvmi2SmdevwhmkA4C8II0AbmoZpjlXXqabO6dFrmyavMl8EAM6MMAK0ISokUEG2hi+RYg8msRqGoY37mS8CAO1FGAHaYLFY3L0jRZXtH6rJL6vW0UqHgqwBGpoSda7KA4AugzACnIZ7RY0He400LekdkhKl4EDrOakLALoSwghwGt07MIn12/kibHYGAO1BGAFOwz1M40EYcc8XSWO+CAC0B2EEOI3EqKaNz9o3gbWsqla7i09IaljWCwA4M8IIcBrdPTyfJvdAQ69I3+7h6hYWdM7qAoCuhDACnIanq2k27edwPADwFGEEOI2mMFJQXtOujc+adl7NYr4IALQbYQQ4jR7dQpQcFazaepcWbTx42ntr6pzaerhcEpudAYAnCCPAaVgDLLrvkj6SpAWf75Gjvu3ekS2HylXnNNQ9wq7UmJDzVSIA+DzCCHAGN12UqsTIYBVW1Ogfmw61ed9G93yRGFksHI4HAO1FGAHOwG6zfts78tnuNntHmiavstkZAHiGMAK0w9SLUtU9wq4j5TV6N7dl74jLZWjTAQ7HA4COIIwA7RAcaNX0CQ29I/M/26Paelez53cWV6qypl6hQVYNTIwwo0QA8FmEEaCdbh7VU/ERdh0+flLvfdG8d6RpC/gRPbvJZuXLCgA8wXdNoJ2CA626d3xvSdLzn+1WnfPb3hHmiwBAxxFGAA/cMipNceF2HTp2Uku+OOy+vmk/80UAoKMII4AHQoK+7R2Z19g7cuT4SR0+flLWAIuGp0abWyAA+CDCCOChW0b3VGxYkPLLqvV+3mH3KprByZEKs9tMrg4AfA9hBPBQaJBNPzmld2T93lJJnEcDAB1FGAE64NYxaYoJC9KB0mq9s6nhzBpO6gWAjiGMAB0QGmTTPeMaekfqnIYkKZMwAgAdQhgBOujWMWmKDg2UJKXFhqp7RLDJFQGAbyKMAB0UbrfpvsZdWSf0jze5GgDwXUz9B87CT8b31tCUaA1NiTK7FADwWYQR4CxYLBaN6RNrdhkA4NMYpgEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYijAAAAFMRRgAAgKk6FEbmz5+v9PR0BQcHKzMzU6tWrWrX69asWSObzabhw4d35G0BAEAX5HEYWbRokWbNmqXHH39ceXl5GjdunCZPnqz8/PzTvq68vFzTpk3TZZdd1uFiAQBA12MxDMPw5AWjRo3SiBEjtGDBAve1QYMGacqUKZo7d26br/vhD3+ofv36yWq16v3339fmzZvbvNfhcMjhcLg/rqioUGpqqsrLyxUZGelJuQAAwCQVFRWKioo6489vj3pGamtrlZubq0mTJjW7PmnSJK1du7bN17322mvas2ePnnjiiXa9z9y5cxUVFeV+pKamelImAADwIR6FkZKSEjmdTiUkJDS7npCQoMLCwlZfs2vXLv385z/XwoULZbPZ2vU+c+bMUXl5uftx8OBBT8oEAAA+pH3p4DssFkuzjw3DaHFNkpxOp26++WY99dRT6t+/f7s/v91ul91u70hpAADAx3gURuLi4mS1Wlv0ghQXF7foLZGkyspKbdq0SXl5eXrggQckSS6XS4ZhyGazafny5Zo4ceJZlA8AAHydR8M0QUFByszMVE5OTrPrOTk5ys7ObnF/ZGSktm7dqs2bN7sf06dP14ABA7R582aNGjXq7KoHAAA+z+NhmtmzZ+vWW29VVlaWxowZoxdffFH5+fmaPn26pIb5HocPH9Ybb7yhgIAAZWRkNHt99+7dFRwc3OI6AADwTx6HkalTp6q0tFRPP/20CgoKlJGRoaVLlyotLU2SVFBQcMY9RwAAAJp4vM+IGdq7ThkAAHiPc7LPCAAAQGcjjAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYqkNhZP78+UpPT1dwcLAyMzO1atWqNu9dvHixrrjiCsXHxysyMlJjxozRsmXLOlwwAADoWjwOI4sWLdKsWbP0+OOPKy8vT+PGjdPkyZOVn5/f6v0rV67UFVdcoaVLlyo3N1eXXnqprr32WuXl5Z118QAAwPdZDMMwPHnBqFGjNGLECC1YsMB9bdCgQZoyZYrmzp3brs8xePBgTZ06Vb/4xS9afd7hcMjhcLg/rqioUGpqqsrLyxUZGelJuQAAwCQVFRWKioo6489vj3pGamtrlZubq0mTJjW7PmnSJK1du7Zdn8PlcqmyslIxMTFt3jN37lxFRUW5H6mpqZ6UCQAAfIhHYaSkpEROp1MJCQnNrickJKiwsLBdn+P3v/+9qqqqdNNNN7V5z5w5c1ReXu5+HDx40JMyAQCAD7F15EUWi6XZx4ZhtLjWmrfeektPPvmk/vnPf6p79+5t3me322W32ztSGgAA8DEehZG4uDhZrdYWvSDFxcUteku+a9GiRbrrrrv0zjvv6PLLL/e8UgAA0CV5NEwTFBSkzMxM5eTkNLuek5Oj7OzsNl/31ltv6fbbb9ebb76pq6++umOVAgCALsnjYZrZs2fr1ltvVVZWlsaMGaMXX3xR+fn5mj59uqSG+R6HDx/WG2+8IakhiEybNk3PPfecRo8e7e5VCQkJUVRUVCf+VQAAgC/yOIxMnTpVpaWlevrpp1VQUKCMjAwtXbpUaWlpkqSCgoJme4785S9/UX19vWbMmKEZM2a4r9922216/fXXz/5vAAAAfJrH+4yYob3rlAEAgPc4J/uMAAAAdDbCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpOhRG5s+fr/T0dAUHByszM1OrVq067f0rVqxQZmamgoOD1bt3b73wwgsdKhYAAHQ9HoeRRYsWadasWXr88ceVl5encePGafLkycrPz2/1/n379umqq67SuHHjlJeXp8cee0wzZ87Ue++9d9bFAwAA32cxDMPw5AWjRo3SiBEjtGDBAve1QYMGacqUKZo7d26L+x999FF98MEH2rFjh/va9OnT9eWXX2rdunWtvofD4ZDD4XB/XF5erp49e+rgwYOKjIz0pFwAAGCSiooKpaam6vjx44qKimr7RsMDDofDsFqtxuLFi5tdnzlzpjF+/PhWXzNu3Dhj5syZza4tXrzYsNlsRm1tbauveeKJJwxJPHjw4MGDB48u8Dh48OBp84VNHigpKZHT6VRCQkKz6wkJCSosLGz1NYWFha3eX19fr5KSEiUlJbV4zZw5czR79mz3xy6XS2VlZYqNjZXFYvGk5NNqSmz0uLQfbeYZ2stztJlnaC/P0WaeOZv2MgxDlZWVSk5OPu19HoWRJt8NBIZhnDYktHZ/a9eb2O122e32Zteio6M7UGn7REZG8g/SQ7SZZ2gvz9FmnqG9PEebeaaj7XXa4ZlGHk1gjYuLk9VqbdELUlxc3KL3o0liYmKr99tsNsXGxnry9gAAoAvyKIwEBQUpMzNTOTk5za7n5OQoOzu71deMGTOmxf3Lly9XVlaWAgMDPSwXAAB0NR4v7Z09e7Zefvllvfrqq9qxY4ceeugh5efna/r06ZIa5ntMmzbNff/06dN14MABzZ49Wzt27NCrr76qV155RT/72c8672/RQXa7XU888USLISG0jTbzDO3lOdrMM7SX52gzz5yP9vJ4aa/UsOnZM888o4KCAmVkZOgPf/iDxo8fL0m6/fbbtX//fn3++efu+1esWKGHHnpI27ZtU3Jysh599FF3eAEAAP6tQ2EEAACgs3A2DQAAMBVhBAAAmIowAgAATEUYAQAApvLrMDJ//nylp6crODhYmZmZWrVqldkleY2VK1fq2muvVXJysiwWi95///1mzxuGoSeffFLJyckKCQnRJZdcom3btplTrMnmzp2riy66SBEREerevbumTJmib775ptk9tFdzCxYs0NChQ907Oo4ZM0Yff/yx+3na6/Tmzp0ri8WiWbNmua/RZs09+eSTslgszR6JiYnu52mvlg4fPqwf//jHio2NVWhoqIYPH67c3Fz38+eyzfw2jCxatEizZs3S448/rry8PI0bN06TJ09Wfn6+2aV5haqqKg0bNkzz5s1r9flnnnlGzz77rObNm6eNGzcqMTFRV1xxhSorK89zpeZbsWKFZsyYofXr1ysnJ0f19fWaNGmSqqqq3PfQXs2lpKToN7/5jTZt2qRNmzZp4sSJuu6669zf2Givtm3cuFEvvviihg4d2uw6bdbS4MGDVVBQ4H5s3brV/Rzt1dyxY8c0duxYBQYG6uOPP9b27dv1+9//vtlRLOe0zU5/Tm/XNXLkSGP69OnNrg0cOND4+c9/blJF3kuSsWTJEvfHLpfLSExMNH7zm9+4r9XU1BhRUVHGCy+8YEKF3qW4uNiQZKxYscIwDNqrvbp162a8/PLLtNdpVFZWGv369TNycnKMCRMmGD/96U8Nw+DfWGueeOIJY9iwYa0+R3u19OijjxoXX3xxm8+f6zbzy56R2tpa5ebmatKkSc2uT5o0SWvXrjWpKt+xb98+FRYWNms/u92uCRMm0H6SysvLJUkxMTGSaK8zcTqdevvtt1VVVaUxY8bQXqcxY8YMXX311br88subXafNWrdr1y4lJycrPT1dP/zhD7V3715JtFdrPvjgA2VlZenGG29U9+7ddeGFF+qll15yP3+u28wvw0hJSYmcTmeLw/0SEhJaHOqHlpraiPZryTAMzZ49WxdffLEyMjIk0V5t2bp1q8LDw2W32zV9+nQtWbJEF1xwAe3VhrfffltffPGF5s6d2+I52qylUaNG6Y033tCyZcv00ksvqbCwUNnZ2SotLaW9WrF3714tWLBA/fr107JlyzR9+nTNnDlTb7zxhqRz/2/MdtafwYdZLJZmHxuG0eIa2kb7tfTAAw9oy5YtWr16dYvnaK/mBgwYoM2bN+v48eN67733dNttt2nFihXu52mvbx08eFA//elPtXz5cgUHB7d5H232rcmTJ7v/PGTIEI0ZM0Z9+vTRX//6V40ePVoS7XUql8ulrKws/frXv5YkXXjhhdq2bZsWLFjQ7Ly5c9VmftkzEhcXJ6vV2iLNFRcXt0h9aKlpRjrt19yDDz6oDz74QJ999plSUlLc12mv1gUFBalv377KysrS3LlzNWzYMD333HO0Vytyc3NVXFyszMxM2Ww22Ww2rVixQn/6059ks9nc7UKbtS0sLExDhgzRrl27+DfWiqSkJF1wwQXNrg0aNMi9qONct5lfhpGgoCBlZmYqJyen2fWcnBxlZ2ebVJXvSE9PV2JiYrP2q62t1YoVK/yy/QzD0AMPPKDFixfr008/VXp6erPnaa/2MQxDDoeD9mrFZZddpq1bt2rz5s3uR1ZWlm655RZt3rxZvXv3ps3OwOFwaMeOHUpKSuLfWCvGjh3bYkuCnTt3Ki0tTdJ5+D521lNgfdTbb79tBAYGGq+88oqxfft2Y9asWUZYWJixf/9+s0vzCpWVlUZeXp6Rl5dnSDKeffZZIy8vzzhw4IBhGIbxm9/8xoiKijIWL15sbN261fjRj35kJCUlGRUVFSZXfv7dd999RlRUlPH5558bBQUF7kd1dbX7HtqruTlz5hgrV6409u3bZ2zZssV47LHHjICAAGP58uWGYdBe7XHqahrDoM2+6+GHHzY+//xzY+/evcb69euNa665xoiIiHB/j6e9mtuwYYNhs9mMX/3qV8auXbuMhQsXGqGhocbf//539z3nss38NowYhmE8//zzRlpamhEUFGSMGDHCvRQThvHZZ58Zklo8brvtNsMwGpZ5PfHEE0ZiYqJht9uN8ePHG1u3bjW3aJO01k6SjNdee819D+3V3J133un+2ouPjzcuu+wydxAxDNqrPb4bRmiz5qZOnWokJSUZgYGBRnJysvH973/f2LZtm/t52qulDz/80MjIyDDsdrsxcOBA48UXX2z2/LlsM4thGMbZ968AAAB0jF/OGQEAAN6DMAIAAExFGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApvr/AbQUXNejqbriAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(60),R2[:,0])\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66f73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_torch_cu121",
   "language": "python",
   "name": "py311_torch_cu121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

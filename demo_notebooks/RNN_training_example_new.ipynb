{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser(\"/network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim\"))\n",
    "from climsim_utils.data_utils import *\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9a8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = '../grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = '../preprocessing/normalizations/'\n",
    "grid_path = 'grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = 'preprocessing/normalizations/'\n",
    "\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean_v4_pervar.nc').astype(np.float32)\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max_v4_pervar.nc').astype(np.float32)\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min_v4_pervar.nc').astype(np.float32)\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc').astype(np.float32)\n",
    "\n",
    "ml_backend = 'pytorch'\n",
    "input_abbrev = 'mlexpand'\n",
    "output_abbrev = 'mlo'\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale,\n",
    "                  ml_backend = ml_backend,\n",
    "                  normalize = True,\n",
    "                  input_abbrev = input_abbrev,\n",
    "                  output_abbrev = output_abbrev,\n",
    "                  save_h5=True,\n",
    "                  save_npy=False,\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "# set data path\n",
    "data.data_path = '/ocean/projects/atm200007p/jlin96/neurips_proj/e3sm_train/'\n",
    "data.data_path = \"/network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim_data/ClimSim_low-res-expanded/train/\"\n",
    "\n",
    "data_save_path =  \"/network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim_data/ClimSim_low-res-expanded/train/preprocessed/\"\n",
    "# set inputs and outputs to V1 subset\n",
    "#data.set_to_v1_vars()\n",
    "#data.set_to_v2_vars()\n",
    "#data.set_to_v4_vars()\n",
    "data.set_to_v4_rnn_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b14bf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/network/group/aopp/predict/HMC009_UKKONEN_CLIMSIM/ClimSim_data/ClimSim_low-res-expanded/train/preprocessed/\"\n",
    "tr_data_fname = \"data_v4_rnn_year2.h5\"\n",
    "tr_data_path = data_dir + tr_data_fname\n",
    "\n",
    "tr_data_fname = [\"data_v4_rnn_year2.h5\", \n",
    "                \"data_v4_rnn_year3.h5\"]\n",
    "tr_data_path = [data_dir + b for b in tr_data_fname]\n",
    "\n",
    "use_val = False \n",
    "#use_val = True\n",
    "\n",
    "if use_val:\n",
    "    val_data_fname = \"data_v4_rnn_year5.h5\"\n",
    "    val_data_path = data_dir + val_data_fname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22eedd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6) (8,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vars_1D_outp = []; vars_2D_outp = []\n",
    "\n",
    "all_vars = list(data.output_scale.keys())\n",
    "for var in all_vars:\n",
    "    if 'lev' in data.output_scale[var].dims:\n",
    "        vars_2D_outp.append(var)\n",
    "    else:\n",
    "        vars_1D_outp.append(var)  \n",
    "        \n",
    "yscale_lev = data.output_scale[vars_2D_outp].to_dataarray(dim='features', name='outputs_lev').transpose().values\n",
    "yscale_sca = data.output_scale[vars_1D_outp].to_dataarray(dim='features', name='outputs_sca').transpose().values\n",
    "\n",
    "print(yscale_lev.shape, yscale_sca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "038d6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['input_lev', 'input_sca', 'output_lev', 'output_sca']>\n",
      "<KeysViewHDF5 ['varnames']>\n",
      "ns 10091136 nlev 60 nx 15\n",
      "['state_t' 'state_rh' 'state_q0002' 'state_q0003' 'state_u' 'state_v'\n",
      " 'state_t_dyn' 'state_q0_dyn' 'state_u_dyn' 'tm_state_t_dyn'\n",
      " 'tm_state_q0_dyn' 'tm_state_u_dyn' 'pbuf_ozone' 'pbuf_CH4' 'pbuf_N2O']\n",
      "nx_sfc: 24\n",
      "['state_ps' 'pbuf_SOLIN' 'pbuf_LHFLX' 'pbuf_SHFLX' 'pbuf_TAUX' 'pbuf_TAUY'\n",
      " 'pbuf_COSZRS' 'cam_in_ALDIF' 'cam_in_ALDIR' 'cam_in_ASDIF' 'cam_in_ASDIR'\n",
      " 'cam_in_LWUP' 'cam_in_ICEFRAC' 'cam_in_LANDFRAC' 'cam_in_OCNFRAC'\n",
      " 'cam_in_SNOWHICE' 'cam_in_SNOWHLAND' 'tm_state_ps' 'tm_pbuf_SOLIN'\n",
      " 'tm_pbuf_LHFLX' 'tm_pbuf_SHFLX' 'tm_pbuf_COSZRS' 'clat' 'slat']\n",
      "ny: 6\n",
      "['ptend_t' 'ptend_q0001' 'ptend_q0002' 'ptend_q0003' 'ptend_u' 'ptend_v']\n",
      "ny_sfc: 8\n",
      "['cam_out_NETSW' 'cam_out_FLWDS' 'cam_out_PRECSC' 'cam_out_PRECC'\n",
      " 'cam_out_SOLS' 'cam_out_SOLL' 'cam_out_SOLSD' 'cam_out_SOLLD']\n"
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "testfile = tr_data_path[0] if type(tr_data_path)==list else tr_data_path\n",
    "hf = h5py.File(testfile, 'r')\n",
    "print(hf.keys())\n",
    "# <KeysViewHDF5 ['input_lev', 'input_sca', 'output_lev', 'output_sca']>\n",
    "#print(hf.attrs.keys())\n",
    "print(hf['input_lev'].attrs.keys())\n",
    "ns, nlev, nx = hf['input_lev'].shape\n",
    "print(\"ns\", ns, \"nlev\", nlev,  \"nx\", nx)\n",
    "print(hf['input_lev'].attrs.get('varnames'))\n",
    "# future training data should have a \"varnames\" attribute for each dataset type \n",
    "\n",
    "#2D Input variables: ['state_t', 'state_q0001', 'state_q0002', 'state_q0003', 'state_u', 'state_v', \n",
    "# 'pbuf_ozone', 'pbuf_CH4', 'pbuf_N2O']\n",
    "# NEW::\n",
    "#['state_t' 'state_rh' 'state_q0002' 'state_q0003' 'state_u' 'state_v'\n",
    "# 'state_t_dyn' 'state_q0_dyn' 'state_u_dyn' 'tm_state_t_dyn'\n",
    "# 'tm_state_q0_dyn' 'tm_state_u_dyn' 'pbuf_ozone' 'pbuf_CH4' 'pbuf_N2O']\n",
    "# We need pressure!\n",
    "\n",
    "#1D (scalar) Input variables: ['state_ps', 'pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX', 'pbuf_TAUX', \n",
    "# 'pbuf_TAUY', 'pbuf_COSZRS', 'cam_in_ALDIF', 'cam_in_ALDIR', 'cam_in_ASDIF', 'cam_in_ASDIR', \n",
    "# 'cam_in_LWUP', 'cam_in_ICEFRAC', 'cam_in_LANDFRAC', 'cam_in_OCNFRAC', 'cam_in_SNOWHICE', \n",
    "# 'cam_in_SNOWHLAND', 'lat', 'lon']\n",
    "ns, nx_sfc = hf['input_sca'].shape\n",
    "print(\"nx_sfc:\", nx_sfc)\n",
    "print(hf['input_sca'].attrs.get('varnames'))\n",
    "\n",
    "#2D Output variables: ['ptend_t', 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', 'ptend_v']\n",
    "ns, nlev, ny = hf['output_lev'].shape\n",
    "print(\"ny:\", ny)\n",
    "print(hf['output_lev'].attrs.get('varnames'))\n",
    "\n",
    "\n",
    "#1D (scalar) Output variables: ['cam_out_NETSW', 'cam_out_FLWDS', 'cam_out_PRECSC', \n",
    "#'cam_out_PRECC', 'cam_out_SOLS', 'cam_out_SOLL', 'cam_out_SOLSD', 'cam_out_SOLLD']\n",
    "(ns, ny_sfc) = hf['output_sca'].shape\n",
    "print(\"ny_sfc:\", ny_sfc)\n",
    "print(hf['output_sca'].attrs.get('varnames'))\n",
    "\n",
    "hf.close()\n",
    "\n",
    "\n",
    "#state_q0001 lev, ncol kg/kg Specific humidity\n",
    "#state_q0002 lev, ncol kg/kg Cloud liquid mixing ratio\n",
    "#state_q0003 lev, ncol kg/kg Cloud ice mixing ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec536b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parameter as Parameter\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, RNN_type='LSTM', \n",
    "                 nx = 9, nx_sfc=17, \n",
    "                 ny = 8, ny_sfc=8, \n",
    "                 nneur=(64,64), \n",
    "                 outputs_one_longer=False, # if True, inputs are a sequence\n",
    "                 # of N and outputs a sequence of N+1 (e.g. predicting fluxes)\n",
    "                 concat=False, out_scale=None, out_sfc_scale = None):\n",
    "        # Simple bidirectional RNN (Either LSTM or GRU) for predicting column \n",
    "        # outputs shaped either (B, L, Ny) or (B, L+1, Ny) from column inputs\n",
    "        # (B, L, Nx) and optionally surface inputs (B, Nx_sfc) \n",
    "        # If surface inputs exist, they are used to initialize first (upward) RNN \n",
    "        # Assumes top-of-atmosphere is first in memory i.e. at index 0 \n",
    "        # if it's not the flip operations need to be moved!\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny \n",
    "        self.nx_sfc = nx_sfc \n",
    "        self.ny_sfc = ny_sfc\n",
    "        self.nneur = nneur \n",
    "        self.outputs_one_longer=outputs_one_longer\n",
    "        if len(nneur) < 1 or len(nneur) > 3:\n",
    "            sys.exit(\"Number of RNN layers and length of nneur should be 2 or 3\")\n",
    "\n",
    "        self.RNN_type=RNN_type\n",
    "        if self.RNN_type=='LSTM':\n",
    "            RNN_model = nn.LSTM\n",
    "        elif self.RNN_type=='GRU':\n",
    "            RNN_model = nn.GRU\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "                    \n",
    "        self.concat=concat\n",
    "        \n",
    "        if out_scale is not None:\n",
    "            cuda = torch.cuda.is_available() \n",
    "            device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "            self.yscale_lev = torch.from_numpy(out_scale).to(device)\n",
    "            self.yscale_sca = torch.from_numpy(out_sfc_scale).to(device)\n",
    "\n",
    "        if self.nx_sfc > 0:\n",
    "            self.mlp_surface1  = nn.Linear(nx_sfc, self.nneur[0])\n",
    "            if self.RNN_type==\"LSTM\":\n",
    "                self.mlp_surface2  = nn.Linear(nx_sfc, self.nneur[0])\n",
    "\n",
    "        self.rnn1      = RNN_model(nx,            self.nneur[0], batch_first=True) # (input_size, hidden_size, num_layers=1\n",
    "        self.rnn2      = RNN_model(self.nneur[0], self.nneur[1], batch_first=True)\n",
    "        if len(self.nneur)==3:\n",
    "            self.rnn3      = RNN_model(self.nneur[1], self.nneur[2], batch_first=True)\n",
    "\n",
    "        # The final hidden variable is either the output from the last RNN, or\n",
    "        # the  concatenated outputs from all RNNs\n",
    "        if concat:\n",
    "            nh_rnn = sum(nneur)\n",
    "        else:\n",
    "            nh_rnn = nneur[-1]\n",
    "\n",
    "        self.mlp_output = nn.Linear(nh_rnn, self.ny)\n",
    "        if self.ny_sfc>0:\n",
    "            self.mlp_surface_output = nn.Linear(nneur[-1], self.ny_sfc)\n",
    "            \n",
    "    def postprocessing(self, out, out_sfc):\n",
    "        out_denorm = out / self.yscale_lev\n",
    "        out_sfc_denorm  = out_sfc / self.yscale_sca\n",
    "        return out_denorm, out_sfc_denorm\n",
    "    \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "            \n",
    "        # batch_size = inputs_main.shape[0]\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "        sfc1 = self.mlp_surface1(inputs_sfc)\n",
    "        sfc1 = nn.Tanh()(sfc1)\n",
    "\n",
    "        if self.RNN_type==\"LSTM\":\n",
    "            sfc2 = self.mlp_surface2(inputs_sfc)\n",
    "            sfc2 = nn.Tanh()(sfc2)\n",
    "            hidden = (sfc1.view(1,-1,self.nneur[0]), sfc2.view(1,-1,self.nneur[0])) # (h0, c0)\n",
    "        else:\n",
    "            hidden = (sfc1.view(1,-1,self.nneur[0]))\n",
    "\n",
    "        # print(f'Using state1 {hidden}')\n",
    "        # TOA is first in memory, so we need to flip the axis\n",
    "        inputs_main = torch.flip(inputs_main, [1])\n",
    "      \n",
    "        out, hidden = self.rnn1(inputs_main, hidden)\n",
    "        \n",
    "        if self.outputs_one_longer:\n",
    "            out = torch.cat((sfc1, out),axis=1)\n",
    "\n",
    "        out = torch.flip(out, [1]) # the surface was processed first, but for\n",
    "        # the second RNN (and the final output) we want TOA first\n",
    "        \n",
    "        out2, hidden2 = self.rnn2(out) \n",
    "        \n",
    "        (last_h, last_c) = hidden2\n",
    "\n",
    "        if len(self.nneur)==3:\n",
    "            rnn3_input = torch.flip(out2, [1])\n",
    "            \n",
    "            out3, hidden3 = self.rnn3(rnn3_input) \n",
    "            \n",
    "            out3 = torch.flip(out3, [1])\n",
    "            \n",
    "            if self.concat:\n",
    "                rnnout = torch.cat((out3, out2, out),axis=2)\n",
    "            else:\n",
    "                rnnout = out3\n",
    "        else:\n",
    "            if self.concat:\n",
    "                rnnout = torch.cat((out2, out),axis=2)\n",
    "            else:\n",
    "                rnnout = out2\n",
    "        \n",
    "        out = self.mlp_output(rnnout)\n",
    "\n",
    "        if self.ny_sfc>0:\n",
    "            #print(\"shape last_c\", last_c.shape)\n",
    "            # use cell state or hidden state?\n",
    "            out_sfc = self.mlp_surface_output(last_h.squeeze())\n",
    "            return out, out_sfc\n",
    "        else:\n",
    "            return out \n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f3cf6fc-d804-41a1-9c67-0d11b411651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class RNN_autoreg(nn.Module):\n",
    "    def __init__(self, nlay=60, nx = 4, nx_sfc=3, ny = 4, ny_sfc=3, nneur=(64,64), \n",
    "                cell_type=\"LSTM\",\n",
    "                memory=\"None\", # \"None\", \"Hidden\", or \"Output\",\n",
    "                concat=False,\n",
    "                use_initial_mlp=False, ensemble_size=1,\n",
    "                random_init_cx=False,\n",
    "                use_intermediate_mlp=True,\n",
    "                add_pres=False,\n",
    "                third_rnn=False,\n",
    "                add_stochastic_layer=False,\n",
    "                coeff_stochastic = 0.0,\n",
    "                dtype=torch.float32,\n",
    "                out_scale=None, out_sfc_scale=None):\n",
    "        super(RNN_autoreg, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny \n",
    "        self.nlay = nlay \n",
    "        self.nx_sfc = nx_sfc \n",
    "        self.ny_sfc = ny_sfc\n",
    "        self.nneur = nneur \n",
    "        self.use_initial_mlp=use_initial_mlp\n",
    "        self.add_pres = add_pres\n",
    "        if self.add_pres:\n",
    "            self.preslay = LayerPressure()\n",
    "            nx = nx +1\n",
    "        self.nh_rnn1 = self.nneur[0]\n",
    "        self.nx_rnn2 = self.nneur[0]\n",
    "        self.nh_rnn2 = self.nneur[1]\n",
    "        self.concat = concat\n",
    "        self.model_type=cell_type\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.random_init_cx= random_init_cx\n",
    "        self.third_rnn=third_rnn\n",
    "        self.add_stochastic_layer = add_stochastic_layer\n",
    "        self.coeff_stochastic = coeff_stochastic\n",
    "        self.dtype=dtype\n",
    "        if self.ensemble_size>1 and not add_stochastic_layer:\n",
    "        # In this case the stochasticity comes purely from random initialization\n",
    "        # of hidden states (probably not enough)\n",
    "            self.random_init_cx=True\n",
    "        self.use_intermediate_mlp=use_intermediate_mlp\n",
    "        self.share_weights = False\n",
    "        if self.share_weights:\n",
    "            self.use_intermediate_mlp=False\n",
    "        if self.use_initial_mlp:\n",
    "            self.nx_rnn1 = self.nneur[0]\n",
    "        else:\n",
    "            self.share_weights=False\n",
    "            self.nx_rnn1 = nx\n",
    "        self.memory = memory\n",
    "        if out_scale is not None:\n",
    "            cuda = torch.cuda.is_available() \n",
    "            device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "            self.yscale_lev = torch.from_numpy(out_scale).to(device)\n",
    "            self.yscale_sca = torch.from_numpy(out_sfc_scale).to(device)\n",
    "        if memory == 'None':\n",
    "            raise NotImplementedError()\n",
    "        elif memory == 'Output':\n",
    "            print(\"Building RNN that feeds its output t0,z0 to its inputs at t1,z0\")\n",
    "            self.rnn1_mem = None \n",
    "            self.nh_mem = self.ny\n",
    "            self.nx_rnn1 = self.nx_rnn1 + self.nh_mem\n",
    "            self.nh_rnn1 = self.nneur[0]\n",
    "        elif memory == 'Hidden':\n",
    "            self.nh_mem = self.nneur[1]\n",
    "            print(\"Building RNN that feeds its hidden memory at t0,z0 to its inputs at t1,z0\")\n",
    "            print(\"Initial mlp: {}, intermediate mlp: {}\".format(self.use_initial_mlp, self.use_intermediate_mlp))\n",
    " \n",
    "            self.rnn1_mem = None \n",
    "            self.nx_rnn1 = self.nx_rnn1 + self.nh_mem\n",
    "            self.nh_rnn1 = self.nneur[0]\n",
    "        elif memory == 'CustomLSTM':\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            sys.exit(\"memory argument must equal one of : 'None', 'Hidden', or 'CustomLSTM'\")\n",
    "            \n",
    "        print(\"nx rnn1\", self.nx_rnn1, \"nh rnn1\", self.nh_rnn1)\n",
    "        print(\"nx rnn2\", self.nx_rnn2, \"nh rnn2\", self.nh_rnn2)  \n",
    "        print(\"Cell type:\", cell_type)\n",
    "        if self.use_initial_mlp:\n",
    "            self.mlp_initial = nn.Linear(nx, self.nneur[0])\n",
    "\n",
    "        self.mlp_surface1  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "        if not self.random_init_cx:\n",
    "            self.mlp_surface2  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "\n",
    "        if self.model_type==\"LSTM\":\n",
    "            # self.rnn1      = nn.LSTMCell(self.nx_rnn1, self.nh_rnn1)  # (input_size, hidden_size)\n",
    "            # self.rnn2      = nn.LSTMCell(self.nx_rnn2, self.nh_rnn2)\n",
    "            self.rnn1      = nn.LSTM(self.nx_rnn1, self.nh_rnn1,  batch_first=True)  # (input_size, hidden_size)\n",
    "            if not self.share_weights:\n",
    "                self.rnn2      = nn.LSTM(self.nx_rnn2, self.nh_rnn2,  batch_first=True)\n",
    "                if self.third_rnn:\n",
    "                    self.rnn3      = nn.LSTM(self.nh_rnn2, self.nneur[2],  batch_first=True)\n",
    "                \n",
    "        elif self.model_type==\"GRU\":\n",
    "              self.rnn1      = nn.GRU(self.nx_rnn1, self.nh_rnn1,  batch_first=True)   # (input_size, hidden_size)\n",
    "              self.rnn2      = nn.GRU(self.nx_rnn2, self.nh_rnn2,  batch_first=True) \n",
    "        elif self.model_type==\"RNN\":\n",
    "              self.rnn1      = nn.RNN(self.nx_rnn1, self.nh_rnn1,  batch_first=True)   # (input_size, hidden_size)\n",
    "              self.rnn2      = nn.RNN(self.nx_rnn2, self.nh_rnn2,  batch_first=True) \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        if self.add_stochastic_layer:\n",
    "            from models_torch_kernels import StochasticGRUCell, MyStochasticGRUCell\n",
    "            nx_srnn = self.nh_rnn2\n",
    "            # self.rnn_stochastic = StochasticGRUCell(nx_srnn, self.nh_rnn2)  # (input_size, hidden_size)\n",
    "            self.rnn_stochastic = MyStochasticGRUCell(nx_srnn, self.nh_rnn2)  # (input_size, hidden_size)\n",
    "        if concat:\n",
    "            nh_rnn = self.nh_rnn2+self.nh_rnn1\n",
    "        else:\n",
    "            nh_rnn = self.nh_rnn2\n",
    "\n",
    "        if self.use_intermediate_mlp: \n",
    "            self.mlp_latent = nn.Linear(nh_rnn, self.nh_rnn1)\n",
    "            self.mlp_output = nn.Linear(self.nh_rnn1, self.ny)\n",
    "        else:\n",
    "            self.mlp_output = nn.Linear(nh_rnn, self.ny)\n",
    "            \n",
    "        if self.ny_sfc>0:\n",
    "            self.mlp_surface_output = nn.Linear(nneur[-1], self.ny_sfc)\n",
    "            \n",
    "    def postprocessing(self, out, out_sfc):\n",
    "        out_denorm = out / self.yscale_lev\n",
    "        out_sfc_denorm  = out_sfc / self.yscale_sca\n",
    "        return out_denorm, out_sfc_denorm\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.rnn1_mem = None\n",
    "\n",
    "    def detach_states(self):\n",
    "        self.rnn1_mem = self.rnn1_mem.detach()\n",
    "   \n",
    "    def get_states(self):\n",
    "        return self.rnn1_mem.detach()\n",
    "\n",
    "    def set_states(self, states):\n",
    "        self.rnn1_mem = states \n",
    "        \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "        if self.ensemble_size>0:\n",
    "            inputs_main = inputs_main.unsqueeze(0)\n",
    "            inputs_sfc = inputs_sfc.unsqueeze(0)\n",
    "            inputs_main = torch.repeat_interleave(inputs_main,repeats=self.ensemble_size,dim=0)\n",
    "            inputs_sfc = torch.repeat_interleave(inputs_sfc,repeats=self.ensemble_size,dim=0)\n",
    "            inputs_main = inputs_main.flatten(0,1)\n",
    "            inputs_sfc = inputs_sfc.flatten(0,1)\n",
    "                    \n",
    "        batch_size = inputs_main.shape[0]\n",
    "        if self.add_pres:\n",
    "            sp = inputs_sfc[:,-1]\n",
    "            pres  = self.preslay(sp)\n",
    "            inputs_main = torch.cat((inputs_main,torch.unsqueeze(pres,2)),dim=2)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "     \n",
    "        if self.rnn1_mem is None: \n",
    "            # self.rnn1_mem = torch.randn(batch_size, self.nlay, self.nh_mem,device=device)\n",
    "            self.rnn1_mem = torch.randn((batch_size, self.nlay, self.nh_mem),dtype=self.dtype,device=device)\n",
    "\n",
    "        hx = self.mlp_surface1(inputs_sfc)\n",
    "        hx = nn.Tanh()(hx)\n",
    "\n",
    "        # TOA is first in memory, so to start at the surface we need to go backwards\n",
    "        inputs_main = torch.flip(inputs_main, [1])\n",
    "\n",
    "        # The input (a vertical sequence) is concatenated with the\n",
    "        # output of the RNN from the previous time step \n",
    "        if self.model_type in [\"LSTM\"]:\n",
    "            if self.random_init_cx:\n",
    "                # cx = torch.randn(batch_size, self.nh_rnn1,device=device)\n",
    "                cx = torch.randn((batch_size, self.nh_rnn1),dtype=self.dtype,device=device)\n",
    "            else:\n",
    "                cx = self.mlp_surface2(inputs_sfc)\n",
    "                cx = nn.Tanh()(cx)\n",
    "            hidden = (torch.unsqueeze(hx,0), torch.unsqueeze(cx,0))\n",
    "        else:\n",
    "            hidden = (torch.unsqueeze(hx,0))\n",
    "\n",
    "        if self.use_initial_mlp:\n",
    "            rnn1_input = self.mlp_initial(inputs_main)\n",
    "            rnn1_input = nn.Tanh()(rnn1_input)\n",
    "        else:\n",
    "            rnn1_input = inputs_main \n",
    "            \n",
    "        rnn1_input = torch.cat((rnn1_input,self.rnn1_mem), axis=2)\n",
    "\n",
    "        rnn1out, states = self.rnn1(rnn1_input, hidden)\n",
    "\n",
    "        rnn1out = torch.flip(rnn1out, [1])\n",
    "\n",
    "        hx2 = torch.randn((batch_size, self.nh_rnn2),dtype=self.dtype,device=device)  # (batch, hidden_size)\n",
    "        if self.model_type in [\"LSTM\"]:\n",
    "            cx2 = torch.randn((batch_size, self.nh_rnn2),dtype=self.dtype,device=device)\n",
    "            hidden2 = (torch.unsqueeze(hx2,0), torch.unsqueeze(cx2,0))\n",
    "        else:\n",
    "            hidden2 = (torch.unsqueeze(hx2,0))\n",
    "\n",
    "        input_rnn2 = rnn1out\n",
    "            \n",
    "        if self.share_weights:\n",
    "            rnn2out, states = self.rnn1(input_rnn2, hidden2)\n",
    "        else:\n",
    "            rnn2out, states = self.rnn2(input_rnn2, hidden2)\n",
    "\n",
    "        (last_h, last_c) = states\n",
    "\n",
    "        if self.third_rnn:\n",
    "            rnn3in = torch.flip(rnn2out, [1])\n",
    "            rnn2out, states = self.rnn3(rnn3in)\n",
    "\n",
    "        if self.concat:\n",
    "            rnn2out = torch.cat((rnn2out,rnn1out),axis=2)\n",
    "        \n",
    "        if self.use_intermediate_mlp: \n",
    "            rnn2out = self.mlp_latent(rnn2out)\n",
    "          \n",
    "        if self.memory==\"Hidden\": \n",
    "            if not self.third_rnn: self.rnn1_mem = torch.flip(rnn2out, [1])\n",
    "            \n",
    "        # Add a stochastic perturbation\n",
    "        # Convective memory is still based on the deterministic model,\n",
    "        # and does not include the stochastic perturbation\n",
    "        # concat and use_intermediate_mlp should be set to false\n",
    "        if self.add_stochastic_layer:\n",
    "            # srnn_input = torch.transpose(rnn2out,0,1)\n",
    "            srnn_input = torch.transpose(self.rnn1_mem,0,1)\n",
    "            # transpose is needed because this layer assumes seq. dim first\n",
    "            z = self.rnn_stochastic(srnn_input)\n",
    "            z = torch.flip(z, [0])\n",
    "\n",
    "            z = torch.transpose(z,0,1)\n",
    "            # z = torch.flip(z, [1])\n",
    "            # rnn2out = z\n",
    "            # z is a perburbation added to the hidden state\n",
    "            # rnn2out = rnn2out + 0.01*z \n",
    "            rnn2out = rnn2out + self.coeff_stochastic*z \n",
    "\n",
    "        out = self.mlp_output(rnn2out)\n",
    "        if self.memory==\"Output\":\n",
    "            if not self.third_rnn: self.rnn1_mem = torch.flip(out, [1])\n",
    "\n",
    "        if self.ny_sfc>0:\n",
    "            #print(\"shape last_c\", last_c.shape)\n",
    "            # use cell state or hidden state?\n",
    "            out_sfc = self.mlp_surface_output(last_h.squeeze())\n",
    "            return out, out_sfc\n",
    "        else:\n",
    "            return out \n",
    "        \n",
    "\n",
    "class SpaceStateModel(nn.Module):\n",
    "    def __init__(self, nlay=30, nx = 4, nx_sfc=3, ny = 4, nneur=(64,64),model_type='LRU', \n",
    "                 use_initial_mlp=True, add_pres=False,  concat=False,\n",
    "                 third_rnn=False, init_with_state_and_sfc=False, memory=None,\n",
    "                 dtype=torch.float32,\n",
    "                 device=None):\n",
    "        super(SpaceStateModel, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny \n",
    "        self.nlay = nlay \n",
    "        self.nx_sfc = nx_sfc \n",
    "        self.nneur = nneur \n",
    "        self.add_pres = add_pres\n",
    "        if self.add_pres:\n",
    "            self.preslay = LayerPressure()\n",
    "            nx = nx +1\n",
    "        self.nh_rnn1 = self.nneur[0]\n",
    "        self.nh_rnn2 = self.nneur[1]\n",
    "        self.ny_rnn1 = self.nh_rnn1\n",
    "        self.use_initial_mlp=use_initial_mlp\n",
    "        self.init_with_state_and_sfc = init_with_state_and_sfc\n",
    "        self.model_type=model_type\n",
    "        self.dtype=dtype\n",
    "        self.device=device\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using model type {}\".format(model_type))\n",
    "        if self.model_type in [\"Mamba\",\"GSS\"]:#,\"QRNN\"]:  #Mamba doesnt support providing the state\n",
    "            print(\"WARNING: this SSM type doesn't support providing the state, so the surface variables are instead added to each point in the sequence\")\n",
    "            \n",
    "            # concatenate the vertical (sequence) inputs with tiled scalars\n",
    "            self.init_with_state_and_sfc=False\n",
    "            self.use_initial_mlp=True\n",
    "            nx = nx + nx_sfc\n",
    "        if self.use_initial_mlp: \n",
    "            self.nh_mlp1 = self.nh_rnn1\n",
    "            self.nx_rnn1 = self.nh_rnn1\n",
    "        else:\n",
    "            self.nx_rnn1 = nx\n",
    "        self.use_intermediate_mlp = False\n",
    "        if memory == 'None':\n",
    "            print(\"Building non-autoregressive SSM, but may be stateful\")\n",
    "            self.autoregressive = False\n",
    "        elif memory == 'Hidden':\n",
    "            print(\"Building autoregressive SSM that feeds a hidden memory at t0,z0 to SSM1 at t1,z0\")\n",
    "            self.autoregressive = True\n",
    "            self.rnn1_mem = None\n",
    "            # self.rnn2_mem = None\n",
    "            # self.use_intermediate_mlp = True\n",
    "            \n",
    "        # if self.use_intermediate_mlp:\n",
    "        #     self.nh_latent = self.nh_rnn2\n",
    "        self.nh_latent = self.nh_rnn2\n",
    "        glu_expand_factor = 2\n",
    "        # if model_type in ['S5','GSS','QRNN','Mamba','GateLoop','SRU','LRU']:\n",
    "        if model_type in ['S5','GSS','QRNN','Mamba','GateLoop','SRU','SRU_2D','LRU']:\n",
    "            self.use_initial_mlp = True\n",
    "            self.use_intermediate_mlp = True; self.nh_latent = self.nh_rnn2\n",
    "            self.nx_rnn1 = self.nh_rnn1 \n",
    "            self.nx_rnn2 = self.nh_rnn2\n",
    "            if self.autoregressive and model_type != 'SRU_2D':\n",
    "                # This is a bit tricky with these SSMs as they expect \n",
    "                # num_inputs = num_hidden. Therefore we need MLPs that halve \n",
    "                # the size of both inputs and the final hidden variable, e.g.\n",
    "                # SSM = 128 neurons, Xlay = 64 neurons, Xhidden = 64 neurons,\n",
    "                # X = concat(Xlay,Xhidden)\n",
    "                # Similarly, to feed the latent variable to RNN2, we would need an MLP before it\n",
    "                # that halves the size of  the output from RNN1\n",
    "                #    mlp1      concat(mlp1,hfin=64)     RNN1(128)     MLP2     concat(mlp2,hfin=64)      RNN2(128)     mlpfin\n",
    "                # nx ----> 64 ---------------------> 128 -------> 128 ---> 64 ---------------------> 128 --------> 128 ----->  64\n",
    "                #   OR let second SSMs hidden size be bigger\n",
    "                #    mlp1      concat(mlp1,hfin=64)     RNN1(128)      concat(hfin=64)           RNN2(192)     mlpfin\n",
    "                # nx ----> 64 ---------------------> 128 -------> 128 ---------------------> 192 --------> 192 ----->  64  \n",
    "                # or Dont bother making RNN2 autoregressive?\n",
    "                # mlp1        concat(hfin=128)          RNN1(256)     GLU      RNN2(128)    GLU      RNN3\n",
    "                # nx ---> 128 ---------------------> 256 -------> 256 ---> 128 -------> 128 ---> 128 --->  128\n",
    "\n",
    "                self.nh_latent = self.nh_rnn1//2\n",
    "                self.ny_rnn1 = self.nh_rnn1//2\n",
    "                self.nh_mlp1  = self.nh_rnn1//2 \n",
    "                self.nh_mlp2  = self.nh_rnn1\n",
    "                # self.nh_rnn2 = self.nh_rnn1  + self.nh_latent\n",
    "                # self.nx_rnn2 = self.nh_rnn2\n",
    "                # self.nh_rnn2 = self.nh_rnn1 \n",
    "                self.nx_rnn2 = self.nh_rnn2\n",
    "                glu_expand_factor = 1\n",
    "        else:\n",
    "            self.nx_rnn2 = self.nh_rnn1\n",
    "            if self.autoregressive:\n",
    "                self.nx_rnn1 = self.nx_rnn1 + self.nh_latent \n",
    "                # self.nx_rnn2 = self.nh_rnn1 + self.nh_latent \n",
    "\n",
    "        if self.use_initial_mlp:\n",
    "            self.mlp_initial = nn.Linear(nx, self.nh_mlp1 )\n",
    "        self.third_rnn=third_rnn\n",
    "\n",
    "        print(\"nx rnn1\", self.nx_rnn1, \"nh rnn1\", self.nh_rnn1, \"ny rnn1\", self.ny_rnn1)\n",
    "        print(\"nx rnn2\", self.nx_rnn2, \"nh rnn2\", self.nh_rnn2)  \n",
    "        if self.autoregressive: \n",
    "            print(\"nh_memory\", self.nh_latent)\n",
    "        self.concat = concat\n",
    "\n",
    "        if self.init_with_state_and_sfc:\n",
    "            self.state1 = None\n",
    "            # self.state2 = None \n",
    "            self.mlp_surface1  = nn.Linear(nx_sfc + self.nh_rnn1, self.nh_rnn1)\n",
    "        else:\n",
    "            self.mlp_surface1  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "\n",
    "        if model_type == 'LRU':\n",
    "            from models_torch_kernels_LRU import LRU\n",
    "            self.rnn1= LRU(in_features=self.nx_rnn1,out_features=self.nh_rnn1,state_features=self.nh_rnn1)\n",
    "            self.rnn2= LRU(in_features=self.nx_rnn2,out_features=self.nh_rnn2,state_features=self.nh_rnn2)\n",
    "        elif model_type == \"MinGRU\":\n",
    "            # MinGRU = models_torch_kernels.MinGRU\n",
    "            from models_torch_kernels import MinGRU as MinGRU\n",
    "            self.rnn1= MinGRU(self.nx_rnn1,self.nh_rnn1)\n",
    "            self.rnn2= MinGRU(self.nx_rnn2,self.nh_rnn2)\n",
    "            # from models_torch_kernels import minGRU as MinGRU\n",
    "            # self.rnn1= MinGRU(self.nh_rnn1)\n",
    "            # self.rnn2= MinGRU(self.nh_rnn2)\n",
    "            if self.third_rnn:\n",
    "                self.rnn3= MinGRU(self.nx_rnn2,self.nh_rnn2)\n",
    "        elif model_type == 'S5':\n",
    "            from s5 import S5\n",
    "            self.liquid=False\n",
    "            self.pres_step_scale = False\n",
    "            # S5 outputs shape equals inputs?\n",
    "            # self.rnn1= S5(self.nx_rnn1,self.nh_rnn1,liquid=self.liquid)\n",
    "            # self.rnn2= S5(self.nx_rnn2,self.nh_rnn2,liquid=self.liquid)   \n",
    "            # self.rnn1= S5(self.nh_rnn1,liquid=self.liquid)\n",
    "            # self.rnn2= S5(self.nh_rnn2,liquid=self.liquid)   \n",
    "            self.rnn1= S5(self.nx_rnn1,liquid=self.liquid)\n",
    "            self.rnn2= S5(self.nx_rnn2,liquid=self.liquid)  \n",
    "        elif model_type == 'Mamba':\n",
    "            from mamba_ssm import Mamba\n",
    "            self.rnn1 = Mamba(\n",
    "                # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "                d_model=self.nx_rnn1, # Model dimension d_model\n",
    "                d_state=16,  # SSM state expansion factor\n",
    "                d_conv=4,    # Local convolution width\n",
    "                expand=2,    # Block expansion factor\n",
    "                )\n",
    "            self.rnn2 = Mamba(\n",
    "                # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "                d_model=self.nx_rnn2, # Model dimension d_model\n",
    "                d_state=16,  # SSM state expansion factor\n",
    "                d_conv=4,    # Local convolution width\n",
    "                expand=2,    # Block expansion factor\n",
    "                )\n",
    "        elif model_type == 'GSS':\n",
    "            from gated_state_spaces_pytorch import GSS\n",
    "            self.rnn1= GSS(dim=self.nx_rnn1,dss_kernel_N=self.nh_rnn1,dss_kernel_H=self.nh_rnn1)\n",
    "            self.rnn2= GSS(dim=self.nx_rnn2,dss_kernel_N=self.nh_rnn2,dss_kernel_H=self.nh_rnn2)\n",
    "        elif model_type == 'QRNN':\n",
    "            from models_torch_kernels import QRNNLayer, QRNNLayer_noncausal\n",
    "            kernelsize = 3\n",
    "            self.rnn1= QRNNLayer_noncausal(self.nx_rnn1,self.nh_rnn1, kernel_size=3, pad =(0,1))\n",
    "            self.rnn2= QRNNLayer_noncausal(self.nx_rnn2,self.nh_rnn2, kernel_size=3) \n",
    "            \n",
    "            if self.init_with_state_and_sfc:\n",
    "                self.mlp_surface2  = nn.Linear(nx_sfc + self.nh_rnn1, self.nh_rnn1)\n",
    "            else:\n",
    "                self.mlp_surface2  = nn.Linear(nx_sfc, self.nh_rnn1)  \n",
    "        elif model_type == 'SRU':\n",
    "            from models_torch_kernels import SRU\n",
    "            self.rnn1= SRU(self.nx_rnn1,self.nh_rnn1)\n",
    "            self.rnn2= SRU(self.nx_rnn2,self.nh_rnn2)  \n",
    "            # from sru import SRU\n",
    "            # self.rnn1= SRU(self.nx_rnn1,self.nh_rnn1,num_layers=1)\n",
    "            # self.rnn2= SRU(self.nx_rnn2,self.nh_rnn2,num_layers=1)\n",
    "        elif model_type == \"SRU_2D\":\n",
    "            from models_torch_kernels import SRU, SRU2\n",
    "            self.rnn1= SRU2(self.nx_rnn1,self.nh_rnn1)\n",
    "            self.rnn2= SRU(self.nx_rnn2,self.nh_rnn2)  \n",
    "        elif model_type == 'GateLoop':\n",
    "            from gateloop_transformer import SimpleGateLoopLayer\n",
    "            self.rnn1= SimpleGateLoopLayer(self.nx_rnn1) #, use_jax_associative_scan=True)\n",
    "            self.rnn2= SimpleGateLoopLayer(self.nx_rnn2) #, use_jax_associative_scan=True)\n",
    "            if self.third_rnn:\n",
    "                self.rnn3= SimpleGateLoopLayer(self.nneur[2])\n",
    "                \n",
    "            if self.init_with_state_and_sfc:\n",
    "                self.mlp_surface2  = nn.Linear(nx_sfc + self.nh_rnn1, self.nh_rnn1)\n",
    "            else:\n",
    "                self.mlp_surface2  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "        else:\n",
    "            \n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # if model_type in ['LRU','S5','GSS','QRNN','Mamba']:\n",
    "        # if model_type in ['LRU','SRU','S5','GSS','QRNN','GateLoop','MinGRU']:\n",
    "        # if model_type in ['LRU','SRU','S5','GSS','QRNN','GateLoop','MinGRU','Mamba']:\n",
    "        if model_type in ['LRU','SRU','SRU_2D','S5','QRNN','GateLoop','MinGRU',]:\n",
    "\n",
    "            self.use_glu_layers = True \n",
    "        else:\n",
    "            self.use_glu_layers = False\n",
    "            \n",
    "        self.reduce_dim_with_mlp=False\n",
    "        if self.use_glu_layers:\n",
    "            print(\"Using GLU layers in between for nonlinearity, expand factor (1 means halving output) for GLU1 is \", glu_expand_factor)\n",
    "            glu_layernorm=True\n",
    "            # glu_layernorm=False\n",
    "\n",
    "            # self.rnn1= LRU(in_features=nx_rnn1,out_features=self.nneur[0],state_features=self.nneur[0])\n",
    "            # self.mlp  = nn.Linear(self.nneur[0], self.nneur[0])\n",
    "            # self.SeqLayer1 = SequenceLayer(nlay=self.nlay,nneur=self.nneur[0],layernorm=True)\n",
    "            self.SeqLayer1 = GLU(nlay=self.nlay,nneur=self.nh_rnn1,layernorm=glu_layernorm, expand_factor=glu_expand_factor)\n",
    "    \n",
    "            # self.rnn2= LRU(in_features=nx_rnn2,out_features=self.nneur[1],state_features=self.nneur[0])\n",
    "            # self.mlp2  = nn.Linear(self.nneur[1], self.nneur[1])\n",
    "            # self.SeqLayer2 = SequenceLayer(nlay=self.nlay,nneur=self.nneur[1],layernorm=True)\n",
    "            self.SeqLayer2 = GLU(nlay=self.nlay,nneur=self.nh_rnn2,layernorm=glu_layernorm)\n",
    "\n",
    "            # self.SeqLayer15 = GLU(nlay=self.nlay,nneur=self.nneur[0],layernorm=glu_layernorm)\n",
    "        else:\n",
    "            if self.autoregressive and model_type in ['S5','GSS','QRNN','Mamba','GateLoop','SRU','SRU_2D''LRU']:\n",
    "                   self.reduce_dim_with_mlp=True\n",
    "                   self.reduce_dim_mlp = nn.Linear(self.nh_rnn1, self.nx_rnn2)\n",
    "                   \n",
    "        if self.third_rnn:\n",
    "            # self.rnn3= LRU(in_features=nx_rnn2,out_features=self.nneur[1],state_features=self.nneur[0])\n",
    "            # self.rnn3= LRU(in_features=self.nneur[1],out_features=self.nneur[1],state_features=self.nneur[1])\n",
    "            # self.mlp3  = nn.Linear(self.nneur[1], self.nneur[1])\n",
    "            # self.SeqLayer3 = SequenceLayer(nlay=self.nlay,nneur=self.nneur[1],layernorm=True)\n",
    "            if self.use_glu_layers: self.SeqLayer3 = GLU(nlay=self.nlay,nneur=self.nneur[2],layernorm=glu_layernorm)\n",
    "\n",
    "        if self.concat:\n",
    "            nx_last = self.ny_rnn1 + self.nh_rnn2\n",
    "            if self.third_rnn:  \n",
    "                nx_last += self.nh_rnn2\n",
    "        else:\n",
    "            nx_last = self.nh_rnn2\n",
    "            \n",
    "        if self.use_intermediate_mlp:\n",
    "            self.mlp_latent = nn.Linear(nx_last, self.nh_latent)\n",
    "            nx_last = self.nh_latent\n",
    "\n",
    "        self.mlp_output        = nn.Linear(nx_last, self.ny)\n",
    "\n",
    "    def reset_states(self):\n",
    "        if self.init_with_state_and_sfc:\n",
    "            self.state1 = None\n",
    "        if self.autoregressive:\n",
    "            self.rnn1_mem = None\n",
    "            # self.rnn2_mem = None\n",
    "\n",
    "    def detach_states(self):\n",
    "        if self.init_with_state_and_sfc:\n",
    "            self.state1 = self.state1.detach()\n",
    "        if self.autoregressive:\n",
    "            self.rnn1_mem = self.rnn1_mem.detach()\n",
    "            # self.rnn2_mem = self.rnn2_mem.detach()\n",
    "        \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "\n",
    "        # print(\"Shape inputs main\", inputs_main.shape)\n",
    "        batch_size = inputs_main.shape[0]\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device=self.device\n",
    "        if self.autoregressive:\n",
    "            if self.rnn1_mem is None: \n",
    "                self.rnn1_mem = torch.randn((batch_size, self.nlay, self.nh_latent),dtype=self.dtype,device=device)\n",
    "    \n",
    "            # if self.rnn2_mem is None: \n",
    "            #     self.rnn2_mem = torch.randn(batch_size, self.nlay, self.nh_latent ,device=device)\n",
    "        if self.add_pres:\n",
    "            sp = inputs_sfc[:,-1]\n",
    "            pres  = self.preslay(sp)\n",
    "            inputs_main = torch.cat((inputs_main,torch.unsqueeze(pres,2)),dim=2)\n",
    "        \n",
    "        if self.model_type==\"S5\" and self.pres_step_scale:\n",
    "                refp = inputs_main[:,:,-1].clone().detach()\n",
    "        \n",
    "        inputs_main = torch.flip(inputs_main, [1])\n",
    "        \n",
    "        if self.model_type==\"S5\" and self.pres_step_scale:\n",
    "                refp_rev = inputs_main[:,:,-1].clone().detach()\n",
    "\n",
    "        if self.model_type in [\"Mamba\",\"GSS\"]:#,,\"QRNN\"]:\n",
    "            #Mamba doesnt support providing the state, so as a hack we instead\n",
    "            # concatenate the vertical (sequence) inputs with tiled scalars\n",
    "            inputs_sfc_tiled = torch.tile(torch.unsqueeze(inputs_sfc,1),(1,self.nlay,1))\n",
    "            inputs_main = torch.cat((inputs_main,inputs_sfc_tiled), dim=2)\n",
    "        else:\n",
    "            # Initial states can be a vector of shape (state_features,) or a matrix of shape (batch_size, state_features)\n",
    "            if self.init_with_state_and_sfc:\n",
    "                if self.state1==None:\n",
    "                    self.state1 =  torch.randn((batch_size, self.nh_rnn1),dtype=self.dtype,device=device)\n",
    "    \n",
    "                init_inputs = torch.cat((inputs_sfc,self.state1), dim=1)\n",
    "            else:\n",
    "                init_inputs = inputs_sfc\n",
    "                \n",
    "            init_states = self.mlp_surface1(init_inputs)\n",
    "            # init_states = nn.Softsign()(init_states)\n",
    "            init_states = nn.Tanh()(init_states)\n",
    "            # print(\"shape init states\" , init_states.shape)\n",
    "        # print(\"shape inp main inp\", inputs_main.shape)\n",
    "\n",
    "        if self.use_initial_mlp:\n",
    "            rnn1_input = self.mlp_initial(inputs_main)\n",
    "            rnn1_input = nn.Tanh()(rnn1_input)\n",
    "        else:\n",
    "            rnn1_input = inputs_main\n",
    "        # print(\"shape rnn1 inp\", rnn1_input.shape)\n",
    "\n",
    "        if self.autoregressive and self.model_type != \"SRU_2D\":\n",
    "            rnn1_input = torch.cat((rnn1_input,self.rnn1_mem), axis=2)\n",
    "\n",
    "        # print(\"shape rnn1 inp\", rnn1_input.shape)\n",
    "            \n",
    "        # B_tilde, C_tilde = self.rnn1.seq.get_BC_tilde()\n",
    "        # print(\"shape B tild\", B_tilde.shape, \"C tild\", C_tilde.shape)\n",
    "        # shape B tild torch.Size([96, 96]) C tild torch.Size([96, 96])\n",
    "        \n",
    "        # if self.layernorm:\n",
    "        #     rnn1_input = self.norm(rnn1_input)\n",
    "        if self.model_type==\"S5\":\n",
    "            if self.pres_step_scale:\n",
    "                out = self.rnn1(rnn1_input,state=init_states, step_scale=refp_rev)\n",
    "                # pres_rev = torch.flip(pres, [1])        \n",
    "                # out = self.rnn1(rnn1_input,state=init_states, step_scale=pres_rev)\n",
    "            else:\n",
    "                out = self.rnn1(rnn1_input,state=init_states)\n",
    "                # out,h = self.rnn1(rnn1_input,state=init_states,return_state=True)\n",
    "                # print(\"OUT\", out[0,-1,0], \"STATE\", h[0,0])\n",
    "                # OUT tensor(-0.2206, device='cuda:0', grad_fn=<SelectBackward0>) STATE tensor(0.0096+0.0025j\n",
    "        elif self.model_type in [\"Mamba\",\"GSS\"]:#,\"QRNN\"]:\n",
    "            out = self.rnn1(rnn1_input) \n",
    "        elif self.model_type == \"QRNN\":\n",
    "            init_states2 = self.mlp_surface2(init_inputs)\n",
    "            init_states2 = nn.Tanh()(init_states2)\n",
    "            init_states = (init_states, init_states2)\n",
    "            out = self.rnn1(rnn1_input,init_states) \n",
    "\n",
    "        elif self.model_type==\"SRU\":\n",
    "            # print(\"init shape\", init_states.shape)\n",
    "            # init_states = init_states.view((1,batch_size,-1)) \n",
    "            out,c = self.rnn1(rnn1_input,init_states) \n",
    "        elif self.model_type==\"SRU_2D\":\n",
    "            # print(\"init shape\", init_states.shape)\n",
    "            out,c = self.rnn1(rnn1_input,init_states, self.rnn1_mem) \n",
    "        elif self.model_type==\"MinGRU\":\n",
    "            init_states = init_states.view((batch_size,1, -1)) \n",
    "            out = self.rnn1(rnn1_input,init_states)      \n",
    "        elif self.model_type==\"GateLoop\":\n",
    "            init_states2 = self.mlp_surface2(init_inputs)\n",
    "            init_states2 = nn.Tanh()(init_states2)\n",
    "            cache = [init_states.view(batch_size*self.nh_rnn1,1), init_states2.view(batch_size*self.nh_rnn1,1)]\n",
    "            out = self.rnn1(rnn1_input,cache=cache)          \n",
    "            # out = self.rnn1(rnn1_input)          \n",
    "\n",
    "        else:\n",
    "            # out = self.rnn1(rnn1_input,state=init_states) \n",
    "            out = self.rnn1(rnn1_input,init_states) \n",
    "        # out = self.rnn1(rnn1_input)\n",
    "        \n",
    "        if self.init_with_state_and_sfc: \n",
    "            self.state1 = out[:,-1,:].clone()\n",
    "            \n",
    "        init_states2 = None \n",
    "            \n",
    "        out = torch.flip(out, [1])\n",
    "        # print(\"shape rnn1 out\", out.shape)\n",
    "\n",
    "        if self.use_glu_layers: \n",
    "            out = self.SeqLayer1(out)\n",
    "            # out = self.SeqLayer15(out)\n",
    "        elif self.reduce_dim_with_mlp:\n",
    "            out = self.reduce_dim_mlp(out)\n",
    "        # if self.autoregressive:\n",
    "        #     rnn2_input = torch.cat((out,self.rnn2_mem), axis=2)\n",
    "        # else:\n",
    "        #     rnn2_input = out \n",
    "        rnn2_input = out  \n",
    "        # print(\"shape rnn2 inp\", rnn2_input.shape)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "        if self.model_type==\"S5\":\n",
    "            if self.pres_step_scale:\n",
    "                out2 = self.rnn2(rnn2_input, state=init_states2,step_scale=refp)\n",
    "            else:\n",
    "                out2 = self.rnn2(rnn2_input, state=init_states2)\n",
    "        elif self.model_type in [\"Mamba\",\"GSS\"]:#,,\"QRNN\"]:\n",
    "            out2 = self.rnn2(rnn2_input)    \n",
    "        elif self.model_type in [\"SRU\",\"SRU_2D\"]:\n",
    "            out2,c = self.rnn2(rnn2_input,init_states2) \n",
    "        elif self.model_type==\"MinGRU\":\n",
    "            init_states2 = torch.randn((batch_size, 1, self.nh_rnn2),dtype=self.dtype,device=device)\n",
    "            out2 = self.rnn2(rnn2_input,init_states2)   \n",
    "        elif self.model_type==\"GateLoop\":\n",
    "            out2 = self.rnn2(rnn2_input)      \n",
    "        else:\n",
    "            out2 = self.rnn2(rnn2_input,init_states2)\n",
    "        # if self.layernorm:\n",
    "        #     out2 = self.normrnn2(out2)                                                                                                                                                                                                                                                                                                                            \n",
    "        # self.state2 = self._detach_state(state2)\n",
    "        #self.state = state\n",
    "        \n",
    "        if self.use_glu_layers: \n",
    "            out2 = self.SeqLayer2(out2)\n",
    "\n",
    "\n",
    "        if self.concat:\n",
    "            outs  = torch.cat((out,out2),axis=2)\n",
    "        else:\n",
    "            outs = out2\n",
    "            \n",
    "        if self.use_intermediate_mlp:\n",
    "            outs = self.mlp_latent(outs)\n",
    "            \n",
    "        if self.autoregressive:\n",
    "            \n",
    "            # self.rnn2_mem = torch.clone(hidden_fin)\n",
    "            # if self.third_rnn: \n",
    "            #     self.rnn1_mem = torch.clone(hidden_fin, [1])\n",
    "            # else:\n",
    "            #     out3 = torch.flip(out3, [1])\n",
    "            self.rnn1_mem = torch.flip(outs, [1])\n",
    "        \n",
    "        outs = self.mlp_output(outs)\n",
    "        \n",
    "        if self.dtype==torch.bfloat16:\n",
    "            outs = outs.to(torch.float32)\n",
    "            \n",
    "        return outs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e39e2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_autoreg0(nn.Module):\n",
    "    def __init__(self, nlay=60, nx = 4, nx_sfc=3, ny = 4, ny_sfc=3, nneur=(64,64), \n",
    "                cell_type=\"LSTM\",\n",
    "                memory=\"None\", # \"None\", \"Hidden\", or \"Output\",\n",
    "                concat=False,\n",
    "                use_initial_mlp=False, ensemble_size=1,\n",
    "                random_init_cx=False,\n",
    "                use_intermediate_mlp=True,\n",
    "                add_pres=False,\n",
    "                third_rnn=False,\n",
    "                add_stochastic_layer=False,\n",
    "                coeff_stochastic = 0.0,\n",
    "                dtype=torch.float32):\n",
    "        super(RNN_autoreg0, self).__init__()\n",
    "        self.nx = nx\n",
    "        self.ny = ny \n",
    "        self.nlay = nlay \n",
    "        self.nx_sfc = nx_sfc \n",
    "        self.ny_sfc = ny_sfc\n",
    "        self.nneur = nneur \n",
    "        self.use_initial_mlp=use_initial_mlp\n",
    "        self.add_pres = add_pres\n",
    "        if self.add_pres:\n",
    "            self.preslay = LayerPressure()\n",
    "            nx = nx +1\n",
    "        self.nh_rnn1 = self.nneur[0]\n",
    "        self.nx_rnn2 = self.nneur[0]\n",
    "        self.nh_rnn2 = self.nneur[1]\n",
    "        self.concat = concat\n",
    "        self.model_type=cell_type\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.random_init_cx= random_init_cx\n",
    "        self.third_rnn=third_rnn\n",
    "        self.add_stochastic_layer = add_stochastic_layer\n",
    "        self.coeff_stochastic = coeff_stochastic\n",
    "        self.dtype=dtype\n",
    "        if self.ensemble_size>1 and not add_stochastic_layer:\n",
    "        # In this case the stochasticity comes purely from random initialization\n",
    "        # of hidden states (probably not enough)\n",
    "            self.random_init_cx=True\n",
    "        self.use_intermediate_mlp=use_intermediate_mlp\n",
    "        self.share_weights = False\n",
    "        if self.share_weights:\n",
    "            self.use_intermediate_mlp=False\n",
    "        if self.use_initial_mlp:\n",
    "            self.nx_rnn1 = self.nneur[0]\n",
    "        else:\n",
    "            self.share_weights=False\n",
    "            self.nx_rnn1 = nx\n",
    "        self.memory = memory\n",
    "\n",
    "        if memory == 'None':\n",
    "            raise NotImplementedError()\n",
    "        elif memory == 'Output':\n",
    "            print(\"Building RNN that feeds its output t0,z0 to its inputs at t1,z0\")\n",
    "            self.rnn1_mem = None \n",
    "            self.nh_mem = self.ny\n",
    "            self.nx_rnn1 = self.nx_rnn1 + self.nh_mem\n",
    "            self.nh_rnn1 = self.nneur[0]\n",
    "        elif memory == 'Hidden':\n",
    "            self.nh_mem = self.nneur[1]\n",
    "            print(\"Building RNN that feeds its hidden memory at t0,z0 to its inputs at t1,z0\")\n",
    "            print(\"Initial mlp: {}, intermediate mlp: {}\".format(self.use_initial_mlp, self.use_intermediate_mlp))\n",
    " \n",
    "            self.rnn1_mem = None \n",
    "            self.nx_rnn1 = self.nx_rnn1 + self.nh_mem\n",
    "            self.nh_rnn1 = self.nneur[0]\n",
    "        elif memory == 'CustomLSTM':\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            sys.exit(\"memory argument must equal one of : 'None', 'Hidden', or 'CustomLSTM'\")\n",
    "            \n",
    "        print(\"nx rnn1\", self.nx_rnn1, \"nh rnn1\", self.nh_rnn1)\n",
    "        print(\"nx rnn2\", self.nx_rnn2, \"nh rnn2\", self.nh_rnn2)  \n",
    "        print(\"Cell type:\", cell_type)\n",
    "        if self.use_initial_mlp:\n",
    "            self.mlp_initial = nn.Linear(nx, self.nneur[0])\n",
    "\n",
    "        self.mlp_surface1  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "        if not self.random_init_cx:\n",
    "            self.mlp_surface2  = nn.Linear(nx_sfc, self.nh_rnn1)\n",
    "\n",
    "        if self.model_type==\"LSTM\":\n",
    "            # self.rnn1      = nn.LSTMCell(self.nx_rnn1, self.nh_rnn1)  # (input_size, hidden_size)\n",
    "            # self.rnn2      = nn.LSTMCell(self.nx_rnn2, self.nh_rnn2)\n",
    "            self.rnn1      = nn.LSTM(self.nx_rnn1, self.nh_rnn1,  batch_first=True)  # (input_size, hidden_size)\n",
    "            if not self.share_weights:\n",
    "                self.rnn2      = nn.LSTM(self.nx_rnn2, self.nh_rnn2,  batch_first=True)\n",
    "                if self.third_rnn:\n",
    "                    self.rnn3      = nn.LSTM(self.nh_rnn2, self.nneur[2],  batch_first=True)\n",
    "                \n",
    "        elif self.model_type==\"GRU\":\n",
    "              self.rnn1      = nn.GRU(self.nx_rnn1, self.nh_rnn1,  batch_first=True)   # (input_size, hidden_size)\n",
    "              self.rnn2      = nn.GRU(self.nx_rnn2, self.nh_rnn2,  batch_first=True) \n",
    "        elif self.model_type==\"RNN\":\n",
    "              self.rnn1      = nn.RNN(self.nx_rnn1, self.nh_rnn1,  batch_first=True)   # (input_size, hidden_size)\n",
    "              self.rnn2      = nn.RNN(self.nx_rnn2, self.nh_rnn2,  batch_first=True) \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        if self.add_stochastic_layer:\n",
    "            from models_torch_kernels import StochasticGRUCell, MyStochasticGRUCell\n",
    "            nx_srnn = self.nh_rnn2\n",
    "            # self.rnn_stochastic = StochasticGRUCell(nx_srnn, self.nh_rnn2)  # (input_size, hidden_size)\n",
    "            self.rnn_stochastic = MyStochasticGRUCell(nx_srnn, self.nh_rnn2)  # (input_size, hidden_size)\n",
    "        if concat:\n",
    "            nh_rnn = self.nh_rnn2+self.nh_rnn1\n",
    "        else:\n",
    "            nh_rnn = self.nh_rnn2\n",
    "\n",
    "        if self.use_intermediate_mlp: \n",
    "            self.mlp_latent = nn.Linear(nh_rnn, self.nh_rnn1)\n",
    "            self.mlp_output = nn.Linear(self.nh_rnn1, self.ny)\n",
    "        else:\n",
    "            self.mlp_output = nn.Linear(nh_rnn, self.ny)\n",
    "            \n",
    "        if self.ny_sfc>0:\n",
    "            self.mlp_surface_output = nn.Linear(nneur[-1], self.ny_sfc)\n",
    "            \n",
    "    def reset_states(self):\n",
    "        self.rnn1_mem = None\n",
    "\n",
    "    def detach_states(self):\n",
    "        self.rnn1_mem = self.rnn1_mem.detach()\n",
    "   \n",
    "    def get_states(self):\n",
    "        return self.rnn1_mem.detach()\n",
    "\n",
    "    def set_states(self, states):\n",
    "        self.rnn1_mem = states \n",
    "        \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "        if self.ensemble_size>0:\n",
    "            inputs_main = inputs_main.unsqueeze(0)\n",
    "            inputs_sfc = inputs_sfc.unsqueeze(0)\n",
    "            inputs_main = torch.repeat_interleave(inputs_main,repeats=self.ensemble_size,dim=0)\n",
    "            inputs_sfc = torch.repeat_interleave(inputs_sfc,repeats=self.ensemble_size,dim=0)\n",
    "            inputs_main = inputs_main.flatten(0,1)\n",
    "            inputs_sfc = inputs_sfc.flatten(0,1)\n",
    "                    \n",
    "        batch_size = inputs_main.shape[0]\n",
    "        if self.add_pres:\n",
    "            sp = inputs_sfc[:,-1]\n",
    "            pres  = self.preslay(sp)\n",
    "            inputs_main = torch.cat((inputs_main,torch.unsqueeze(pres,2)),dim=2)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "     \n",
    "        if self.rnn1_mem is None: \n",
    "            # self.rnn1_mem = torch.randn(batch_size, self.nlay, self.nh_mem,device=device)\n",
    "            self.rnn1_mem = torch.randn((batch_size, self.nlay, self.nh_mem),dtype=self.dtype,device=device)\n",
    "\n",
    "        hx = self.mlp_surface1(inputs_sfc)\n",
    "        hx = nn.Tanh()(hx)\n",
    "\n",
    "        # TOA is first in memory, so to start at the surface we need to go backwards\n",
    "        inputs_main = torch.flip(inputs_main, [1])\n",
    "\n",
    "        # The input (a vertical sequence) is concatenated with the\n",
    "        # output of the RNN from the previous time step \n",
    "        if self.model_type in [\"LSTM\"]:\n",
    "            if self.random_init_cx:\n",
    "                # cx = torch.randn(batch_size, self.nh_rnn1,device=device)\n",
    "                cx = torch.randn((batch_size, self.nh_rnn1),dtype=self.dtype,device=device)\n",
    "            else:\n",
    "                cx = self.mlp_surface2(inputs_sfc)\n",
    "                cx = nn.Tanh()(cx)\n",
    "            hidden = (torch.unsqueeze(hx,0), torch.unsqueeze(cx,0))\n",
    "        else:\n",
    "            hidden = (torch.unsqueeze(hx,0))\n",
    "\n",
    "        if self.use_initial_mlp:\n",
    "            rnn1_input = self.mlp_initial(inputs_main)\n",
    "            rnn1_input = nn.Tanh()(rnn1_input)\n",
    "        else:\n",
    "            rnn1_input = inputs_main \n",
    "            \n",
    "        rnn1_input = torch.cat((rnn1_input,self.rnn1_mem), axis=2)\n",
    "\n",
    "        rnn1out, states = self.rnn1(rnn1_input, hidden)\n",
    "\n",
    "        rnn1out = torch.flip(rnn1out, [1])\n",
    "\n",
    "        hx2 = torch.randn((batch_size, self.nh_rnn2),dtype=self.dtype,device=device)  # (batch, hidden_size)\n",
    "        if self.model_type in [\"LSTM\"]:\n",
    "            cx2 = torch.randn((batch_size, self.nh_rnn2),dtype=self.dtype,device=device)\n",
    "            hidden2 = (torch.unsqueeze(hx2,0), torch.unsqueeze(cx2,0))\n",
    "        else:\n",
    "            hidden2 = (torch.unsqueeze(hx2,0))\n",
    "\n",
    "        input_rnn2 = rnn1out\n",
    "            \n",
    "        if self.share_weights:\n",
    "            rnn2out, states = self.rnn1(input_rnn2, hidden2)\n",
    "        else:\n",
    "            rnn2out, states = self.rnn2(input_rnn2, hidden2)\n",
    "\n",
    "        (last_h, last_c) = states\n",
    "\n",
    "        if self.third_rnn:\n",
    "            rnn3in = torch.flip(rnn2out, [1])\n",
    "            rnn2out, states = self.rnn3(rnn3in)\n",
    "\n",
    "        if self.concat:\n",
    "            rnn2out = torch.cat((rnn2out,rnn1out),axis=2)\n",
    "        \n",
    "        if self.use_intermediate_mlp: \n",
    "            rnn2out = self.mlp_latent(rnn2out)\n",
    "          \n",
    "        if self.memory==\"Hidden\": \n",
    "            if not self.third_rnn: self.rnn1_mem = torch.flip(rnn2out, [1])\n",
    "            \n",
    "        # Add a stochastic perturbation\n",
    "        # Convective memory is still based on the deterministic model,\n",
    "        # and does not include the stochastic perturbation\n",
    "        # concat and use_intermediate_mlp should be set to false\n",
    "        if self.add_stochastic_layer:\n",
    "            # srnn_input = torch.transpose(rnn2out,0,1)\n",
    "            srnn_input = torch.transpose(self.rnn1_mem,0,1)\n",
    "            # transpose is needed because this layer assumes seq. dim first\n",
    "            z = self.rnn_stochastic(srnn_input)\n",
    "            z = torch.flip(z, [0])\n",
    "\n",
    "            z = torch.transpose(z,0,1)\n",
    "            # z = torch.flip(z, [1])\n",
    "            # rnn2out = z\n",
    "            # z is a perburbation added to the hidden state\n",
    "            # rnn2out = rnn2out + 0.01*z \n",
    "            rnn2out = rnn2out + self.coeff_stochastic*z \n",
    "\n",
    "        out = self.mlp_output(rnn2out)\n",
    "        if self.memory==\"Output\":\n",
    "            if not self.third_rnn: self.rnn1_mem = torch.flip(out, [1])\n",
    "\n",
    "        if self.ny_sfc>0:\n",
    "            #print(\"shape last_c\", last_c.shape)\n",
    "            # use cell state or hidden state?\n",
    "            out_sfc = self.mlp_surface_output(last_h.squeeze())\n",
    "            return out, out_sfc\n",
    "        else:\n",
    "            return out \n",
    "        \n",
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, *args, original_model, out_scale, out_sfc_scale, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.original_model = original_model\n",
    "        if out_scale is not None:\n",
    "            cuda = torch.cuda.is_available() \n",
    "            device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "            self.yscale_lev = torch.from_numpy(out_scale).to(device)\n",
    "            self.yscale_sca = torch.from_numpy(out_sfc_scale).to(device)\n",
    "        \n",
    "    def postprocessing(self, out, out_sfc):\n",
    "        out_denorm = out / self.yscale_lev\n",
    "        out_sfc_denorm  = out_sfc / self.yscale_sca\n",
    "        return out_denorm, out_sfc_denorm\n",
    "    \n",
    "    def combine_outputs(self, out, out_sfc):\n",
    "        out = out.transpose(1,2).flatten(1)\n",
    "        out = torch.cat((out,out_sfc),dim=1)\n",
    "        \n",
    "\n",
    "    def reset_states(self):\n",
    "        if self.init_with_state_and_sfc:\n",
    "            self.state1 = None\n",
    "        if self.autoregressive:\n",
    "            self.rnn1_mem = None\n",
    "            # self.rnn2_mem = None\n",
    "\n",
    "    def detach_states(self):\n",
    "        if self.init_with_state_and_sfc:\n",
    "            self.state1 = self.state1.detach()\n",
    "        if self.autoregressive:\n",
    "            self.rnn1_mem = self.rnn1_mem.detach()\n",
    "            # self.rnn2_mem = self.rnn2_mem.detach()   \n",
    "    \n",
    "    def forward(self, inputs_main, inputs_sfc):\n",
    "        #x = self.preprocessing(x)\n",
    " \n",
    "        out, out_sfc = self.original_model(inputs_main, inputs_sfc)\n",
    "        return out, out_sfc\n",
    "       # out, out_sfc = self.postprocessing(out, out_sfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69209a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up RNN model using nx=16, nx_sfc=24, ny=6, ny_sfc=8\n",
      "Building RNN that feeds its hidden memory at t0,z0 to its inputs at t1,z0\n",
      "Initial mlp: False, intermediate mlp: False\n",
      "nx rnn1 176 nh rnn1 160\n",
      "nx rnn2 160 nh rnn2 160\n",
      "Cell type: LSTM\n",
      "cuda\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "RNN_autoreg                              --\n",
      "Linear: 1-1                            4,000\n",
      "Linear: 1-2                            4,000\n",
      "LSTM: 1-3                              216,320\n",
      "LSTM: 1-4                              206,080\n",
      "Linear: 1-5                            966\n",
      "Linear: 1-6                            1,288\n",
      "=================================================================\n",
      "Total params: 432,654\n",
      "Trainable params: 432,654\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "nx = 15\n",
    "add_refpres = True\n",
    "if add_refpres:\n",
    "    nx = nx + 1\n",
    "\n",
    "print(\"Setting up RNN model using nx={}, nx_sfc={}, ny={}, ny_sfc={}\".format(nx,nx_sfc,ny,ny_sfc))\n",
    "autoregressive = True\n",
    "\n",
    "memory = \"Hidden\"\n",
    "concat = False \n",
    "use_initial_mlp = False \n",
    "use_intermediate_mlp = False\n",
    "add_pres = False \n",
    "ensemble_size = 1\n",
    "add_stochastic_layer = False\n",
    "\n",
    "nneur = (64,64)\n",
    "nneur = (128,128)\n",
    "nneur = (160, 160)\n",
    "\n",
    "use_wrapper = False \n",
    "\n",
    "if autoregressive:\n",
    "    if use_wrapper:\n",
    "        model0 = RNN_autoreg0(cell_type='LSTM', \n",
    "                    nlay = nlev, \n",
    "                    nx = nx, nx_sfc=nx_sfc, \n",
    "                    ny = ny, ny_sfc=ny_sfc, \n",
    "                    nneur=nneur,\n",
    "                    memory=memory,\n",
    "                    concat=concat,\n",
    "                    use_initial_mlp=use_initial_mlp,\n",
    "                    use_intermediate_mlp=use_intermediate_mlp,\n",
    "                    add_pres=add_pres,\n",
    "                    ensemble_size=ensemble_size,\n",
    "                    add_stochastic_layer=add_stochastic_layer)\n",
    "\n",
    "        model = WrappedModel(original_model = model0, out_scale = yscale_lev, out_sfc_scale = yscale_sca)\n",
    "    else:\n",
    "        model = RNN_autoreg(cell_type='LSTM', \n",
    "                nlay = nlev, \n",
    "                nx = nx, nx_sfc=nx_sfc, \n",
    "                ny = ny, ny_sfc=ny_sfc, \n",
    "                nneur=nneur,\n",
    "                memory=memory,\n",
    "                concat=concat,\n",
    "                use_initial_mlp=use_initial_mlp,\n",
    "                use_intermediate_mlp=use_intermediate_mlp,\n",
    "                add_pres=add_pres,\n",
    "                ensemble_size=ensemble_size,\n",
    "                add_stochastic_layer=add_stochastic_layer,      \n",
    "                out_scale = yscale_lev,\n",
    "                out_sfc_scale = yscale_sca)\n",
    "else:\n",
    "\n",
    "    model = MyRNN(RNN_type='LSTM', \n",
    "                 nx = nx, nx_sfc=nx_sfc, \n",
    "                 ny = ny, ny_sfc=ny_sfc, \n",
    "                 nneur=nneur,\n",
    "                 out_scale = yscale_lev,\n",
    "                 out_sfc_scale = yscale_sca)\n",
    "\n",
    "cuda = torch.cuda.is_available() \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "infostr = summary(model)\n",
    "num_params = infostr.total_params\n",
    "print(infostr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3ccea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    mp_autocast = True \n",
    "    print(torch.cuda.is_bf16_supported())\n",
    "    # if torch.cuda.is_bf16_supported(): \n",
    "    #     dtype=torch.bfloat16 \n",
    "    #     use_scaler = False\n",
    "    # else:\n",
    "    #     dtype=torch.float16\n",
    "    #     use_scaler = True \n",
    "    dtype=torch.float16\n",
    "    use_scaler = True \n",
    "else:\n",
    "    mp_autocast = False\n",
    "    use_scaler = False\n",
    "    \n",
    "    \n",
    "if use_scaler:\n",
    "    # scaler = torch.amp.GradScaler(autocast = True)\n",
    "    scaler = torch.amp.GradScaler(device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1261f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3840, 60, 6]) torch.Size([3840, 8])\n"
     ]
    }
   ],
   "source": [
    "test_with_real_data = False\n",
    "\n",
    "if test_with_real_data:\n",
    "    \n",
    "    hf = h5py.File(tr_data_path, 'r')\n",
    "    bsize = 384 \n",
    "    nb = 10\n",
    "    x_lay = hf['input_lev'][0:nb*bsize]\n",
    "    x_sfc = hf['input_sca'][0:nb*bsize]\n",
    "    y_lay = hf['output_lev'][0:nb*bsize]\n",
    "    y_sfc = hf['output_sca'][0:nb*bsize]\n",
    "    hf.close()\n",
    "\n",
    "    print(x_lay.shape, x_lay.min(), x_lay.max())\n",
    "    for i in range(nx-1):\n",
    "        print(\"x={}, min {} max {}\".format(i, x_lay[:,:,i].min(), x_lay[:,:,i].max()))\n",
    "    print(x_sfc.shape, x_sfc.min(), x_sfc.max())\n",
    "    print(y_lay.shape, y_lay.min(), y_lay.max())\n",
    "    print(y_sfc.shape, y_sfc.min(), y_sfc.max())\n",
    "\n",
    "    # Test the model with real data \n",
    "    x_lay0 = torch.from_numpy(x_lay).to(device)\n",
    "    x_sfc0 = torch.from_numpy(x_sfc).to(device)\n",
    "    y_lay0 = torch.from_numpy(y_lay).to(device)\n",
    "    y_sfc0 = torch.from_numpy(y_sfc).to(device)\n",
    "\n",
    "    print(x_lay0.shape, x_sfc0.shape)\n",
    "    print(y_lay0.shape, y_sfc0.shape)\n",
    "\n",
    "    #ns, nlay, nx = x_lay0.shape\n",
    "    #_, nx_sfc    = x_sfc0.shape\n",
    "    #_, _, ny     = y_lay0.shape\n",
    "    #_, ny_sfc    = y_sfc0.shape\n",
    "\n",
    "\n",
    "    out, out_sfc = model(x_lay0, x_sfc0)\n",
    "    print(out.shape, out_sfc.shape)\n",
    "else:\n",
    "    # Test the model with dummy data \n",
    "    bsize = 384 \n",
    "    nb = 10\n",
    "    x_lay = torch.zeros((nb*bsize, nlev, nx))\n",
    "    x_sfc = torch.zeros((nb*bsize, nx_sfc))\n",
    "\n",
    "    x_lay = x_lay.to(device)\n",
    "    x_sfc = x_sfc.to(device)\n",
    "\n",
    "    out, out_sfc = model(x_lay, x_sfc)\n",
    "    print(out.shape, out_sfc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bdd79968",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda: torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a0bcab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "1261764608 42298834944 0.029829772136052146\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "free, avail = torch.cuda.mem_get_info()\n",
    "print(free, avail, free/avail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a3d3419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3840, 60, 16])\n",
      "tensor(0., device='cuda:0')\n",
      "torch.Size([230400]) torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "print(x_lay.shape)\n",
    "rh = x_lay[:,:,1]\n",
    "print(rh.max())\n",
    "rhm = rh[rh<1.0]\n",
    "rhb = rh[rh>1.1]\n",
    "\n",
    "print(rhm.shape, rhb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e35bba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, out_sfc = model.postprocessing(out, out_sfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c610834",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ps = x_sfc0[:,0:1]\n",
    "\n",
    "if data.normalize:\n",
    "    state_ps = state_ps*(data.input_max['state_ps'].values - data.input_min['state_ps'].values) + data.input_mean['state_ps'].values\n",
    "\n",
    "print(\"shape ps\", state_ps.shape, \"min\", state_ps.min(), \"max\", state_ps.max())\n",
    "\n",
    "#pressure_grid_p1 = np.array(data.grid_info['P0']*data.grid_info['hyai'])[:,np.newaxis,np.newaxis]\n",
    "pressure_grid_p1 = torch.from_numpy(np.array(data.grid_info['P0']*data.grid_info['hyai'])[np.newaxis,:])\n",
    "#print(pressure_grid_p1.shape)\n",
    "pressure_grid_p2 = torch.from_numpy(data.grid_info['hybi'].values[np.newaxis, :]) * state_ps\n",
    "#print(pressure_grid_p2.shape)\n",
    "pressure_grid = pressure_grid_p1 + pressure_grid_p2\n",
    "#print(pressure_grid.shape, pressure_grid.min(), pressure_grid.max())\n",
    "dp     = pressure_grid[:,1:61] - pressure_grid[:,0:60]\n",
    "\n",
    "\n",
    "p1 = np.array(data.grid_info['P0']*data.grid_info['hyam'])[np.newaxis,:] \n",
    "p2 = data.grid_info['hybm'].values[np.newaxis, :] * data.grid_info['P0'].values\n",
    "\n",
    "pref = p1 + p2 \n",
    "print(pref/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "147abd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator_xy(torch.utils.data.Dataset):\n",
    "    def __init__(self, filepath, nloc=384, nlev=60, add_refpres=True, cuda=False):\n",
    "        self.filepath = filepath\n",
    "        # The file list will be divided into chunks (a list of lists)eg [[12,4,32],[1,9,3]..]\n",
    "        # where the length of each item is the chunk size; i.e. how many files \n",
    "        # are loaded at once (in this example 3 files)\n",
    "        # self.chunk_size = chunk_size # how many batches are loaded at once in getitem\n",
    "        self.nloc = nloc\n",
    "        self.nlev = nlev\n",
    "        if type(self.filepath)==list:\n",
    "            # In this case, each time a chunk is fetched, all the files are opened and the\n",
    "            # data is concatenated along the column dimension\n",
    "            self.num_files = len(self.filepath)\n",
    "            self.ncol = self.num_files * nloc\n",
    "            print(\"Number of files: {}\".format(self.num_files))\n",
    "            hdf = h5py.File(self.filepath[0], 'r')\n",
    "            self.ntimesteps = hdf['input_lev'].shape[0]//self.nloc\n",
    "            hdf.close()\n",
    "        else:\n",
    "            self.num_files = 1\n",
    "            self.ncol = nloc\n",
    "            hdf = h5py.File(self.filepath, 'r')\n",
    "            self.ntimesteps = hdf['input_lev'].shape[0]//self.nloc\n",
    "            hdf.close()\n",
    "        # self.nloc = int(os.path.basename(self.filepath).split('_')[-1])\n",
    "        # self.stateful = stateful\n",
    "        self.refpres = np.array([7.83478113e-02,1.41108318e-01,2.52923297e-01,4.49250635e-01,\n",
    "                    7.86346161e-01,1.34735576e+00,2.24477729e+00,3.61643148e+00,\n",
    "                    5.61583643e+00,8.40325322e+00,1.21444894e+01,1.70168280e+01,\n",
    "                    2.32107981e+01,3.09143463e+01,4.02775807e+01,5.13746323e+01,\n",
    "                    6.41892284e+01,7.86396576e+01,9.46300920e+01,1.12091274e+02,\n",
    "                    1.30977804e+02,1.51221318e+02,1.72673905e+02,1.95087710e+02,\n",
    "                    2.18155935e+02,2.41600379e+02,2.65258515e+02,2.89122322e+02,\n",
    "                    3.13312087e+02,3.38006999e+02,3.63373492e+02,3.89523338e+02,\n",
    "                    4.16507922e+02,4.44331412e+02,4.72957206e+02,5.02291917e+02,\n",
    "                    5.32152273e+02,5.62239392e+02,5.92149276e+02,6.21432841e+02,\n",
    "                    6.49689897e+02,6.76656485e+02,7.02242188e+02,7.26498589e+02,\n",
    "                    7.49537645e+02,7.71445217e+02,7.92234260e+02,8.11856675e+02,\n",
    "                    8.30259643e+02,8.47450653e+02,8.63535902e+02,8.78715875e+02,\n",
    "                    8.93246018e+02,9.07385213e+02,9.21354397e+02,9.35316717e+02,\n",
    "                    9.49378056e+02,9.63599599e+02,9.78013432e+02,9.92635544e+02],dtype=np.float32)\n",
    "        self.refpres_norm = (self.refpres-self.refpres.min())/(self.refpres.max()-self.refpres.min())*2 - 1\n",
    "\n",
    "        #if 'train' in self.filepath:\n",
    "        #    self.is_validation = False\n",
    "        #    print(\"Training dataset, path is: {}\".format(self.filepath))\n",
    "        #else:\n",
    "        #    self.is_validation = True\n",
    "        #    print(\"Validation dataset, path is: {}\".format(self.filepath))\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.add_refpres = add_refpres\n",
    "        # batch_idx_expanded =  [0,1,2,3...ntime*1024]\n",
    "\n",
    "        print(\"Number of locations {}; colums {}, time steps {}\".format(self.nloc,self.ncol, self.ntimesteps))\n",
    "        # indices_all = list(np.arange(self.ntimesteps*self.nloc))\n",
    "        # chunksize_tot = self.nloc*self.chunk_size\n",
    "        # indices_chunked = self.chunkize(indices_all,chunksize_tot,False) \n",
    "        # self.hdf = h5py.File(self.filepath, 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ntimesteps*self.nloc\n",
    "        #return self.ntimesteps*self.ncol\n",
    "\n",
    "    def __getitem__(self, batch_indices):\n",
    "        if self.num_files>1:\n",
    "            i = 0\n",
    "            for filepath in self.filepath:\n",
    "                hdf = h5py.File(filepath, 'r')\n",
    "                # hdf = self.hdf\n",
    "\n",
    "                x_lay_b0 = hdf['input_lev'][batch_indices,:]\n",
    "                x_sfc_b0 = hdf['input_sca'][batch_indices,:]\n",
    "                y_lay_b0 = hdf['output_lev'][batch_indices,:]\n",
    "                y_sfc_b0 = hdf['output_sca'][batch_indices,:]\n",
    "                \n",
    "                if i==0:\n",
    "                    x_lay_b = x_lay_b0\n",
    "                    x_sfc_b = x_sfc_b0\n",
    "                    y_lay_b = y_lay_b0\n",
    "                    y_sfc_b = y_sfc_b0\n",
    "                else:\n",
    "                    x_lay_b = np.concatenate((x_lay_b, x_lay_b0), axis=0)\n",
    "                    x_sfc_b = np.concatenate((x_sfc_b, x_sfc_b0), axis=0)\n",
    "                    y_lay_b = np.concatenate((y_lay_b, y_lay_b0), axis=0)\n",
    "                    y_sfc_b = np.concatenate((y_sfc_b, y_sfc_b0), axis=0)\n",
    "        else:       \n",
    "            hdf = h5py.File(self.filepath, 'r')\n",
    "            # hdf = self.hdf\n",
    "\n",
    "            x_lay_b = hdf['input_lev'][batch_indices,:]\n",
    "            x_sfc_b = hdf['input_sca'][batch_indices,:]\n",
    "            y_lay_b = hdf['output_lev'][batch_indices,:]\n",
    "            y_sfc_b = hdf['output_sca'][batch_indices,:]\n",
    "        \n",
    "        if self.add_refpres:\n",
    "            dim0,dim1,dim2 = x_lay_b.shape\n",
    "            # if self.norm==\"minmax\":\n",
    "            refpres_norm = self.refpres_norm.reshape((1,-1,1))\n",
    "            refpres_norm = np.repeat(refpres_norm, dim0,axis=0)\n",
    "            #self.x[:,:,nx-1] = refpres_norm\n",
    "            x_lay_b = np.concatenate((x_lay_b, refpres_norm),axis=2)\n",
    "            # self.x  = torch.cat((self.x,refpres_norm),dim=3)\n",
    "            del refpres_norm \n",
    "\n",
    "        hdf.close()\n",
    "\n",
    "        x_lay_b = torch.from_numpy(x_lay_b)\n",
    "        x_sfc_b = torch.from_numpy(x_sfc_b)\n",
    "        y_lay_b = torch.from_numpy(y_lay_b)\n",
    "        y_sfc_b = torch.from_numpy(y_sfc_b)\n",
    "\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # x_lay_b = x_lay_b.to(device)\n",
    "        # x_sfc_b = x_sfc_b.to(device)\n",
    "        # y_lay_b = y_lay_b.to(device)\n",
    "        # sp = sp.to(device)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return x_lay_b, x_sfc_b, y_lay_b, y_sfc_b\n",
    "\n",
    "def chunkize(filelist, chunk_size, shuffle_before_chunking=False, shuffle_after_chunking=True):\n",
    "    import random\n",
    "    # Takes a list, shuffles it (optional), and divides into chunks of length n\n",
    "    # (no concept of batches within this function, chunk size is given in number of samples)\n",
    "    def divide(filelist,chunk_size):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(filelist), chunk_size): \n",
    "            yield filelist[i:i + chunk_size]  \n",
    "    if shuffle_before_chunking:\n",
    "        random.shuffle(filelist)\n",
    "        # we need the indices to be sorted within a chunk because these indices\n",
    "        # are used to index into the first dimension of a H5 file\n",
    "        for i in range(filelist):\n",
    "            filelist[i] = sorted(filelist[i])\n",
    "            \n",
    "    mylist = list(divide(filelist,chunk_size))\n",
    "    if shuffle_after_chunking:\n",
    "        random.shuffle(mylist)  \n",
    "    return mylist\n",
    "\n",
    "\n",
    "class BatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, num_samples_per_chunk, num_samples, shuffle=False):\n",
    "        self.num_samples_per_chunk = num_samples_per_chunk\n",
    "        self.num_samples = num_samples\n",
    "        indices_all = list(range(self.num_samples))\n",
    "        print(\"Shuffling the indices: {}\".format(shuffle))\n",
    "        self.indices_chunked = chunkize(indices_all,self.num_samples_per_chunk,\n",
    "                                        shuffle_before_chunking=False,\n",
    "                                        shuffle_after_chunking=shuffle)\n",
    "        #print(\"indices chunked [0]\", self.indices_chunked[0])\n",
    "        # one item is one chunk, consisting of chunk_factor*batch_size samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices_chunked)\n",
    "        # for batch in self.indices_chunked:\n",
    "        #     yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1725a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data = False \n",
    "\n",
    "nloc = 384\n",
    "\n",
    "batch_size_tr = nloc\n",
    "\n",
    "# To improve IO, which is a bottleneck, increase the batch size by a factor of chunk_factor and \n",
    "# load this many batches at once. These chunks then need to be manually split into batches \n",
    "# within the data iteration loop   \n",
    "\n",
    "if type(tr_data_path)==list:  \n",
    "    num_files = len(tr_data_path)\n",
    "    batch_size_tr = num_files*batch_size_tr \n",
    "    chunk_size_tr = 720 // num_files\n",
    "    chunk_size_tr = 720 \n",
    "else:\n",
    "    # chunk size in number of batches\n",
    "    chunk_size_tr = 720 # 10 days (3 time steps in an hour, 72 in a day)\n",
    "    \n",
    "# chunk size in number of elements\n",
    "num_samples_per_chunk_tr = chunk_size_tr*batch_size_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4880073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_val:\n",
    "    batch_size_val = nloc\n",
    "\n",
    "    # To improve IO, which is a bottleneck, increase the batch size by a factor of chunk_factor and \n",
    "    # load this many batches at once. These chunks then need to be manually split into batches \n",
    "    # within the data iteration loop   \n",
    "\n",
    "    if type(val_data_path)==list:  \n",
    "        num_files = len(tr_data_path)\n",
    "        batch_size_val = num_files*batch_size_val\n",
    "        chunk_size_val = 720 // num_files\n",
    "    else:\n",
    "        # chunk size in number of batches\n",
    "        chunk_size_val = 720 # 10 days (3 time steps in an hour, 72 in a day)\n",
    "\n",
    "    # chunk size in number of elements\n",
    "    num_samples_per_chunk_val = chunk_size_val*batch_size_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "795048a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 2\n",
      "Number of locations 384; colums 768, time steps 26279\n",
      "Shuffling the indices: False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 2\n",
    "prefetch_factor = 1\n",
    "pin = False\n",
    "persistent=False\n",
    "\n",
    "\n",
    "train_data = generator_xy(tr_data_path, nloc=nloc, add_refpres=add_refpres)\n",
    "\n",
    "train_batch_sampler = BatchSampler(num_samples_per_chunk_tr, num_samples=train_data.__len__(), shuffle=shuffle_data)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, num_workers=num_workers, sampler=train_batch_sampler, \n",
    "                          batch_size=None,batch_sampler=None,prefetch_factor=prefetch_factor, \n",
    "                          pin_memory=pin, persistent_workers=persistent)\n",
    "\n",
    "if use_val:\n",
    "    \n",
    "    val_data = generator_xy(val_data_path, nloc=nloc, add_refpres=add_refpres)\n",
    "\n",
    "    val_batch_sampler = BatchSampler(num_samples_per_chunk_val, num_samples=val_data.__len__(), shuffle=shuffle_data)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_data, num_workers=num_workers,sampler=val_batch_sampler,\n",
    "                            batch_size=None,batch_sampler=None,prefetch_factor=prefetch_factor, \n",
    "                            pin_memory=pin, persistent_workers=persistent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6e76580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "78b54b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybi = torch.from_numpy(data.grid_info['hybi'].values).to(device)\n",
    "hyai = torch.from_numpy(data.grid_info['hyai'].values).to(device)\n",
    "sp_max = torch.from_numpy(data.input_max['state_ps'].values).to(device)\n",
    "sp_min = torch.from_numpy(data.input_min['state_ps'].values).to(device)\n",
    "sp_mean = torch.from_numpy(data.input_mean['state_ps'].values).to(device)\n",
    "\n",
    "def corrcoeff_pairs_batchfirst(A, B):\n",
    "    nb, nlev, nx = A.shape\n",
    "    # A and B are (N,M1,M2) vectors. \n",
    "    # Reshape to (N,M1*M2), then compute corrcoef for each (N,m),(N,m) pair\n",
    "    # reshape back at the end\n",
    "    A = A.reshape(nb,-1)\n",
    "    B = B.reshape(nb,-1)\n",
    "\n",
    "    A_mA = A - A.mean(0)[None,:]\n",
    "    B_mB = B - B.mean(0)[None,:] # (N, M)  \n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(0)\n",
    "    ssB = (B_mB**2).sum(0)\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    # dividend = np.dot(A_mA, B_mB.T) # (M,M)\n",
    "    dividend = np.sum(A_mA*B_mB, axis=0)  # (M)\n",
    "    \n",
    "    # divisor = np.sqrt(np.dot(ssA[:, None],ssB[None])) # (M, M)\n",
    "    divisor =  np.sqrt(ssA*ssB)\n",
    "    corrcoef = dividend / divisor\n",
    "    return corrcoef.reshape(nlev, nx)\n",
    "\n",
    "\n",
    "def my_mse(y_true_lay, y_true_sfc, y_pred_lay, y_pred_sfc):\n",
    "    mse1 = torch.mean(torch.square(y_pred_lay - y_true_lay))\n",
    "    mse2 = torch.mean(torch.square(y_pred_sfc - y_true_sfc))\n",
    "    return (mse1+mse2)/2\n",
    "\n",
    "def my_mse_flatten(y_true_lay, y_true_sfc, y_pred_lay, y_pred_sfc):\n",
    "\n",
    "    if len(y_true_lay.shape)==4: # autoregressive, time dimension included \n",
    "        y_pred_flat =  torch.cat(( y_pred_lay.flatten(start_dim=0,end_dim=1).flatten(start_dim=1) , y_pred_sfc.flatten(start_dim=0,end_dim=1) ),dim=1)\n",
    "        y_true_flat =  torch.cat(( y_true_lay.flatten(start_dim=0,end_dim=1).flatten(start_dim=1) , y_true_sfc.flatten(start_dim=0,end_dim=1) ),dim=1)\n",
    "    else:\n",
    "        y_pred_flat =  torch.cat((y_pred_lay.flatten(start_dim=1),y_pred_sfc),dim=1)\n",
    "        y_true_flat =  torch.cat((y_true_lay.flatten(start_dim=1),y_true_sfc),dim=1)\n",
    "\n",
    "    mse = torch.mean(torch.square(y_pred_flat - y_true_flat))\n",
    "    return mse\n",
    "\n",
    "def energy_metric(yto, ypo, sp, hyai,hybi):\n",
    "    \n",
    "    cp = torch.tensor(1004.0)\n",
    "    Lv = torch.tensor(2.5104e6)\n",
    "    one_over_grav = torch.tensor(0.1020408163) # 1/9.8\n",
    "    if len(yto.shape)==3:\n",
    "        thick= one_over_grav*(sp * (hybi[1:61].view(1,-1)-hybi[0:60].view(1,-1)) \n",
    "                             + torch.tensor(100000)*(hyai[1:61].view(1,-1)-hyai[0:60].view(1,-1)))\n",
    "    \n",
    "        dq_pred = ypo[:,:,0]\n",
    "        dT_pred = ypo[:,:,1] \n",
    "        dq_true = yto[:,:,0]\n",
    "        dT_true = yto[:,:,1]\n",
    "        \n",
    "        energy=torch.mean(torch.square(torch.sum(dq_pred*thick*(Lv)+dT_pred*thick*cp,1)\n",
    "                                -      torch.sum(dq_true*thick*(Lv)+dT_true*thick*cp,1)))\n",
    "    else: \n",
    "        # time dimension included\n",
    "                                       #      batch,time,1     (1,1,30)\n",
    "        thick= one_over_grav *(sp * (hybi[1:61].view(1,1,-1)-hybi[0:60].view(1,1,-1)) \n",
    "             + torch.tensor(100000)*(hyai[1:61].view(1,1,-1)-hyai[0:60].view(1,1,-1)))\n",
    "        dT_pred = ypo[:,:,:,0]\n",
    "        dq_pred = ypo[:,:,:,1] \n",
    "        \n",
    "        dT_true = yto[:,:,:,0]\n",
    "        dq_true = yto[:,:,:,1] \n",
    "        \n",
    "        energy=torch.mean(torch.square(torch.sum(dq_pred*thick*Lv + dT_pred*thick*cp,2)\n",
    "                                -      torch.sum(dq_true*thick*Lv + dT_true*thick*cp,2)))\n",
    "    return energy\n",
    "\n",
    "def get_energy_metric(hyai, hybi):\n",
    "    def energy(y_true, y_pred, sp):\n",
    "        return energy_metric(y_true, y_pred, sp, hyai, hybi)\n",
    "    return energy\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    mse = torch.mean(torch.square(y_pred- y_true))\n",
    "    return mse\n",
    "\n",
    "\n",
    "def loss_con(y_true_norm, y_pred_norm, y_true, y_pred, sp, _lambda):\n",
    "\n",
    "    energy = energy_metric(y_true, y_pred, sp, hyai,hybi)\n",
    "    #mse = torch.mean(torch.square(y_pred- y_true))\n",
    "    mse = my_mse_flatten(y_true_norm, y_pred_norm)\n",
    "    loss = mse + _lambda*energy\n",
    "    return loss, energy, mse\n",
    "\n",
    "def get_loss_con(hyai, hybi, _lambda, denorm_func):\n",
    "    def hybrid_loss(y_true_norm, y_pred_norm, y_true, y_pred, sp):\n",
    "        return loss_con(y_true_norm, y_pred_norm, y_true, y_pred, sp, _lambda)\n",
    "    return hybrid_loss\n",
    "\n",
    "def my_hybrid_loss(mse, energy, _lambda):\n",
    "    loss = mse + _lambda*energy\n",
    "    return loss \n",
    "\n",
    "def get_hybrid_loss(_lambda):\n",
    "    def hybrid_loss(mse, energy):\n",
    "        return my_hybrid_loss(mse, energy, _lambda)\n",
    "    return hybrid_loss\n",
    "\n",
    "metric_h_con = get_energy_metric(hyai, hybi)\n",
    "#loss_fn = my_mse_flatten\n",
    "_lambda = torch.tensor(1.0e-7) \n",
    "#_lambda = torch.tensor(1.0e-6) \n",
    "\n",
    "loss_fn = get_hybrid_loss(_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ec5df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fa03d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.regression import R2Score\n",
    "import time\n",
    "\n",
    "if not autoregressive:\n",
    "    timewindow = 1\n",
    "    timestep_scheduling=False\n",
    "else:\n",
    "    timewindow = 3\n",
    "    timestep_scheduling=True\n",
    "    timestep_schedule = np.arange(1000)\n",
    "    timestep_schedule[:] = timewindow\n",
    "\n",
    "    if timestep_scheduling:\n",
    "        timestep_schedule[0:3] = 1\n",
    "        timestep_schedule[3:4] = timewindow-1\n",
    "        timestep_schedule[4:] = timewindow\n",
    "        timestep_schedule[5:] = timewindow+1\n",
    "        timestep_schedule[6:] = timewindow+2\n",
    "\n",
    "    \n",
    "use_wandb = False\n",
    "\n",
    "class model_train_eval:\n",
    "    def __init__(self, dataloader, model, batch_size = 384, autoregressive=True, train=True):\n",
    "        super().__init__()\n",
    "        self.loader = dataloader\n",
    "        self.train = train\n",
    "        self.report_freq = 800\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model \n",
    "        self.autoregressive = autoregressive\n",
    "        if self.autoregressive:\n",
    "            self.model.reset_states()\n",
    "        self.metric_R2 =  R2Score().to(device) \n",
    "        self.metric_R2_heating =  R2Score().to(device) \n",
    "        self.metric_R2_precc =  R2Score().to(device) \n",
    "        self.metric_R2_moistening =  R2Score().to(device) \n",
    "\n",
    "        self.metrics= {'loss': 0, 'mean_squared_error': 0,  # the latter is just MSE\n",
    "                        'mean_absolute_error': 0, 'R2' : 0, 'R2_heating' : 0,'R2_moistening' : 0,  \n",
    "                        'R2_precc' : 0, 'R2_lev' : np.zeros((nlev,ny)),\n",
    "                        'h_conservation' : 0 }\n",
    "\n",
    "    def eval_one_epoch(self, epoch, timewindow=1):\n",
    "        report_freq = self.report_freq\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0\n",
    "        epoch_mse = 0.0; epoch_mae = 0.0\n",
    "        epoch_R2precc = 0.0\n",
    "        epoch_hcon = 0.0\n",
    "        epoch_r2_lev = 0.0\n",
    "        t_comp =0 \n",
    "        if self.autoregressive:\n",
    "            preds_lay = []; preds_sfc = []\n",
    "            targets_lay = []; targets_sfc = [] \n",
    "            sps = []\n",
    "        t0_it = time.time()\n",
    "        j = 0; k = 0; k2=2    \n",
    "        if self.autoregressive:\n",
    "            loss_update_start_index = 60\n",
    "        else:\n",
    "            loss_update_start_index = 0\n",
    "        for i,data in enumerate(self.loader):\n",
    "            inputs_lay_chunks, inputs_sfc_chunks, targets_lay_chunks, targets_sfc_chunks = data\n",
    "            inputs_lay_chunks   = inputs_lay_chunks.to(device)\n",
    "            inputs_sfc_chunks   = inputs_sfc_chunks.to(device)\n",
    "            targets_sfc_chunks  = targets_sfc_chunks.to(device)\n",
    "            targets_lay_chunks  = targets_lay_chunks.to(device)\n",
    "            \n",
    "            inputs_lay_chunks    = torch.split(inputs_lay_chunks, self.batch_size)\n",
    "            inputs_sfc_chunks    = torch.split(inputs_sfc_chunks, self.batch_size)\n",
    "            targets_sfc_chunks   = torch.split(targets_sfc_chunks, self.batch_size)\n",
    "            targets_lay_chunks   = torch.split(targets_lay_chunks, self.batch_size)\n",
    "         \n",
    "            # to speed-up IO, we loaded chunks=many batches, which now need to be divided into batches\n",
    "            for ichunk in range(len(inputs_lay_chunks)):\n",
    "                inputs_lay = inputs_lay_chunks[ichunk]\n",
    "                inputs_sfc = inputs_sfc_chunks[ichunk]\n",
    "                target_lay = targets_lay_chunks[ichunk]\n",
    "                target_sfc = targets_sfc_chunks[ichunk]\n",
    "                sp = inputs_sfc[:,0:1] # surface pressure\n",
    "\n",
    "\n",
    "                tcomp0= time.time()\n",
    "                    \n",
    "                if mp_autocast:\n",
    "                    with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                        pred_lay, pred_sfc = self.model(inputs_lay, inputs_sfc)\n",
    "                else:\n",
    "                    pred_lay, pred_sfc = self.model(inputs_lay, inputs_sfc)\n",
    "                    \n",
    "                if self.autoregressive:\n",
    "                    # In the autoregressive training case are gathering many time steps before computing loss\n",
    "                    preds_lay.append(pred_lay)\n",
    "                    preds_sfc.append(pred_sfc)\n",
    "                    targets_lay.append(target_lay)\n",
    "                    targets_sfc.append(target_sfc)\n",
    "                    sps.append(sp) \n",
    "                    \n",
    "                else:\n",
    "                    preds_lay = pred_lay\n",
    "                    preds_sfc = pred_sfc \n",
    "                    targets_lay = target_lay\n",
    "                    targets_sfc = target_sfc\n",
    "                    sps = sp\n",
    "                    \n",
    "                if (not self.autoregressive) or (self.autoregressive and (j+1) % timewindow==0):\n",
    "            \n",
    "                    if self.autoregressive:\n",
    "                        preds_lay   = torch.stack(preds_lay)\n",
    "                        preds_sfc   = torch.stack(preds_sfc)\n",
    "                        targets_lay = torch.stack(targets_lay)\n",
    "                        targets_sfc = torch.stack(targets_sfc)\n",
    "                        sps         = torch.stack(sps)\n",
    "                                \n",
    "                    if mp_autocast:\n",
    "                        with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                            #loss = loss_fn(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                            \n",
    "                            mse = my_mse_flatten(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                            \n",
    "                            ypo_lay, ypo_sfc = model.postprocessing(preds_lay, preds_sfc)\n",
    "                            yto_lay, yto_sfc = model.postprocessing(targets_lay, targets_sfc)\n",
    "                            sps_denorm = sp = sps*(sp_max - sp_min) + sp_mean\n",
    "                            h_con = metric_h_con(yto_lay, ypo_lay, sps_denorm)\n",
    "                            \n",
    "                            loss = loss_fn(mse, h_con)\n",
    "                    else:\n",
    "                        #loss = loss_fn(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                        \n",
    "                        mse = my_mse_flatten(targets_lay, targets_sfc, preds_lay, preds_sfc)\n",
    "                        ypo_lay, ypo_sfc = model.postprocessing(preds_lay, preds_sfc)\n",
    "                        yto_lay, yto_sfc = model.postprocessing(targets_lay, targets_sfc)\n",
    "                        sps_denorm = sp = sps*(sp_max - sp_min) + sp_mean\n",
    "                        h_con = metric_h_con(yto_lay, ypo_lay, sps_denorm)\n",
    "                        loss = loss_fn(mse, h_con)\n",
    "                        \n",
    "                    if self.train:\n",
    "                        if use_scaler:\n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            loss.backward()       \n",
    "                            optimizer.step()\n",
    "            \n",
    "                        optimizer.zero_grad()\n",
    "                            \n",
    "                    running_loss    += loss.item()\n",
    "                    #mae             = metrics.mean_absolute_error(targets_lay, preds_lay)\n",
    "                    if j>loss_update_start_index:\n",
    "                        with torch.no_grad():\n",
    "                            epoch_loss      += loss.item()\n",
    "                            #epoch_energy    += energy.item()\n",
    "                            epoch_mse       += mse.item()\n",
    "                            #epoch_mae       += mae.item()\n",
    "                        \n",
    "                           # yto, ypo =  denorm_func(targets_lay, preds_lay)\n",
    "                            # -------------- TO-DO:  DE-NORM OUTPUT --------------\n",
    "                            #yto, ypo = targets_lay, preds_lay\n",
    "\n",
    "                            epoch_hcon  += h_con.item()\n",
    "                            \n",
    "                            self.metric_R2.update(ypo_lay.reshape((-1,ny)), yto_lay.reshape((-1,ny)))\n",
    "                            self.metric_R2_heating.update(ypo_lay[:,:,0].reshape(-1,1), yto_lay[:,:,0].reshape(-1,1))\n",
    "                            self.metric_R2_moistening.update(ypo_lay[:,:,1].reshape(-1,1), yto_lay[:,:,1].reshape(-1,1))\n",
    "\n",
    "                            self.metric_R2_precc.update(ypo_sfc[:,3].reshape(-1,1), yto_sfc[:,3].reshape(-1,1))\n",
    "                            \n",
    "                            r2_np = np.corrcoef((ypo_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy(),yto_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy()))[0,1]\n",
    "                            epoch_R2precc += r2_np\n",
    "                            #print(\"R2 numpy\", r2_np, \"R2 torch\", self.metric_R2_precc(ypo_sfc[:,3:4], yto_sfc[:,3:4]) )\n",
    "\n",
    "                            ypo_lay = ypo_lay.reshape(-1,nlev,ny).detach().cpu().numpy()\n",
    "                            yto_lay = yto_lay.reshape(-1,nlev,ny).detach().cpu().numpy()\n",
    "\n",
    "                            epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay) \n",
    "                           # if track_ks:\n",
    "                           #     if (j+1) % max(timewindow*4,12)==0:\n",
    "                           #         epoch_ks += kolmogorov_smirnov(yto,ypo).item()\n",
    "                           #         k2 += 1\n",
    "                            k += 1\n",
    "                    if self.autoregressive:\n",
    "                        preds_lay = []; preds_sfc = []\n",
    "                        targets_lay = []; targets_sfc = [] \n",
    "                        sps = []\n",
    "                    if self.autoregressive: \n",
    "                        model.detach_states()\n",
    "                \n",
    "                t_comp += time.time() - tcomp0\n",
    "                # # print statistics \n",
    "                if j % report_freq == (report_freq-1): # print every 200 minibatches\n",
    "                    elaps = time.time() - t0_it\n",
    "                    running_loss = running_loss / (report_freq/timewindow)\n",
    "                    #running_energy = running_energy / (report_freq/timewindow)\n",
    "                    r2raw = self.metric_R2.compute()\n",
    "                    #r2raw_prec = self.metric_R2_precc.compute()\n",
    "\n",
    "                    #ypo_lay, ypo_sfc = model.postprocessing(preds_lay, preds_sfc)\n",
    "                    #yto_lay, yto_sfc = model.postprocessing(targets_lay, targets_sfc) \n",
    "                    #r2_np = np.corrcoef((ypo_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy(),yto_sfc.reshape(-1,ny_sfc)[:,3].detach().cpu().numpy()))[0,1]\n",
    "\n",
    "                   # print(torch.mean(ypo_sfc[:,3] -  yto_sfc[:,3]))\n",
    "                   # print(torch.mean(preds_sfc[:,3] - targets_sfc[:,3]))\n",
    "                    \n",
    "\n",
    "                    print(\"[{:d}, {:d}] Loss: {:.2e}  runR2: {:.2f},  elapsed {:.1f}s (compute {:.1f})\" .format(epoch + 1, \n",
    "                                                    j+1, running_loss, r2raw, elaps, t_comp))\n",
    "                    running_loss = 0.0\n",
    "                    running_energy = 0.0\n",
    "                    t0_it = time.time()\n",
    "                    t_comp = 0\n",
    "                j += 1\n",
    "\n",
    "        self.metrics['loss'] =  epoch_loss / k\n",
    "        self.metrics['mean_squared_error'] = epoch_mse / k\n",
    "        self.metrics[\"h_conservation\"] =  epoch_hcon / k\n",
    "\n",
    "        #self.metrics['energymetric'] = epoch_energy / k\n",
    "        #self.metrics['mean_absolute_error'] = epoch_mae / k\n",
    "        #self.metrics['ks'] =  epoch_ks / k2\n",
    "        self.metrics['R2'] = self.metric_R2.compute()\n",
    "        self.metrics['R2_heating'] = self.metric_R2_heating.compute()\n",
    "        self.metrics['R2_moistening'] = self.metric_R2_moistening.compute()\n",
    "\n",
    "        #self.metrics['R2_precc'] = self.metric_R2_precc.compute()\n",
    "        self.metrics['R2_precc'] = epoch_R2precc / k\n",
    "        \n",
    "        self.metrics['R2_lev'] = epoch_r2_lev / k\n",
    "\n",
    "        self.metric_R2.reset(); self.metric_R2_heating.reset(); self.metric_R2_precc.reset()\n",
    "        if self.autoregressive:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "        datatype = \"TRAIN\" if self.train else \"VAL\"\n",
    "        print('Epoch {} {} loss: {:.2e}  MSE: {:.2e}  h-con:  {:.2e}   R2: {:.2f}  R2-dT/dt: {:.2f}   R2-dq/dt: {:.2f}   R2-precc: {:.3f}'.format(epoch+1, datatype, \n",
    "                                                            self.metrics['loss'], \n",
    "                                                            self.metrics['mean_squared_error'], \n",
    "                                                            self.metrics['h_conservation'],\n",
    "                                                            self.metrics['R2'],\n",
    "                                                            self.metrics['R2_heating'],\n",
    "                                                            self.metrics['R2_moistening'],                                                              \n",
    "                                                            self.metrics['R2_precc'] ))\n",
    "\n",
    "    if cuda: torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e55c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/28272/ipykernel_1666826/1465003934.py:28: RuntimeWarning: divide by zero encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_1666826/1465003934.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_1666826/1891790002.py:176: RuntimeWarning: invalid value encountered in add\n",
      "  epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 800] Loss: 3.42e-03  runR2: 0.11,  elapsed 13.8s (compute 7.8)\n",
      "[1, 1600] Loss: 2.11e-03  runR2: 0.36,  elapsed 8.4s (compute 8.0)\n",
      "[1, 2400] Loss: 1.83e-03  runR2: 0.45,  elapsed 8.0s (compute 7.6)\n",
      "[1, 3200] Loss: 1.71e-03  runR2: 0.49,  elapsed 7.9s (compute 7.5)\n",
      "[1, 4000] Loss: 1.75e-03  runR2: 0.52,  elapsed 7.7s (compute 7.3)\n",
      "[1, 4800] Loss: 1.72e-03  runR2: 0.55,  elapsed 7.9s (compute 7.5)\n",
      "[1, 5600] Loss: 1.68e-03  runR2: 0.56,  elapsed 7.7s (compute 7.3)\n",
      "[1, 6400] Loss: 1.64e-03  runR2: 0.57,  elapsed 7.6s (compute 7.3)\n",
      "[1, 7200] Loss: 1.60e-03  runR2: 0.58,  elapsed 8.9s (compute 8.5)\n",
      "[1, 8000] Loss: 1.64e-03  runR2: 0.59,  elapsed 8.9s (compute 8.1)\n",
      "[1, 8800] Loss: 1.70e-03  runR2: 0.59,  elapsed 7.6s (compute 7.1)\n",
      "[1, 9600] Loss: 1.74e-03  runR2: 0.60,  elapsed 8.4s (compute 7.9)\n",
      "[1, 10400] Loss: 1.72e-03  runR2: 0.60,  elapsed 8.3s (compute 7.8)\n",
      "[1, 11200] Loss: 1.56e-03  runR2: 0.61,  elapsed 8.4s (compute 7.9)\n",
      "[1, 12000] Loss: 1.65e-03  runR2: 0.62,  elapsed 8.3s (compute 7.9)\n",
      "[1, 12800] Loss: 1.42e-03  runR2: 0.63,  elapsed 8.2s (compute 7.8)\n",
      "[1, 13600] Loss: 1.38e-03  runR2: 0.64,  elapsed 7.9s (compute 7.5)\n",
      "[1, 14400] Loss: 1.33e-03  runR2: 0.65,  elapsed 7.9s (compute 7.5)\n",
      "[1, 15200] Loss: 1.30e-03  runR2: 0.66,  elapsed 8.8s (compute 8.0)\n",
      "[1, 16000] Loss: 1.29e-03  runR2: 0.67,  elapsed 8.3s (compute 7.8)\n",
      "[1, 16800] Loss: 1.21e-03  runR2: 0.67,  elapsed 8.5s (compute 8.1)\n",
      "[1, 17600] Loss: 1.21e-03  runR2: 0.68,  elapsed 8.6s (compute 8.1)\n",
      "[1, 18400] Loss: 1.17e-03  runR2: 0.68,  elapsed 8.7s (compute 8.2)\n",
      "[1, 19200] Loss: 1.14e-03  runR2: 0.68,  elapsed 8.3s (compute 7.8)\n",
      "[1, 20000] Loss: 1.27e-03  runR2: 0.69,  elapsed 10.0s (compute 9.5)\n",
      "[1, 20800] Loss: 1.22e-03  runR2: 0.69,  elapsed 15.0s (compute 14.5)\n",
      "[1, 21600] Loss: 1.30e-03  runR2: 0.69,  elapsed 14.9s (compute 14.5)\n",
      "[1, 22400] Loss: 1.44e-03  runR2: 0.69,  elapsed 9.9s (compute 8.8)\n",
      "[1, 23200] Loss: 1.37e-03  runR2: 0.70,  elapsed 8.5s (compute 7.9)\n",
      "[1, 24000] Loss: 1.30e-03  runR2: 0.70,  elapsed 8.4s (compute 7.8)\n",
      "[1, 24800] Loss: 1.24e-03  runR2: 0.70,  elapsed 8.5s (compute 8.2)\n",
      "[1, 25600] Loss: 1.31e-03  runR2: 0.70,  elapsed 9.5s (compute 7.9)\n",
      "Epoch 1 TRAIN loss: 1.51e-03  MSE: 1.22e-03  h-con:  2.90e+03   R2: 0.70  R2-dT/dt: -0.60   R2-dq/dt: 0.53   R2-precc: 0.935\n",
      "Epoch 1/10 complete, took 294.91 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.36e-03  runR2: 0.76,  elapsed 14.3s (compute 7.9)\n",
      "[2, 1600] Loss: 1.17e-03  runR2: 0.78,  elapsed 7.9s (compute 7.5)\n",
      "[2, 2400] Loss: 1.13e-03  runR2: 0.78,  elapsed 8.2s (compute 7.8)\n",
      "[2, 3200] Loss: 1.13e-03  runR2: 0.78,  elapsed 7.8s (compute 7.3)\n",
      "[2, 4000] Loss: 1.22e-03  runR2: 0.78,  elapsed 7.4s (compute 7.0)\n",
      "[2, 4800] Loss: 1.26e-03  runR2: 0.78,  elapsed 7.5s (compute 7.1)\n",
      "[2, 5600] Loss: 1.27e-03  runR2: 0.78,  elapsed 8.3s (compute 8.0)\n",
      "[2, 6400] Loss: 1.28e-03  runR2: 0.78,  elapsed 8.6s (compute 8.2)\n",
      "[2, 7200] Loss: 1.28e-03  runR2: 0.78,  elapsed 8.2s (compute 7.9)\n",
      "[2, 8000] Loss: 1.34e-03  runR2: 0.78,  elapsed 8.9s (compute 8.0)\n",
      "[2, 8800] Loss: 1.42e-03  runR2: 0.78,  elapsed 8.3s (compute 7.8)\n",
      "[2, 9600] Loss: 1.45e-03  runR2: 0.78,  elapsed 8.3s (compute 7.8)\n",
      "[2, 10400] Loss: 1.45e-03  runR2: 0.78,  elapsed 9.0s (compute 8.6)\n",
      "[2, 11200] Loss: 1.31e-03  runR2: 0.78,  elapsed 8.7s (compute 8.2)\n",
      "[2, 12000] Loss: 1.42e-03  runR2: 0.79,  elapsed 8.4s (compute 7.9)\n",
      "[2, 12800] Loss: 1.23e-03  runR2: 0.79,  elapsed 8.3s (compute 7.9)\n",
      "[2, 13600] Loss: 1.20e-03  runR2: 0.79,  elapsed 8.4s (compute 7.9)\n",
      "[2, 14400] Loss: 1.17e-03  runR2: 0.79,  elapsed 8.4s (compute 7.9)\n",
      "[2, 15200] Loss: 1.15e-03  runR2: 0.79,  elapsed 9.1s (compute 8.3)\n",
      "[2, 16000] Loss: 1.15e-03  runR2: 0.80,  elapsed 8.9s (compute 8.4)\n",
      "[2, 16800] Loss: 1.08e-03  runR2: 0.80,  elapsed 8.3s (compute 7.9)\n",
      "[2, 17600] Loss: 1.07e-03  runR2: 0.80,  elapsed 9.7s (compute 9.2)\n",
      "[2, 18400] Loss: 1.06e-03  runR2: 0.80,  elapsed 9.0s (compute 8.5)\n",
      "[2, 19200] Loss: 1.03e-03  runR2: 0.80,  elapsed 8.6s (compute 8.0)\n",
      "[2, 20000] Loss: 1.16e-03  runR2: 0.80,  elapsed 8.5s (compute 8.0)\n",
      "[2, 20800] Loss: 1.11e-03  runR2: 0.80,  elapsed 8.4s (compute 7.9)\n",
      "[2, 21600] Loss: 1.18e-03  runR2: 0.80,  elapsed 8.7s (compute 8.1)\n",
      "[2, 22400] Loss: 1.32e-03  runR2: 0.80,  elapsed 8.3s (compute 7.2)\n",
      "[2, 23200] Loss: 1.26e-03  runR2: 0.80,  elapsed 12.4s (compute 11.8)\n",
      "[2, 24000] Loss: 1.20e-03  runR2: 0.80,  elapsed 15.2s (compute 14.7)\n",
      "[2, 24800] Loss: 1.15e-03  runR2: 0.80,  elapsed 12.6s (compute 12.1)\n",
      "[2, 25600] Loss: 1.21e-03  runR2: 0.80,  elapsed 9.0s (compute 8.6)\n",
      "Epoch 2 TRAIN loss: 1.22e-03  MSE: 1.00e-03  h-con:  2.19e+03   R2: 0.80  R2-dT/dt: 0.73   R2-dq/dt: 0.71   R2-precc: 0.971\n",
      "Epoch 2/10 complete, took 299.36 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.42e-03  runR2: 0.79,  elapsed 17.1s (compute 7.6)\n",
      "[3, 1600] Loss: 1.10e-03  runR2: 0.80,  elapsed 11.7s (compute 7.7)\n",
      "[3, 2400] Loss: 1.06e-03  runR2: 0.81,  elapsed 8.9s (compute 8.5)\n",
      "[3, 3200] Loss: 1.06e-03  runR2: 0.81,  elapsed 19.1s (compute 8.8)\n",
      "[3, 4000] Loss: 1.15e-03  runR2: 0.81,  elapsed 8.6s (compute 8.1)\n",
      "[3, 4800] Loss: 1.19e-03  runR2: 0.81,  elapsed 23.3s (compute 8.0)\n",
      "[3, 5600] Loss: 1.20e-03  runR2: 0.81,  elapsed 8.7s (compute 8.1)\n",
      "[3, 6400] Loss: 1.21e-03  runR2: 0.81,  elapsed 25.6s (compute 8.1)\n",
      "[3, 7200] Loss: 1.22e-03  runR2: 0.81,  elapsed 8.5s (compute 7.9)\n",
      "[3, 8000] Loss: 1.27e-03  runR2: 0.80,  elapsed 13.4s (compute 8.2)\n",
      "[3, 8800] Loss: 1.35e-03  runR2: 0.80,  elapsed 79.8s (compute 8.0)\n",
      "[3, 9600] Loss: 1.38e-03  runR2: 0.80,  elapsed 20.4s (compute 8.1)\n",
      "[3, 10400] Loss: 1.38e-03  runR2: 0.80,  elapsed 110.9s (compute 8.3)\n",
      "[3, 11200] Loss: 1.25e-03  runR2: 0.81,  elapsed 8.9s (compute 8.2)\n",
      "[3, 12000] Loss: 1.35e-03  runR2: 0.81,  elapsed 46.7s (compute 8.1)\n",
      "[3, 12800] Loss: 1.18e-03  runR2: 0.81,  elapsed 8.5s (compute 7.9)\n",
      "[3, 13600] Loss: 1.14e-03  runR2: 0.81,  elapsed 423.6s (compute 9.4)\n",
      "[3, 14400] Loss: 1.12e-03  runR2: 0.81,  elapsed 8.8s (compute 8.1)\n",
      "[3, 15200] Loss: 1.10e-03  runR2: 0.81,  elapsed 197.1s (compute 7.9)\n",
      "[3, 16000] Loss: 1.10e-03  runR2: 0.82,  elapsed 34.3s (compute 8.2)\n",
      "[3, 16800] Loss: 1.04e-03  runR2: 0.82,  elapsed 8.6s (compute 8.1)\n",
      "[3, 17600] Loss: 1.03e-03  runR2: 0.82,  elapsed 8.2s (compute 7.6)\n",
      "[3, 18400] Loss: 1.02e-03  runR2: 0.82,  elapsed 8.4s (compute 8.0)\n",
      "[3, 19200] Loss: 9.94e-04  runR2: 0.82,  elapsed 9.1s (compute 8.5)\n",
      "[3, 20000] Loss: 1.12e-03  runR2: 0.82,  elapsed 8.6s (compute 8.1)\n",
      "[3, 20800] Loss: 1.07e-03  runR2: 0.82,  elapsed 7.9s (compute 7.4)\n",
      "[3, 21600] Loss: 1.14e-03  runR2: 0.82,  elapsed 8.3s (compute 7.9)\n",
      "[3, 22400] Loss: 1.27e-03  runR2: 0.82,  elapsed 9.2s (compute 8.2)\n",
      "[3, 23200] Loss: 1.22e-03  runR2: 0.82,  elapsed 10.1s (compute 9.5)\n",
      "[3, 24000] Loss: 1.16e-03  runR2: 0.82,  elapsed 8.4s (compute 8.0)\n",
      "[3, 24800] Loss: 1.11e-03  runR2: 0.82,  elapsed 7.8s (compute 7.3)\n",
      "[3, 25600] Loss: 1.17e-03  runR2: 0.82,  elapsed 8.3s (compute 7.9)\n",
      "Epoch 3 TRAIN loss: 1.16e-03  MSE: 9.63e-04  h-con:  2.01e+03   R2: 0.82  R2-dT/dt: 0.75   R2-dq/dt: 0.78   R2-precc: 0.976\n",
      "Epoch 3/10 complete, took 1193.96 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 1.48e-03  runR2: 0.80,  elapsed 16.1s (compute 6.4)\n",
      "[4, 1600] Loss: 1.05e-03  runR2: 0.81,  elapsed 6.8s (compute 6.4)\n",
      "[4, 2400] Loss: 1.02e-03  runR2: 0.82,  elapsed 7.8s (compute 7.4)\n",
      "[4, 3200] Loss: 1.01e-03  runR2: 0.82,  elapsed 8.0s (compute 6.6)\n",
      "[4, 4000] Loss: 1.09e-03  runR2: 0.82,  elapsed 7.1s (compute 6.7)\n",
      "[4, 4800] Loss: 1.13e-03  runR2: 0.82,  elapsed 7.7s (compute 7.3)\n",
      "[4, 5600] Loss: 1.14e-03  runR2: 0.81,  elapsed 7.3s (compute 6.9)\n",
      "[4, 6400] Loss: 1.15e-03  runR2: 0.81,  elapsed 7.5s (compute 7.1)\n",
      "[4, 7200] Loss: 1.16e-03  runR2: 0.81,  elapsed 7.3s (compute 6.9)\n",
      "[4, 8000] Loss: 1.20e-03  runR2: 0.81,  elapsed 7.6s (compute 6.8)\n",
      "[4, 8800] Loss: 1.29e-03  runR2: 0.81,  elapsed 7.0s (compute 6.6)\n",
      "[4, 9600] Loss: 1.31e-03  runR2: 0.81,  elapsed 7.3s (compute 6.9)\n",
      "[4, 10400] Loss: 1.32e-03  runR2: 0.81,  elapsed 7.2s (compute 6.7)\n",
      "[4, 11200] Loss: 1.19e-03  runR2: 0.81,  elapsed 8.2s (compute 6.6)\n",
      "[4, 12000] Loss: 1.29e-03  runR2: 0.82,  elapsed 7.0s (compute 6.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 12800] Loss: 1.13e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[4, 13600] Loss: 1.08e-03  runR2: 0.82,  elapsed 7.1s (compute 6.7)\n",
      "[4, 14400] Loss: 1.07e-03  runR2: 0.82,  elapsed 7.3s (compute 6.9)\n",
      "[4, 15200] Loss: 1.04e-03  runR2: 0.82,  elapsed 7.0s (compute 6.3)\n",
      "[4, 16000] Loss: 1.05e-03  runR2: 0.82,  elapsed 7.1s (compute 6.8)\n",
      "[4, 16800] Loss: 9.88e-04  runR2: 0.82,  elapsed 7.0s (compute 6.7)\n",
      "[4, 17600] Loss: 9.75e-04  runR2: 0.83,  elapsed 7.2s (compute 6.8)\n",
      "[4, 18400] Loss: 9.63e-04  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[4, 19200] Loss: 9.40e-04  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[4, 20000] Loss: 1.06e-03  runR2: 0.83,  elapsed 7.2s (compute 6.8)\n",
      "[4, 20800] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[4, 21600] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[4, 22400] Loss: 1.21e-03  runR2: 0.83,  elapsed 7.2s (compute 6.1)\n",
      "[4, 23200] Loss: 1.16e-03  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[4, 24000] Loss: 1.11e-03  runR2: 0.83,  elapsed 7.1s (compute 6.6)\n",
      "[4, 24800] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.8s (compute 6.4)\n",
      "[4, 25600] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "Epoch 4 TRAIN loss: 1.11e-03  MSE: 9.26e-04  h-con:  1.82e+03   R2: 0.83  R2-dT/dt: 0.76   R2-dq/dt: 0.82   R2-precc: 0.980\n",
      "Epoch 4/10 complete, took 244.88 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 1.52e-03  runR2: 0.81,  elapsed 11.7s (compute 6.1)\n",
      "[5, 1600] Loss: 9.98e-04  runR2: 0.82,  elapsed 6.9s (compute 6.5)\n",
      "[5, 2400] Loss: 9.68e-04  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[5, 3200] Loss: 9.68e-04  runR2: 0.83,  elapsed 7.0s (compute 6.6)\n",
      "[5, 4000] Loss: 1.04e-03  runR2: 0.82,  elapsed 6.8s (compute 6.4)\n",
      "[5, 4800] Loss: 1.09e-03  runR2: 0.82,  elapsed 6.7s (compute 6.4)\n",
      "[5, 5600] Loss: 1.10e-03  runR2: 0.82,  elapsed 6.4s (compute 6.1)\n",
      "[5, 6400] Loss: 1.11e-03  runR2: 0.82,  elapsed 6.4s (compute 6.1)\n",
      "[5, 7200] Loss: 1.12e-03  runR2: 0.82,  elapsed 6.8s (compute 6.5)\n",
      "[5, 8000] Loss: 1.15e-03  runR2: 0.82,  elapsed 7.2s (compute 6.4)\n",
      "[5, 8800] Loss: 1.24e-03  runR2: 0.82,  elapsed 6.8s (compute 6.4)\n",
      "[5, 9600] Loss: 1.27e-03  runR2: 0.82,  elapsed 7.1s (compute 6.6)\n",
      "[5, 10400] Loss: 1.27e-03  runR2: 0.82,  elapsed 6.8s (compute 6.4)\n",
      "[5, 11200] Loss: 1.14e-03  runR2: 0.82,  elapsed 6.8s (compute 6.3)\n",
      "[5, 12000] Loss: 1.24e-03  runR2: 0.82,  elapsed 6.8s (compute 6.3)\n",
      "[5, 12800] Loss: 1.08e-03  runR2: 0.82,  elapsed 6.6s (compute 6.2)\n",
      "[5, 13600] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[5, 14400] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[5, 15200] Loss: 9.99e-04  runR2: 0.83,  elapsed 7.0s (compute 6.1)\n",
      "[5, 16000] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.6s (compute 6.1)\n",
      "[5, 16800] Loss: 9.51e-04  runR2: 0.83,  elapsed 6.6s (compute 6.1)\n",
      "[5, 17600] Loss: 9.39e-04  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[5, 18400] Loss: 9.30e-04  runR2: 0.83,  elapsed 7.0s (compute 6.5)\n",
      "[5, 19200] Loss: 9.14e-04  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[5, 20000] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.6s (compute 6.1)\n",
      "[5, 20800] Loss: 9.81e-04  runR2: 0.83,  elapsed 6.7s (compute 6.2)\n",
      "[5, 21600] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.9s (compute 6.4)\n",
      "[5, 22400] Loss: 1.17e-03  runR2: 0.83,  elapsed 8.5s (compute 5.9)\n",
      "[5, 23200] Loss: 1.12e-03  runR2: 0.83,  elapsed 7.2s (compute 6.8)\n",
      "[5, 24000] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.9s (compute 6.4)\n",
      "[5, 24800] Loss: 1.02e-03  runR2: 0.83,  elapsed 7.2s (compute 6.6)\n",
      "[5, 25600] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "Epoch 5 TRAIN loss: 1.07e-03  MSE: 9.00e-04  h-con:  1.67e+03   R2: 0.83  R2-dT/dt: 0.77   R2-dq/dt: 0.84   R2-precc: 0.983\n",
      "Epoch 5/10 complete, took 229.02 seconds, autoreg window was 3\n",
      "Epoch 6 Training rollout timesteps: 4 \n",
      "[6, 800] Loss: 1.60e-03  runR2: 0.82,  elapsed 11.3s (compute 5.8)\n",
      "[6, 1600] Loss: 9.71e-04  runR2: 0.83,  elapsed 6.2s (compute 5.9)\n",
      "[6, 2400] Loss: 9.42e-04  runR2: 0.83,  elapsed 6.2s (compute 5.9)\n",
      "[6, 3200] Loss: 9.48e-04  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[6, 4000] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[6, 4800] Loss: 1.06e-03  runR2: 0.83,  elapsed 5.9s (compute 5.5)\n",
      "[6, 5600] Loss: 1.07e-03  runR2: 0.83,  elapsed 5.9s (compute 5.5)\n",
      "[6, 6400] Loss: 1.08e-03  runR2: 0.83,  elapsed 5.8s (compute 5.4)\n",
      "[6, 7200] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.3s (compute 5.8)\n",
      "[6, 8000] Loss: 1.12e-03  runR2: 0.83,  elapsed 6.2s (compute 5.5)\n",
      "[6, 8800] Loss: 1.21e-03  runR2: 0.82,  elapsed 6.4s (compute 6.0)\n",
      "[6, 9600] Loss: 1.24e-03  runR2: 0.82,  elapsed 5.8s (compute 5.4)\n",
      "[6, 10400] Loss: 1.24e-03  runR2: 0.82,  elapsed 5.9s (compute 5.5)\n",
      "[6, 11200] Loss: 1.12e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[6, 12000] Loss: 1.21e-03  runR2: 0.83,  elapsed 5.9s (compute 5.5)\n",
      "[6, 12800] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[6, 13600] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[6, 14400] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[6, 15200] Loss: 9.81e-04  runR2: 0.83,  elapsed 6.4s (compute 5.7)\n",
      "[6, 16000] Loss: 9.91e-04  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[6, 16800] Loss: 9.27e-04  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[6, 17600] Loss: 9.19e-04  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "[6, 18400] Loss: 9.04e-04  runR2: 0.84,  elapsed 6.2s (compute 5.9)\n",
      "[6, 19200] Loss: 8.90e-04  runR2: 0.84,  elapsed 6.3s (compute 5.9)\n",
      "[6, 20000] Loss: 1.00e-03  runR2: 0.84,  elapsed 5.5s (compute 5.2)\n",
      "[6, 20800] Loss: 9.58e-04  runR2: 0.84,  elapsed 5.9s (compute 5.6)\n",
      "[6, 21600] Loss: 1.02e-03  runR2: 0.84,  elapsed 5.7s (compute 5.4)\n",
      "[6, 22400] Loss: 1.15e-03  runR2: 0.84,  elapsed 8.7s (compute 5.2)\n",
      "[6, 23200] Loss: 1.10e-03  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[6, 24000] Loss: 1.05e-03  runR2: 0.83,  elapsed 11.9s (compute 6.0)\n",
      "[6, 24800] Loss: 1.00e-03  runR2: 0.83,  elapsed 6.0s (compute 5.7)\n",
      "[6, 25600] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.9s (compute 5.6)\n",
      "Epoch 6 TRAIN loss: 1.04e-03  MSE: 8.85e-04  h-con:  1.57e+03   R2: 0.83  R2-dT/dt: 0.78   R2-dq/dt: 0.86   R2-precc: 0.985\n",
      "Epoch 6/10 complete, took 215.58 seconds, autoreg window was 4\n",
      "Epoch 7 Training rollout timesteps: 5 \n",
      "[7, 800] Loss: 1.70e-03  runR2: 0.82,  elapsed 11.3s (compute 5.8)\n",
      "[7, 1600] Loss: 9.56e-04  runR2: 0.83,  elapsed 5.6s (compute 5.3)\n",
      "[7, 2400] Loss: 9.27e-04  runR2: 0.83,  elapsed 5.7s (compute 5.3)\n",
      "[7, 3200] Loss: 9.31e-04  runR2: 0.83,  elapsed 6.8s (compute 6.3)\n",
      "[7, 4000] Loss: 9.94e-04  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[7, 4800] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[7, 5600] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[7, 6400] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.2s (compute 5.9)\n",
      "[7, 7200] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "[7, 8000] Loss: 1.11e-03  runR2: 0.83,  elapsed 7.3s (compute 6.3)\n",
      "[7, 8800] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[7, 9600] Loss: 1.22e-03  runR2: 0.83,  elapsed 5.9s (compute 5.5)\n",
      "[7, 10400] Loss: 1.21e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[7, 11200] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.0s (compute 5.6)\n",
      "[7, 12000] Loss: 1.20e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[7, 12800] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[7, 13600] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[7, 14400] Loss: 9.96e-04  runR2: 0.83,  elapsed 5.7s (compute 5.4)\n",
      "[7, 15200] Loss: 9.67e-04  runR2: 0.83,  elapsed 6.5s (compute 5.7)\n",
      "[7, 16000] Loss: 9.76e-04  runR2: 0.84,  elapsed 6.3s (compute 5.8)\n",
      "[7, 16800] Loss: 9.17e-04  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[7, 17600] Loss: 9.10e-04  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[7, 18400] Loss: 8.92e-04  runR2: 0.84,  elapsed 5.9s (compute 5.6)\n",
      "[7, 19200] Loss: 8.79e-04  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[7, 20000] Loss: 9.90e-04  runR2: 0.84,  elapsed 5.7s (compute 5.3)\n",
      "[7, 20800] Loss: 9.43e-04  runR2: 0.84,  elapsed 5.7s (compute 5.3)\n",
      "[7, 21600] Loss: 1.00e-03  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[7, 22400] Loss: 1.13e-03  runR2: 0.84,  elapsed 6.6s (compute 5.9)\n",
      "[7, 23200] Loss: 1.09e-03  runR2: 0.84,  elapsed 5.9s (compute 5.4)\n",
      "[7, 24000] Loss: 1.04e-03  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[7, 24800] Loss: 9.87e-04  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 25600] Loss: 1.04e-03  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "Epoch 7 TRAIN loss: 1.03e-03  MSE: 8.75e-04  h-con:  1.51e+03   R2: 0.84  R2-dT/dt: 0.79   R2-dq/dt: 0.87   R2-precc: 0.986\n",
      "Epoch 7/10 complete, took 208.30 seconds, autoreg window was 5\n",
      "Epoch 8 Training rollout timesteps: 5 \n",
      "[8, 800] Loss: 1.76e-03  runR2: 0.82,  elapsed 12.5s (compute 5.6)\n",
      "[8, 1600] Loss: 9.46e-04  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[8, 2400] Loss: 9.20e-04  runR2: 0.84,  elapsed 5.8s (compute 5.4)\n",
      "[8, 3200] Loss: 9.23e-04  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "[8, 4000] Loss: 9.86e-04  runR2: 0.83,  elapsed 5.5s (compute 5.1)\n",
      "[8, 4800] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[8, 5600] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[8, 6400] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[8, 7200] Loss: 1.06e-03  runR2: 0.83,  elapsed 26.4s (compute 5.9)\n",
      "[8, 8000] Loss: 1.10e-03  runR2: 0.83,  elapsed 53.9s (compute 5.8)\n",
      "[8, 8800] Loss: 1.18e-03  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[8, 9600] Loss: 1.21e-03  runR2: 0.83,  elapsed 9.1s (compute 5.7)\n",
      "[8, 10400] Loss: 1.20e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[8, 11200] Loss: 1.10e-03  runR2: 0.83,  elapsed 8.6s (compute 6.0)\n",
      "[8, 12000] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[8, 12800] Loss: 1.04e-03  runR2: 0.83,  elapsed 8.7s (compute 6.2)\n",
      "[8, 13600] Loss: 9.99e-04  runR2: 0.83,  elapsed 6.6s (compute 6.0)\n",
      "[8, 14400] Loss: 9.88e-04  runR2: 0.83,  elapsed 14.8s (compute 5.8)\n",
      "[8, 15200] Loss: 9.60e-04  runR2: 0.84,  elapsed 7.0s (compute 6.1)\n",
      "[8, 16000] Loss: 9.69e-04  runR2: 0.84,  elapsed 6.4s (compute 5.9)\n",
      "[8, 16800] Loss: 9.11e-04  runR2: 0.84,  elapsed 22.7s (compute 5.9)\n",
      "[8, 17600] Loss: 9.04e-04  runR2: 0.84,  elapsed 6.8s (compute 5.8)\n",
      "[8, 18400] Loss: 8.86e-04  runR2: 0.84,  elapsed 125.4s (compute 5.6)\n",
      "[8, 19200] Loss: 8.74e-04  runR2: 0.84,  elapsed 6.7s (compute 5.7)\n",
      "[8, 20000] Loss: 9.84e-04  runR2: 0.84,  elapsed 53.9s (compute 5.5)\n",
      "[8, 20800] Loss: 9.37e-04  runR2: 0.84,  elapsed 6.8s (compute 5.7)\n",
      "[8, 21600] Loss: 9.98e-04  runR2: 0.84,  elapsed 31.6s (compute 5.8)\n",
      "[8, 22400] Loss: 1.13e-03  runR2: 0.84,  elapsed 48.1s (compute 5.7)\n",
      "[8, 23200] Loss: 1.08e-03  runR2: 0.84,  elapsed 6.7s (compute 5.9)\n",
      "[8, 24000] Loss: 1.03e-03  runR2: 0.84,  elapsed 23.6s (compute 5.7)\n",
      "[8, 24800] Loss: 9.80e-04  runR2: 0.84,  elapsed 6.6s (compute 5.8)\n",
      "[8, 25600] Loss: 1.04e-03  runR2: 0.84,  elapsed 16.1s (compute 5.8)\n",
      "Epoch 8 TRAIN loss: 1.02e-03  MSE: 8.71e-04  h-con:  1.49e+03   R2: 0.84  R2-dT/dt: 0.79   R2-dq/dt: 0.88   R2-precc: 0.986\n",
      "Epoch 8/10 complete, took 576.14 seconds, autoreg window was 5\n",
      "Epoch 9 Training rollout timesteps: 5 \n",
      "[9, 800] Loss: 1.80e-03  runR2: 0.82,  elapsed 12.8s (compute 5.9)\n",
      "[9, 1600] Loss: 9.40e-04  runR2: 0.83,  elapsed 6.7s (compute 6.2)\n",
      "[9, 2400] Loss: 9.14e-04  runR2: 0.84,  elapsed 6.6s (compute 6.1)\n",
      "[9, 3200] Loss: 9.17e-04  runR2: 0.84,  elapsed 6.4s (compute 6.0)\n",
      "[9, 4000] Loss: 9.80e-04  runR2: 0.83,  elapsed 7.0s (compute 6.6)\n",
      "[9, 4800] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[9, 5600] Loss: 1.04e-03  runR2: 0.83,  elapsed 5.7s (compute 5.3)\n",
      "[9, 6400] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[9, 7200] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "[9, 8000] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.4s (compute 5.6)\n",
      "[9, 8800] Loss: 1.18e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "[9, 9600] Loss: 1.20e-03  runR2: 0.83,  elapsed 5.8s (compute 5.4)\n",
      "[9, 10400] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.2s (compute 5.7)\n",
      "[9, 11200] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.3s (compute 5.8)\n",
      "[9, 12000] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[9, 12800] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[9, 13600] Loss: 9.94e-04  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[9, 14400] Loss: 9.84e-04  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[9, 15200] Loss: 9.55e-04  runR2: 0.84,  elapsed 6.1s (compute 5.3)\n",
      "[9, 16000] Loss: 9.64e-04  runR2: 0.84,  elapsed 5.8s (compute 5.4)\n",
      "[9, 16800] Loss: 9.07e-04  runR2: 0.84,  elapsed 6.4s (compute 5.9)\n",
      "[9, 17600] Loss: 8.99e-04  runR2: 0.84,  elapsed 6.5s (compute 6.1)\n",
      "[9, 18400] Loss: 8.83e-04  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[9, 19200] Loss: 8.71e-04  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[9, 20000] Loss: 9.80e-04  runR2: 0.84,  elapsed 5.6s (compute 5.1)\n",
      "[9, 20800] Loss: 9.33e-04  runR2: 0.84,  elapsed 5.3s (compute 4.9)\n",
      "[9, 21600] Loss: 9.93e-04  runR2: 0.84,  elapsed 5.3s (compute 4.9)\n",
      "[9, 22400] Loss: 1.12e-03  runR2: 0.84,  elapsed 6.7s (compute 5.8)\n",
      "[9, 23200] Loss: 1.08e-03  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "[9, 24000] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[9, 24800] Loss: 9.76e-04  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[9, 25600] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "Epoch 9 TRAIN loss: 1.01e-03  MSE: 8.68e-04  h-con:  1.47e+03   R2: 0.84  R2-dT/dt: 0.79   R2-dq/dt: 0.89   R2-precc: 0.986\n",
      "Epoch 9/10 complete, took 209.45 seconds, autoreg window was 5\n",
      "Epoch 10 Training rollout timesteps: 5 \n",
      "[10, 800] Loss: 1.79e-03  runR2: 0.82,  elapsed 10.7s (compute 5.2)\n",
      "[10, 1600] Loss: 9.36e-04  runR2: 0.83,  elapsed 5.8s (compute 5.5)\n",
      "[10, 2400] Loss: 9.10e-04  runR2: 0.84,  elapsed 5.3s (compute 5.0)\n",
      "[10, 3200] Loss: 9.14e-04  runR2: 0.84,  elapsed 5.6s (compute 5.3)\n",
      "[10, 4000] Loss: 9.77e-04  runR2: 0.83,  elapsed 6.0s (compute 5.6)\n",
      "[10, 4800] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.8s (compute 6.4)\n",
      "[10, 5600] Loss: 1.03e-03  runR2: 0.83,  elapsed 5.8s (compute 5.4)\n",
      "[10, 6400] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.8s (compute 5.4)\n",
      "[10, 7200] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[10, 8000] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.8s (compute 5.9)\n",
      "[10, 8800] Loss: 1.18e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[10, 9600] Loss: 1.20e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[10, 10400] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.0s (compute 5.6)\n",
      "[10, 11200] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[10, 12000] Loss: 1.18e-03  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "[10, 12800] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[10, 13600] Loss: 9.91e-04  runR2: 0.83,  elapsed 6.6s (compute 6.1)\n",
      "[10, 14400] Loss: 9.81e-04  runR2: 0.84,  elapsed 6.4s (compute 6.0)\n",
      "[10, 15200] Loss: 9.51e-04  runR2: 0.84,  elapsed 7.0s (compute 6.2)\n",
      "[10, 16000] Loss: 9.61e-04  runR2: 0.84,  elapsed 6.4s (compute 5.9)\n",
      "[10, 16800] Loss: 9.04e-04  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "[10, 17600] Loss: 8.96e-04  runR2: 0.84,  elapsed 6.4s (compute 6.0)\n",
      "[10, 18400] Loss: 8.81e-04  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "[10, 19200] Loss: 8.69e-04  runR2: 0.84,  elapsed 6.4s (compute 5.9)\n",
      "[10, 20000] Loss: 9.78e-04  runR2: 0.84,  elapsed 6.6s (compute 6.2)\n",
      "[10, 20800] Loss: 9.30e-04  runR2: 0.84,  elapsed 7.0s (compute 6.5)\n",
      "[10, 21600] Loss: 9.90e-04  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[10, 22400] Loss: 1.12e-03  runR2: 0.84,  elapsed 7.8s (compute 7.0)\n",
      "[10, 23200] Loss: 1.08e-03  runR2: 0.84,  elapsed 6.6s (compute 6.2)\n",
      "[10, 24000] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.3s (compute 5.9)\n",
      "[10, 24800] Loss: 9.74e-04  runR2: 0.84,  elapsed 6.3s (compute 6.0)\n",
      "[10, 25600] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.0s (compute 5.7)\n",
      "Epoch 10 TRAIN loss: 1.01e-03  MSE: 8.66e-04  h-con:  1.45e+03   R2: 0.84  R2-dT/dt: 0.79   R2-dq/dt: 0.89   R2-precc: 0.986\n",
      "Epoch 10/10 complete, took 212.71 seconds, autoreg window was 5\n"
     ]
    }
   ],
   "source": [
    "# 160 160\n",
    "# autoreg, hybrid-loss, 1 yr\n",
    "num_epochs = 10\n",
    "save_model = False\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, batch_size_tr, autoregressive, train=True)\n",
    "if use_val: val = model_train_eval(val_loader, model, batch_size_val, autoregressive, train=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3e35b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/28272/ipykernel_1666826/1465003934.py:28: RuntimeWarning: divide by zero encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_1666826/1465003934.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_1666826/837733283.py:177: RuntimeWarning: invalid value encountered in add\n",
      "  epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1729647348947/work/c10/cuda/CUDACachingAllocator.cpp\":963, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     timewindoww\u001b[38;5;241m=\u001b[39mtimewindow\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Training rollout timesteps: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, timewindoww))\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimewindoww\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_wandb: wandb\u001b[38;5;241m.\u001b[39mlog(train_runner\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_val:\n",
      "Cell \u001b[0;32mIn[102], line 65\u001b[0m, in \u001b[0;36mmodel_train_eval.eval_one_epoch\u001b[0;34m(self, epoch, timewindow)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader):\n\u001b[1;32m     64\u001b[0m     inputs_lay_chunks, inputs_sfc_chunks, targets_lay_chunks, targets_sfc_chunks \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 65\u001b[0m     inputs_lay_chunks   \u001b[38;5;241m=\u001b[39m \u001b[43minputs_lay_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     inputs_sfc_chunks   \u001b[38;5;241m=\u001b[39m inputs_sfc_chunks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m     targets_sfc_chunks  \u001b[38;5;241m=\u001b[39m targets_sfc_chunks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1729647348947/work/c10/cuda/CUDACachingAllocator.cpp\":963, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# 160 160\n",
    "# autoreg, hybrid-loss, 2 years concat\n",
    "num_epochs = 10\n",
    "save_model = False\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, batch_size_tr, autoregressive, train=True)\n",
    "if use_val: val = model_train_eval(val_loader, model, batch_size_val, autoregressive, train=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f48c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training rollout timesteps: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/28272/ipykernel_1582727/1465003934.py:28: RuntimeWarning: divide by zero encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_1582727/1465003934.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  corrcoef = dividend / divisor\n",
      "/tmp/user/28272/ipykernel_1582727/1891790002.py:176: RuntimeWarning: invalid value encountered in add\n",
      "  epoch_r2_lev += corrcoeff_pairs_batchfirst(ypo_lay, yto_lay)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 800] Loss: 3.78e-03  runR2: 0.15,  elapsed 18.0s (compute 7.9)\n",
      "[1, 1600] Loss: 2.36e-03  runR2: 0.41,  elapsed 8.4s (compute 8.0)\n",
      "[1, 2400] Loss: 2.09e-03  runR2: 0.47,  elapsed 8.1s (compute 7.7)\n",
      "[1, 3200] Loss: 1.95e-03  runR2: 0.51,  elapsed 8.4s (compute 8.0)\n",
      "[1, 4000] Loss: 1.70e-03  runR2: 0.54,  elapsed 8.3s (compute 7.9)\n",
      "[1, 4800] Loss: 1.65e-03  runR2: 0.57,  elapsed 7.7s (compute 7.4)\n",
      "[1, 5600] Loss: 1.61e-03  runR2: 0.59,  elapsed 7.9s (compute 7.6)\n",
      "[1, 6400] Loss: 1.60e-03  runR2: 0.59,  elapsed 7.7s (compute 7.1)\n",
      "[1, 7200] Loss: 1.46e-03  runR2: 0.61,  elapsed 7.9s (compute 7.6)\n",
      "[1, 8000] Loss: 1.43e-03  runR2: 0.61,  elapsed 7.8s (compute 7.1)\n",
      "[1, 8800] Loss: 1.56e-03  runR2: 0.61,  elapsed 7.7s (compute 7.4)\n",
      "[1, 9600] Loss: 1.51e-03  runR2: 0.62,  elapsed 8.6s (compute 8.3)\n",
      "[1, 10400] Loss: 1.67e-03  runR2: 0.62,  elapsed 10.9s (compute 7.9)\n",
      "[1, 11200] Loss: 1.43e-03  runR2: 0.62,  elapsed 8.5s (compute 8.2)\n",
      "[1, 12000] Loss: 1.53e-03  runR2: 0.63,  elapsed 7.8s (compute 7.4)\n",
      "[1, 12800] Loss: 1.59e-03  runR2: 0.63,  elapsed 7.9s (compute 7.6)\n",
      "[1, 13600] Loss: 1.46e-03  runR2: 0.63,  elapsed 8.3s (compute 7.9)\n",
      "[1, 14400] Loss: 1.26e-03  runR2: 0.64,  elapsed 8.2s (compute 7.8)\n",
      "[1, 15200] Loss: 1.45e-03  runR2: 0.64,  elapsed 7.2s (compute 6.8)\n",
      "[1, 16000] Loss: 1.33e-03  runR2: 0.64,  elapsed 7.3s (compute 7.0)\n",
      "[1, 16800] Loss: 1.51e-03  runR2: 0.65,  elapsed 7.4s (compute 7.0)\n",
      "[1, 17600] Loss: 1.37e-03  runR2: 0.65,  elapsed 9.5s (compute 8.7)\n",
      "[1, 18400] Loss: 1.31e-03  runR2: 0.65,  elapsed 7.9s (compute 7.5)\n",
      "[1, 19200] Loss: 1.19e-03  runR2: 0.66,  elapsed 8.7s (compute 8.3)\n",
      "[1, 20000] Loss: 1.34e-03  runR2: 0.66,  elapsed 8.9s (compute 8.4)\n",
      "[1, 20800] Loss: 1.37e-03  runR2: 0.66,  elapsed 8.6s (compute 8.1)\n",
      "[1, 21600] Loss: 1.27e-03  runR2: 0.66,  elapsed 8.8s (compute 8.3)\n",
      "[1, 22400] Loss: 1.32e-03  runR2: 0.67,  elapsed 8.7s (compute 8.2)\n",
      "[1, 23200] Loss: 1.21e-03  runR2: 0.67,  elapsed 8.4s (compute 8.0)\n",
      "[1, 24000] Loss: 1.34e-03  runR2: 0.68,  elapsed 8.2s (compute 7.8)\n",
      "Epoch 1 TRAIN loss: 1.55e-03  MSE: 1.25e-03  h-con:  3.03e+03   R2: 0.68  R2-dT/dt: -0.45   R2-dq/dt: 0.41   R2-precc: 0.930\n",
      "Epoch 1/10 complete, took 258.47 seconds, autoreg window was 1\n",
      "Epoch 2 Training rollout timesteps: 1 \n",
      "[2, 800] Loss: 1.45e-03  runR2: 0.74,  elapsed 13.9s (compute 7.7)\n",
      "[2, 1600] Loss: 1.37e-03  runR2: 0.77,  elapsed 7.9s (compute 7.4)\n",
      "[2, 2400] Loss: 1.34e-03  runR2: 0.76,  elapsed 8.6s (compute 8.1)\n",
      "[2, 3200] Loss: 1.35e-03  runR2: 0.76,  elapsed 8.5s (compute 8.1)\n",
      "[2, 4000] Loss: 1.20e-03  runR2: 0.77,  elapsed 8.5s (compute 8.1)\n",
      "[2, 4800] Loss: 1.22e-03  runR2: 0.78,  elapsed 8.7s (compute 8.2)\n",
      "[2, 5600] Loss: 1.23e-03  runR2: 0.78,  elapsed 8.6s (compute 8.1)\n",
      "[2, 6400] Loss: 1.24e-03  runR2: 0.78,  elapsed 7.9s (compute 7.4)\n",
      "[2, 7200] Loss: 1.15e-03  runR2: 0.78,  elapsed 8.6s (compute 8.1)\n",
      "[2, 8000] Loss: 1.17e-03  runR2: 0.78,  elapsed 9.4s (compute 8.5)\n",
      "[2, 8800] Loss: 1.28e-03  runR2: 0.78,  elapsed 8.7s (compute 8.2)\n",
      "[2, 9600] Loss: 1.27e-03  runR2: 0.78,  elapsed 8.4s (compute 8.0)\n",
      "[2, 10400] Loss: 1.42e-03  runR2: 0.78,  elapsed 9.1s (compute 8.4)\n",
      "[2, 11200] Loss: 1.21e-03  runR2: 0.78,  elapsed 8.3s (compute 7.9)\n",
      "[2, 12000] Loss: 1.33e-03  runR2: 0.78,  elapsed 8.5s (compute 8.1)\n",
      "[2, 12800] Loss: 1.39e-03  runR2: 0.78,  elapsed 9.0s (compute 8.6)\n",
      "[2, 13600] Loss: 1.28e-03  runR2: 0.78,  elapsed 8.9s (compute 8.5)\n",
      "[2, 14400] Loss: 1.11e-03  runR2: 0.78,  elapsed 7.6s (compute 7.2)\n",
      "[2, 15200] Loss: 1.29e-03  runR2: 0.78,  elapsed 9.0s (compute 8.5)\n",
      "[2, 16000] Loss: 1.18e-03  runR2: 0.78,  elapsed 8.4s (compute 7.9)\n",
      "[2, 16800] Loss: 1.36e-03  runR2: 0.78,  elapsed 8.2s (compute 7.7)\n",
      "[2, 17600] Loss: 1.24e-03  runR2: 0.78,  elapsed 9.3s (compute 8.4)\n",
      "[2, 18400] Loss: 1.19e-03  runR2: 0.78,  elapsed 8.8s (compute 8.4)\n",
      "[2, 19200] Loss: 1.07e-03  runR2: 0.78,  elapsed 8.7s (compute 8.2)\n",
      "[2, 20000] Loss: 1.23e-03  runR2: 0.79,  elapsed 8.8s (compute 8.4)\n",
      "[2, 20800] Loss: 1.25e-03  runR2: 0.79,  elapsed 9.0s (compute 8.6)\n",
      "[2, 21600] Loss: 1.17e-03  runR2: 0.79,  elapsed 8.7s (compute 8.2)\n",
      "[2, 22400] Loss: 1.22e-03  runR2: 0.79,  elapsed 8.3s (compute 7.9)\n",
      "[2, 23200] Loss: 1.13e-03  runR2: 0.79,  elapsed 7.8s (compute 7.4)\n",
      "[2, 24000] Loss: 1.24e-03  runR2: 0.79,  elapsed 7.7s (compute 7.4)\n",
      "Epoch 2 TRAIN loss: 1.25e-03  MSE: 1.02e-03  h-con:  2.29e+03   R2: 0.79  R2-dT/dt: 0.61   R2-dq/dt: 0.55   R2-precc: 0.968\n",
      "Epoch 2/10 complete, took 262.31 seconds, autoreg window was 1\n",
      "Epoch 3 Training rollout timesteps: 1 \n",
      "[3, 800] Loss: 1.43e-03  runR2: 0.79,  elapsed 13.6s (compute 7.5)\n",
      "[3, 1600] Loss: 1.28e-03  runR2: 0.81,  elapsed 8.7s (compute 8.2)\n",
      "[3, 2400] Loss: 1.25e-03  runR2: 0.80,  elapsed 8.3s (compute 7.9)\n",
      "[3, 3200] Loss: 1.27e-03  runR2: 0.81,  elapsed 8.1s (compute 7.7)\n",
      "[3, 4000] Loss: 1.12e-03  runR2: 0.81,  elapsed 8.5s (compute 8.1)\n",
      "[3, 4800] Loss: 1.14e-03  runR2: 0.81,  elapsed 8.1s (compute 7.7)\n",
      "[3, 5600] Loss: 1.16e-03  runR2: 0.82,  elapsed 7.5s (compute 7.0)\n",
      "[3, 6400] Loss: 1.17e-03  runR2: 0.82,  elapsed 8.2s (compute 7.8)\n",
      "[3, 7200] Loss: 1.08e-03  runR2: 0.82,  elapsed 7.8s (compute 7.4)\n",
      "[3, 8000] Loss: 1.11e-03  runR2: 0.82,  elapsed 8.5s (compute 7.7)\n",
      "[3, 8800] Loss: 1.21e-03  runR2: 0.82,  elapsed 7.7s (compute 7.3)\n",
      "[3, 9600] Loss: 1.20e-03  runR2: 0.81,  elapsed 7.9s (compute 7.6)\n",
      "[3, 10400] Loss: 1.35e-03  runR2: 0.81,  elapsed 8.4s (compute 7.6)\n",
      "[3, 11200] Loss: 1.15e-03  runR2: 0.81,  elapsed 7.9s (compute 7.5)\n",
      "[3, 12000] Loss: 1.26e-03  runR2: 0.81,  elapsed 8.5s (compute 8.0)\n",
      "[3, 12800] Loss: 1.33e-03  runR2: 0.81,  elapsed 8.1s (compute 7.7)\n",
      "[3, 13600] Loss: 1.22e-03  runR2: 0.81,  elapsed 7.9s (compute 7.4)\n",
      "[3, 14400] Loss: 1.06e-03  runR2: 0.81,  elapsed 8.0s (compute 7.5)\n",
      "[3, 15200] Loss: 1.24e-03  runR2: 0.81,  elapsed 8.2s (compute 7.8)\n",
      "[3, 16000] Loss: 1.13e-03  runR2: 0.81,  elapsed 8.4s (compute 8.0)\n",
      "[3, 16800] Loss: 1.30e-03  runR2: 0.81,  elapsed 8.3s (compute 7.9)\n",
      "[3, 17600] Loss: 1.19e-03  runR2: 0.81,  elapsed 8.4s (compute 7.6)\n",
      "[3, 18400] Loss: 1.14e-03  runR2: 0.81,  elapsed 8.7s (compute 8.3)\n",
      "[3, 19200] Loss: 1.03e-03  runR2: 0.81,  elapsed 8.6s (compute 8.1)\n",
      "[3, 20000] Loss: 1.18e-03  runR2: 0.81,  elapsed 8.3s (compute 7.9)\n",
      "[3, 20800] Loss: 1.20e-03  runR2: 0.81,  elapsed 7.9s (compute 7.4)\n",
      "[3, 21600] Loss: 1.13e-03  runR2: 0.81,  elapsed 7.3s (compute 6.9)\n",
      "[3, 22400] Loss: 1.17e-03  runR2: 0.81,  elapsed 7.2s (compute 6.8)\n",
      "[3, 23200] Loss: 1.09e-03  runR2: 0.81,  elapsed 7.2s (compute 6.8)\n",
      "[3, 24000] Loss: 1.20e-03  runR2: 0.81,  elapsed 7.2s (compute 6.8)\n",
      "Epoch 3 TRAIN loss: 1.19e-03  MSE: 9.77e-04  h-con:  2.09e+03   R2: 0.81  R2-dT/dt: 0.67   R2-dq/dt: 0.61   R2-precc: 0.973\n",
      "Epoch 3/10 complete, took 248.01 seconds, autoreg window was 1\n",
      "Epoch 4 Training rollout timesteps: 2 \n",
      "[4, 800] Loss: 1.48e-03  runR2: 0.80,  elapsed 11.5s (compute 5.7)\n",
      "[4, 1600] Loss: 1.23e-03  runR2: 0.82,  elapsed 6.3s (compute 5.8)\n",
      "[4, 2400] Loss: 1.20e-03  runR2: 0.82,  elapsed 6.7s (compute 6.3)\n",
      "[4, 3200] Loss: 1.22e-03  runR2: 0.82,  elapsed 6.8s (compute 6.3)\n",
      "[4, 4000] Loss: 1.06e-03  runR2: 0.82,  elapsed 6.7s (compute 6.2)\n",
      "[4, 4800] Loss: 1.08e-03  runR2: 0.82,  elapsed 6.2s (compute 5.8)\n",
      "[4, 5600] Loss: 1.11e-03  runR2: 0.82,  elapsed 6.0s (compute 5.6)\n",
      "[4, 6400] Loss: 1.12e-03  runR2: 0.82,  elapsed 6.8s (compute 6.3)\n",
      "[4, 7200] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[4, 8000] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.9s (compute 6.0)\n",
      "[4, 8800] Loss: 1.15e-03  runR2: 0.82,  elapsed 6.6s (compute 6.1)\n",
      "[4, 9600] Loss: 1.14e-03  runR2: 0.82,  elapsed 6.4s (compute 6.0)\n",
      "[4, 10400] Loss: 1.29e-03  runR2: 0.82,  elapsed 7.8s (compute 7.1)\n",
      "[4, 11200] Loss: 1.10e-03  runR2: 0.82,  elapsed 7.0s (compute 6.6)\n",
      "[4, 12000] Loss: 1.21e-03  runR2: 0.82,  elapsed 7.4s (compute 7.0)\n",
      "[4, 12800] Loss: 1.27e-03  runR2: 0.82,  elapsed 6.8s (compute 6.4)\n",
      "[4, 13600] Loss: 1.16e-03  runR2: 0.82,  elapsed 6.5s (compute 6.1)\n",
      "[4, 14400] Loss: 1.00e-03  runR2: 0.82,  elapsed 6.7s (compute 6.3)\n",
      "[4, 15200] Loss: 1.18e-03  runR2: 0.82,  elapsed 6.4s (compute 6.1)\n",
      "[4, 16000] Loss: 1.07e-03  runR2: 0.82,  elapsed 6.6s (compute 6.2)\n",
      "[4, 16800] Loss: 1.24e-03  runR2: 0.82,  elapsed 6.6s (compute 6.2)\n",
      "[4, 17600] Loss: 1.13e-03  runR2: 0.82,  elapsed 6.9s (compute 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 18400] Loss: 1.08e-03  runR2: 0.82,  elapsed 6.8s (compute 6.4)\n",
      "[4, 19200] Loss: 9.76e-04  runR2: 0.82,  elapsed 7.3s (compute 6.9)\n",
      "[4, 20000] Loss: 1.12e-03  runR2: 0.82,  elapsed 6.9s (compute 6.5)\n",
      "[4, 20800] Loss: 1.14e-03  runR2: 0.82,  elapsed 7.1s (compute 6.7)\n",
      "[4, 21600] Loss: 1.07e-03  runR2: 0.82,  elapsed 6.9s (compute 6.5)\n",
      "[4, 22400] Loss: 1.11e-03  runR2: 0.82,  elapsed 6.6s (compute 6.2)\n",
      "[4, 23200] Loss: 1.03e-03  runR2: 0.82,  elapsed 6.7s (compute 6.4)\n",
      "[4, 24000] Loss: 1.14e-03  runR2: 0.82,  elapsed 6.5s (compute 6.1)\n",
      "Epoch 4 TRAIN loss: 1.13e-03  MSE: 9.40e-04  h-con:  1.90e+03   R2: 0.82  R2-dT/dt: 0.68   R2-dq/dt: 0.64   R2-precc: 0.977\n",
      "Epoch 4/10 complete, took 207.46 seconds, autoreg window was 2\n",
      "Epoch 5 Training rollout timesteps: 3 \n",
      "[5, 800] Loss: 1.44e-03  runR2: 0.81,  elapsed 12.6s (compute 6.3)\n",
      "[5, 1600] Loss: 1.16e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[5, 2400] Loss: 1.14e-03  runR2: 0.82,  elapsed 5.9s (compute 5.4)\n",
      "[5, 3200] Loss: 1.16e-03  runR2: 0.82,  elapsed 6.4s (compute 6.0)\n",
      "[5, 4000] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.6s (compute 6.1)\n",
      "[5, 4800] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[5, 5600] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[5, 6400] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.0s (compute 5.5)\n",
      "[5, 7200] Loss: 9.88e-04  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[5, 8000] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.8s (compute 5.9)\n",
      "[5, 8800] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.9s (compute 6.4)\n",
      "[5, 9600] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.1s (compute 5.9)\n",
      "[5, 10400] Loss: 1.23e-03  runR2: 0.83,  elapsed 6.5s (compute 5.7)\n",
      "[5, 11200] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[5, 12000] Loss: 1.16e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[5, 12800] Loss: 1.22e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[5, 13600] Loss: 1.12e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[5, 14400] Loss: 9.62e-04  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[5, 15200] Loss: 1.13e-03  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "[5, 16000] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[5, 16800] Loss: 1.20e-03  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[5, 17600] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.5s (compute 5.7)\n",
      "[5, 18400] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[5, 19200] Loss: 9.47e-04  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[5, 20000] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[5, 20800] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[5, 21600] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[5, 22400] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[5, 23200] Loss: 1.00e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[5, 24000] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.1s (compute 5.7)\n",
      "Epoch 5 TRAIN loss: 1.09e-03  MSE: 9.13e-04  h-con:  1.73e+03   R2: 0.83  R2-dT/dt: 0.70   R2-dq/dt: 0.66   R2-precc: 0.981\n",
      "Epoch 5/10 complete, took 198.37 seconds, autoreg window was 3\n",
      "Epoch 6 Training rollout timesteps: 4 \n",
      "[6, 800] Loss: 1.44e-03  runR2: 0.81,  elapsed 11.4s (compute 5.7)\n",
      "[6, 1600] Loss: 1.13e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[6, 2400] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[6, 3200] Loss: 1.13e-03  runR2: 0.83,  elapsed 7.3s (compute 6.6)\n",
      "[6, 4000] Loss: 9.80e-04  runR2: 0.83,  elapsed 6.6s (compute 6.2)\n",
      "[6, 4800] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[6, 5600] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.3s (compute 5.9)\n",
      "[6, 6400] Loss: 1.05e-03  runR2: 0.83,  elapsed 5.7s (compute 5.3)\n",
      "[6, 7200] Loss: 9.72e-04  runR2: 0.84,  elapsed 6.2s (compute 5.7)\n",
      "[6, 8000] Loss: 9.89e-04  runR2: 0.84,  elapsed 6.6s (compute 5.7)\n",
      "[6, 8800] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.8s (compute 6.4)\n",
      "[6, 9600] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.6s (compute 6.3)\n",
      "[6, 10400] Loss: 1.21e-03  runR2: 0.83,  elapsed 7.0s (compute 6.2)\n",
      "[6, 11200] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[6, 12000] Loss: 1.14e-03  runR2: 0.83,  elapsed 5.9s (compute 5.4)\n",
      "[6, 12800] Loss: 1.19e-03  runR2: 0.83,  elapsed 6.5s (compute 6.0)\n",
      "[6, 13600] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[6, 14400] Loss: 9.40e-04  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[6, 15200] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.0s (compute 5.5)\n",
      "[6, 16000] Loss: 1.02e-03  runR2: 0.83,  elapsed 5.8s (compute 5.4)\n",
      "[6, 16800] Loss: 1.17e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[6, 17600] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.9s (compute 6.0)\n",
      "[6, 18400] Loss: 1.03e-03  runR2: 0.83,  elapsed 7.4s (compute 6.9)\n",
      "[6, 19200] Loss: 9.28e-04  runR2: 0.83,  elapsed 7.4s (compute 6.9)\n",
      "[6, 20000] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.6s (compute 6.1)\n",
      "[6, 20800] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.5s (compute 6.1)\n",
      "[6, 21600] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[6, 22400] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[6, 23200] Loss: 9.80e-04  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[6, 24000] Loss: 1.08e-03  runR2: 0.83,  elapsed 5.9s (compute 5.5)\n",
      "Epoch 6 TRAIN loss: 1.06e-03  MSE: 8.98e-04  h-con:  1.63e+03   R2: 0.83  R2-dT/dt: 0.70   R2-dq/dt: 0.68   R2-precc: 0.984\n",
      "Epoch 6/10 complete, took 201.30 seconds, autoreg window was 4\n",
      "Epoch 7 Training rollout timesteps: 5 \n",
      "[7, 800] Loss: 1.45e-03  runR2: 0.82,  elapsed 12.1s (compute 6.2)\n",
      "[7, 1600] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.8s (compute 6.4)\n",
      "[7, 2400] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[7, 3200] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.3s (compute 5.8)\n",
      "[7, 4000] Loss: 9.66e-04  runR2: 0.83,  elapsed 6.0s (compute 5.6)\n",
      "[7, 4800] Loss: 9.90e-04  runR2: 0.84,  elapsed 6.3s (compute 5.9)\n",
      "[7, 5600] Loss: 1.01e-03  runR2: 0.84,  elapsed 6.5s (compute 6.1)\n",
      "[7, 6400] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.7s (compute 6.2)\n",
      "[7, 7200] Loss: 9.55e-04  runR2: 0.84,  elapsed 6.8s (compute 6.4)\n",
      "[7, 8000] Loss: 9.73e-04  runR2: 0.84,  elapsed 7.2s (compute 6.4)\n",
      "[7, 8800] Loss: 1.06e-03  runR2: 0.84,  elapsed 6.8s (compute 6.3)\n",
      "[7, 9600] Loss: 1.06e-03  runR2: 0.84,  elapsed 6.8s (compute 6.4)\n",
      "[7, 10400] Loss: 1.19e-03  runR2: 0.83,  elapsed 7.4s (compute 6.6)\n",
      "[7, 11200] Loss: 1.01e-03  runR2: 0.83,  elapsed 7.1s (compute 6.6)\n",
      "[7, 12000] Loss: 1.12e-03  runR2: 0.83,  elapsed 8.0s (compute 7.6)\n",
      "[7, 12800] Loss: 1.17e-03  runR2: 0.83,  elapsed 7.1s (compute 6.7)\n",
      "[7, 13600] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[7, 14400] Loss: 9.27e-04  runR2: 0.83,  elapsed 6.7s (compute 6.2)\n",
      "[7, 15200] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[7, 16000] Loss: 1.00e-03  runR2: 0.83,  elapsed 6.8s (compute 6.4)\n",
      "[7, 16800] Loss: 1.15e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[7, 17600] Loss: 1.06e-03  runR2: 0.83,  elapsed 7.6s (compute 6.7)\n",
      "[7, 18400] Loss: 1.01e-03  runR2: 0.83,  elapsed 6.8s (compute 6.3)\n",
      "[7, 19200] Loss: 9.13e-04  runR2: 0.83,  elapsed 7.0s (compute 6.6)\n",
      "[7, 20000] Loss: 1.05e-03  runR2: 0.83,  elapsed 7.0s (compute 6.5)\n",
      "[7, 20800] Loss: 1.05e-03  runR2: 0.83,  elapsed 7.0s (compute 6.5)\n",
      "[7, 21600] Loss: 1.01e-03  runR2: 0.83,  elapsed 7.2s (compute 6.7)\n",
      "[7, 22400] Loss: 1.03e-03  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "[7, 23200] Loss: 9.68e-04  runR2: 0.83,  elapsed 7.0s (compute 6.6)\n",
      "[7, 24000] Loss: 1.06e-03  runR2: 0.83,  elapsed 7.0s (compute 6.6)\n",
      "Epoch 7 TRAIN loss: 1.04e-03  MSE: 8.88e-04  h-con:  1.56e+03   R2: 0.83  R2-dT/dt: 0.70   R2-dq/dt: 0.69   R2-precc: 0.985\n",
      "Epoch 7/10 complete, took 212.60 seconds, autoreg window was 5\n",
      "Epoch 8 Training rollout timesteps: 5 \n",
      "[8, 800] Loss: 1.52e-03  runR2: 0.82,  elapsed 11.7s (compute 5.8)\n",
      "[8, 1600] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.8s (compute 6.3)\n",
      "[8, 2400] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.9s (compute 6.5)\n",
      "[8, 3200] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[8, 4000] Loss: 9.57e-04  runR2: 0.83,  elapsed 6.2s (compute 5.7)\n",
      "[8, 4800] Loss: 9.81e-04  runR2: 0.84,  elapsed 6.1s (compute 5.6)\n",
      "[8, 5600] Loss: 1.00e-03  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[8, 6400] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.0s (compute 5.6)\n",
      "[8, 7200] Loss: 9.46e-04  runR2: 0.84,  elapsed 6.1s (compute 5.6)\n",
      "[8, 8000] Loss: 9.64e-04  runR2: 0.84,  elapsed 6.5s (compute 5.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8800] Loss: 1.05e-03  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[8, 9600] Loss: 1.05e-03  runR2: 0.84,  elapsed 5.8s (compute 5.5)\n",
      "[8, 10400] Loss: 1.18e-03  runR2: 0.84,  elapsed 6.0s (compute 5.2)\n",
      "[8, 11200] Loss: 1.00e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[8, 12000] Loss: 1.11e-03  runR2: 0.83,  elapsed 6.4s (compute 5.9)\n",
      "[8, 12800] Loss: 1.16e-03  runR2: 0.83,  elapsed 5.9s (compute 5.4)\n",
      "[8, 13600] Loss: 1.06e-03  runR2: 0.83,  elapsed 6.0s (compute 5.5)\n",
      "[8, 14400] Loss: 9.19e-04  runR2: 0.83,  elapsed 6.3s (compute 5.8)\n",
      "[8, 15200] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.6s (compute 6.0)\n",
      "[8, 16000] Loss: 9.93e-04  runR2: 0.83,  elapsed 6.2s (compute 5.7)\n",
      "[8, 16800] Loss: 1.14e-03  runR2: 0.83,  elapsed 6.2s (compute 5.7)\n",
      "[8, 17600] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.3s (compute 5.3)\n",
      "[8, 18400] Loss: 1.00e-03  runR2: 0.83,  elapsed 5.7s (compute 5.3)\n",
      "[8, 19200] Loss: 9.06e-04  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[8, 20000] Loss: 1.04e-03  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[8, 20800] Loss: 1.04e-03  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[8, 21600] Loss: 9.99e-04  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[8, 22400] Loss: 1.03e-03  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[8, 23200] Loss: 9.61e-04  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "[8, 24000] Loss: 1.06e-03  runR2: 0.83,  elapsed 5.7s (compute 5.2)\n",
      "Epoch 8 TRAIN loss: 1.04e-03  MSE: 8.83e-04  h-con:  1.53e+03   R2: 0.83  R2-dT/dt: 0.72   R2-dq/dt: 0.70   R2-precc: 0.985\n",
      "Epoch 8/10 complete, took 189.22 seconds, autoreg window was 5\n",
      "Epoch 9 Training rollout timesteps: 5 \n",
      "[9, 800] Loss: 1.52e-03  runR2: 0.82,  elapsed 11.5s (compute 5.8)\n",
      "[9, 1600] Loss: 1.09e-03  runR2: 0.84,  elapsed 6.6s (compute 6.2)\n",
      "[9, 2400] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.8s (compute 6.4)\n",
      "[9, 3200] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[9, 4000] Loss: 9.50e-04  runR2: 0.83,  elapsed 6.8s (compute 6.3)\n",
      "[9, 4800] Loss: 9.75e-04  runR2: 0.84,  elapsed 6.8s (compute 6.4)\n",
      "[9, 5600] Loss: 9.99e-04  runR2: 0.84,  elapsed 7.2s (compute 6.8)\n",
      "[9, 6400] Loss: 1.02e-03  runR2: 0.84,  elapsed 7.4s (compute 7.0)\n",
      "[9, 7200] Loss: 9.39e-04  runR2: 0.84,  elapsed 6.8s (compute 6.4)\n",
      "[9, 8000] Loss: 9.57e-04  runR2: 0.84,  elapsed 6.5s (compute 5.7)\n",
      "[9, 8800] Loss: 1.05e-03  runR2: 0.84,  elapsed 6.8s (compute 6.4)\n",
      "[9, 9600] Loss: 1.04e-03  runR2: 0.84,  elapsed 7.1s (compute 6.8)\n",
      "[9, 10400] Loss: 1.17e-03  runR2: 0.84,  elapsed 7.1s (compute 6.2)\n",
      "[9, 11200] Loss: 9.97e-04  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[9, 12000] Loss: 1.10e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[9, 12800] Loss: 1.15e-03  runR2: 0.83,  elapsed 6.9s (compute 6.4)\n",
      "[9, 13600] Loss: 1.05e-03  runR2: 0.83,  elapsed 7.1s (compute 6.6)\n",
      "[9, 14400] Loss: 9.12e-04  runR2: 0.83,  elapsed 7.1s (compute 6.6)\n",
      "[9, 15200] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.9s (compute 6.4)\n",
      "[9, 16000] Loss: 9.86e-04  runR2: 0.83,  elapsed 7.0s (compute 6.5)\n",
      "[9, 16800] Loss: 1.14e-03  runR2: 0.84,  elapsed 5.7s (compute 5.2)\n",
      "[9, 17600] Loss: 1.04e-03  runR2: 0.83,  elapsed 6.7s (compute 5.7)\n",
      "[9, 18400] Loss: 9.97e-04  runR2: 0.83,  elapsed 6.7s (compute 6.2)\n",
      "[9, 19200] Loss: 9.01e-04  runR2: 0.83,  elapsed 6.7s (compute 6.3)\n",
      "[9, 20000] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.6s (compute 6.2)\n",
      "[9, 20800] Loss: 1.04e-03  runR2: 0.84,  elapsed 6.4s (compute 6.0)\n",
      "[9, 21600] Loss: 9.94e-04  runR2: 0.83,  elapsed 6.2s (compute 5.7)\n",
      "[9, 22400] Loss: 1.02e-03  runR2: 0.83,  elapsed 6.8s (compute 6.3)\n",
      "[9, 23200] Loss: 9.56e-04  runR2: 0.83,  elapsed 6.3s (compute 5.9)\n",
      "[9, 24000] Loss: 1.05e-03  runR2: 0.84,  elapsed 6.2s (compute 5.8)\n",
      "Epoch 9 TRAIN loss: 1.03e-03  MSE: 8.79e-04  h-con:  1.50e+03   R2: 0.84  R2-dT/dt: 0.71   R2-dq/dt: 0.70   R2-precc: 0.985\n",
      "Epoch 9/10 complete, took 206.15 seconds, autoreg window was 5\n",
      "Epoch 10 Training rollout timesteps: 5 \n",
      "[10, 800] Loss: 1.52e-03  runR2: 0.82,  elapsed 11.4s (compute 5.6)\n",
      "[10, 1600] Loss: 1.09e-03  runR2: 0.84,  elapsed 6.5s (compute 6.0)\n",
      "[10, 2400] Loss: 1.07e-03  runR2: 0.83,  elapsed 6.7s (compute 6.2)\n",
      "[10, 3200] Loss: 1.09e-03  runR2: 0.83,  elapsed 6.2s (compute 5.7)\n",
      "[10, 4000] Loss: 9.44e-04  runR2: 0.83,  elapsed 7.0s (compute 6.5)\n",
      "[10, 4800] Loss: 9.70e-04  runR2: 0.84,  elapsed 6.8s (compute 6.3)\n",
      "[10, 5600] Loss: 9.95e-04  runR2: 0.84,  elapsed 6.3s (compute 5.9)\n",
      "[10, 6400] Loss: 1.01e-03  runR2: 0.84,  elapsed 6.7s (compute 6.3)\n",
      "[10, 7200] Loss: 9.35e-04  runR2: 0.84,  elapsed 6.8s (compute 6.4)\n",
      "[10, 8000] Loss: 9.51e-04  runR2: 0.84,  elapsed 6.7s (compute 5.8)\n",
      "[10, 8800] Loss: 1.04e-03  runR2: 0.84,  elapsed 6.2s (compute 5.9)\n",
      "[10, 9600] Loss: 1.04e-03  runR2: 0.84,  elapsed 6.3s (compute 6.0)\n",
      "[10, 10400] Loss: 1.16e-03  runR2: 0.84,  elapsed 6.6s (compute 5.8)\n",
      "[10, 11200] Loss: 9.91e-04  runR2: 0.84,  elapsed 6.9s (compute 6.5)\n",
      "[10, 12000] Loss: 1.10e-03  runR2: 0.84,  elapsed 6.8s (compute 6.3)\n",
      "[10, 12800] Loss: 1.15e-03  runR2: 0.83,  elapsed 6.9s (compute 6.4)\n",
      "[10, 13600] Loss: 1.05e-03  runR2: 0.83,  elapsed 6.2s (compute 5.8)\n",
      "[10, 14400] Loss: 9.06e-04  runR2: 0.84,  elapsed 6.5s (compute 6.0)\n",
      "[10, 15200] Loss: 1.08e-03  runR2: 0.83,  elapsed 6.4s (compute 6.0)\n",
      "[10, 16000] Loss: 9.82e-04  runR2: 0.84,  elapsed 6.4s (compute 5.9)\n",
      "[10, 16800] Loss: 1.13e-03  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[10, 17600] Loss: 1.04e-03  runR2: 0.84,  elapsed 6.8s (compute 5.9)\n",
      "[10, 18400] Loss: 9.93e-04  runR2: 0.84,  elapsed 6.0s (compute 5.5)\n",
      "[10, 19200] Loss: 8.97e-04  runR2: 0.84,  elapsed 5.8s (compute 5.4)\n",
      "[10, 20000] Loss: 1.03e-03  runR2: 0.84,  elapsed 5.8s (compute 5.4)\n",
      "[10, 20800] Loss: 1.03e-03  runR2: 0.84,  elapsed 6.0s (compute 5.4)\n",
      "[10, 21600] Loss: 9.90e-04  runR2: 0.84,  elapsed 5.8s (compute 5.4)\n",
      "[10, 22400] Loss: 1.01e-03  runR2: 0.84,  elapsed 6.6s (compute 6.1)\n",
      "[10, 23200] Loss: 9.53e-04  runR2: 0.84,  elapsed 6.1s (compute 5.7)\n",
      "[10, 24000] Loss: 1.04e-03  runR2: 0.84,  elapsed 5.9s (compute 5.5)\n",
      "Epoch 10 TRAIN loss: 1.02e-03  MSE: 8.77e-04  h-con:  1.48e+03   R2: 0.84  R2-dT/dt: 0.71   R2-dq/dt: 0.71   R2-precc: 0.985\n",
      "Epoch 10/10 complete, took 197.70 seconds, autoreg window was 5\n"
     ]
    }
   ],
   "source": [
    "# 160 160\n",
    "# autoreg, hybrid-loss, 1 yr\n",
    "num_epochs = 10\n",
    "save_model = False\n",
    "\n",
    "train_runner = model_train_eval(train_loader, model, autoregressive, train=True)\n",
    "if use_val: val = model_train_eval(val_loader, model, autoregressive, train=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if timestep_scheduling:\n",
    "        timewindoww=timestep_schedule[epoch]            \n",
    "    else:\n",
    "        timewindoww=timewindow\n",
    "        \n",
    "    print(\"Epoch {} Training rollout timesteps: {} \".format(epoch+1, timewindoww))\n",
    "    train_runner.eval_one_epoch(epoch, timewindoww)\n",
    "    \n",
    "    if use_wandb: wandb.log(train_runner.metrics)\n",
    "    \n",
    "    if use_val:\n",
    "        if epoch%2:\n",
    "            print(\"VALIDATION..\")\n",
    "            val_runner.eval_one_epoch(epoch, timewindoww)\n",
    "\n",
    "            losses_val = {\"val_\"+k: v for k, v in val_runner.metrics.items()}\n",
    "            if use_wandb: wandb.log(losses_val)\n",
    "\n",
    "            val_loss = losses_val[\"val_loss\"]\n",
    "\n",
    "            # MODEL CHECKPOINT IF VALIDATION LOSS IMPROVED\n",
    "            if save_model and val_loss < best_val_loss:\n",
    "              torch.save({\n",
    "                          'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'val_loss': val_loss,\n",
    "                          }, SAVE_PATH)  \n",
    "              best_val_loss = val_loss \n",
    "              \n",
    "    print('Epoch {}/{} complete, took {:.2f} seconds, autoreg window was {}'.format(epoch+1,num_epochs,time.time() - t0,timewindoww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d8f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = train_runner.metrics[\"R2_lev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0726a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fccba1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(R2[:,1],np.arange(60))\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a5140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(60),R2[:,1])\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec4a3176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(60),R2[:,0])\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb66f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9466978 , 0.9770285 , 0.99382734, 0.99799865, 0.99909216,\n",
       "       0.9994714 , 0.9993281 , 0.99887717, 0.99846625, 0.9980534 ,\n",
       "       0.9971655 , 0.99597454, 0.993997  , 0.98964214, 0.98169845,\n",
       "       0.9661159 , 0.921062  , 0.7394638 , 0.5695126 , 0.5660285 ,\n",
       "       0.6403861 , 0.6843695 , 0.74047786, 0.79226625, 0.83503765,\n",
       "       0.87337685, 0.90147734, 0.9217613 , 0.9357034 , 0.9452235 ,\n",
       "       0.95050424, 0.95207465, 0.950873  , 0.9475209 , 0.94296634,\n",
       "       0.93585926, 0.9265152 , 0.9148766 , 0.90190625, 0.88793665,\n",
       "       0.8727932 , 0.8568837 , 0.84192264, 0.8285098 , 0.81639445,\n",
       "       0.8056849 , 0.79353935, 0.77991784, 0.7662803 , 0.7547418 ,\n",
       "       0.746002  , 0.74183106, 0.74000907, 0.7438193 , 0.75155765,\n",
       "       0.761369  , 0.7761432 , 0.78656244, 0.8160264 , 0.9044912 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb230c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_torch_cu121",
   "language": "python",
   "name": "py311_torch_cu121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
